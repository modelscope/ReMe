{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ReMe (formerly MemoryScope): Memory Management Framework for Agents Remember Me, Refine Me. ReMe provides AI agents with a unified memory system\u2014enabling the ability to extract, reuse, and share memories across users, tasks, and agents. Personal Memory + Task Memory + Tool Memory = Agent Memory Personal memory helps \" understand user preferences \", task memory helps agents \" perform better \", and tool memory enables \" smarter tool usage \". Architecture Design ReMe integrates three complementary memory capabilities: Task Memory/Experience Procedural knowledge reused across agents Success Pattern Recognition : Identify effective strategies and understand their underlying principles Failure Analysis Learning : Learn from mistakes and avoid repeating the same issues Comparative Patterns : Different sampling trajectories provide more valuable memories through comparison Validation Patterns : Confirm the effectiveness of extracted memories through validation modules Learn more about how to use task memory from task memory Personal Memory Contextualized memory for specific users \ud83d\udd27 Tool Memory Data-driven tool selection and usage optimization Historical Performance Tracking : Success rates, execution times, and token costs from real usage LLM-as-Judge Evaluation : Qualitative insights on why tools succeed or fail Parameter Optimization : Learn optimal parameter configurations from successful calls Dynamic Guidelines : Transform static tool descriptions into living, learned manuals Learn more about how to use tool memory from tool memory Learn more about how to use personal memory from [ personal memory ]( personal_memory / personal_memory . md ) Installation Install from PyPI (Recommended) pip install reme-ai Install from Source git clone https://github.com/modelscope/ReMe.git cd ReMe pip install . Environment Configuration Copy example.env to .env and modify the corresponding parameters: FLOW_APP_NAME = ReMe FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 Quick Start HTTP Service Startup reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local MCP Server Support reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Core API Usage Task Memory Management import requests # Experience Summarizer: Learn from execution trajectories response = requests . post ( \"http://localhost:8002/summary_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Help me create a project plan\" }], \"score\" : 1.0 } ] }) # Retriever: Get relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"How to efficiently manage project progress?\" , \"top_k\" : 1 }) curl version # Experience Summarizer: Learn from execution trajectories curl -X POST http://localhost:8002/summary_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [{\"role\": \"user\", \"content\": \"Help me create a project plan\"}], \"score\": 1.0} ] }' # Retriever: Get relevant memories curl -X POST http://localhost:8002/retrieve_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"How to efficiently manage project progress?\", \"top_k\": 1 }' Node.js version // Experience Summarizer: Learn from execution trajectories fetch ( \"http://localhost:8002/summary_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [{ role : \"user\" , content : \"Help me create a project plan\" }], score : 1.0 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retriever: Get relevant memories fetch ( \"http://localhost:8002/retrieve_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"How to efficiently manage project progress?\" , top_k : 1 }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); Personal Memory Management # Memory Integration: Learn from user interactions response = requests . post ( \"http://localhost:8002/summary_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"I like to drink coffee while working in the morning\" }, { \"role\" : \"assistant\" , \"content\" : \"I understand, you prefer to start your workday with coffee to stay energized\" } ] } ] }) # Memory Retrieval: Get personal memory fragments response = requests . post ( \"http://localhost:8002/retrieve_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"What are the user's work habits?\" , \"top_k\" : 5 }) curl version # Memory Integration: Learn from user interactions curl -X POST http://localhost:8002/summary_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [ {\"role\": \"user\", \"content\": \"I like to drink coffee while working in the morning\"}, {\"role\": \"assistant\", \"content\": \"I understand, you prefer to start your workday with coffee to stay energized\"} ]} ] }' # Memory Retrieval: Get personal memory fragments curl -X POST http://localhost:8002/retrieve_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"What are the user' s work habits? \", \" top_k \": 5 }' Node.js version // Memory Integration: Learn from user interactions fetch ( \"http://localhost:8002/summary_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [ { role : \"user\" , content : \"I like to drink coffee while working in the morning\" }, { role : \"assistant\" , content : \"I understand, you prefer to start your workday with coffee to stay energized\" } ]} ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Memory Retrieval: Get personal memory fragments fetch ( \"http://localhost:8002/retrieve_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"What are the user's work habits?\" , top_k : 5 }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); Tool Memory Management import requests # Record tool execution results response = requests . post ( \"http://localhost:8002/add_tool_call_result\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_call_results\" : [ { \"create_time\" : \"2025-10-21 10:30:00\" , \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio tutorial\" , \"max_results\" : 10 }, \"output\" : \"Found 10 relevant results...\" , \"token_cost\" : 150 , \"success\" : True , \"time_cost\" : 2.3 } ] }) # Generate usage guidelines from history response = requests . post ( \"http://localhost:8002/summary_tool_memory\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_names\" : \"web_search\" }) # Retrieve tool guidelines before use response = requests . post ( \"http://localhost:8002/retrieve_tool_memory\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_names\" : \"web_search\" }) curl version # Record tool execution results curl -X POST http://localhost:8002/add_tool_call_result \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_call_results\": [ { \"create_time\": \"2025-10-21 10:30:00\", \"tool_name\": \"web_search\", \"input\": {\"query\": \"Python asyncio tutorial\", \"max_results\": 10}, \"output\": \"Found 10 relevant results...\", \"token_cost\": 150, \"success\": true, \"time_cost\": 2.3 } ] }' # Generate usage guidelines from history curl -X POST http://localhost:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_names\": \"web_search\" }' # Retrieve tool guidelines before use curl -X POST http://localhost:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_names\": \"web_search\" }' Node.js version // Record tool execution results fetch ( \"http://localhost:8002/add_tool_call_result\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_call_results : [ { create_time : \"2025-10-21 10:30:00\" , tool_name : \"web_search\" , input : { query : \"Python asyncio tutorial\" , max_results : 10 }, output : \"Found 10 relevant results...\" , token_cost : 150 , success : true , time_cost : 2.3 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Generate usage guidelines from history fetch ( \"http://localhost:8002/summary_tool_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_names : \"web_search\" }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retrieve tool guidelines before use fetch ( \"http://localhost:8002/retrieve_tool_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_names : \"web_search\" }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); \ud83d\udce6 Ready-to-Use Libraries ReMe provides pre-built memory libraries that agents can immediately use with verified best practices: Available Libraries appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls Quick Usage # Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 }) \ud83e\uddea Experiments \ud83c\udf0d Appworld Experiment We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md . \ud83e\uddca Frozenlake Experiment without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md . \ud83d\udd27 BFCL-V3 Experiment We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%) \ud83d\udee0\ufe0f Tool Memory Benchmark We evaluated Tool Memory effectiveness using a controlled benchmark with three mock search tools using Qwen3-30B-Instruct: Scenario Avg Score Improvement Train (No Memory) 0.650 - Test (No Memory) 0.672 Baseline Test (With Memory) 0.772 +14.88% Key Findings: - Tool Memory enables data-driven tool selection based on historical performance - Success rates improved by ~15% with learned parameter configurations You can find more details in tool_bench.md and the implementation at run_reme_tool_bench.py . \ud83d\udcda Resources Quick Start : Get started quickly with practical examples Tool Memory Demo : Complete lifecycle demonstration of tool memory Tool Memory Benchmark : Evaluate tool memory effectiveness Vector Storage Setup : Configure local/vector databases and usage MCP Guide : Create MCP services Personal Memory , Task Memory & Tool Memory : Operators used in personal memory, task memory and tool memory. You can modify the config to customize the pipelines. Example Collection : Real use cases and best practices Citation @software { ReMe2025 , title = {ReMe: Memory Management Framework for Agents} , author = {Li Yu, Jiaji Deng, Zouying Cao} , url = {https://github.com/modelscope/ReMe} , year = {2025} }","title":"Welcome"},{"location":"#architecture-design","text":"ReMe integrates three complementary memory capabilities: Task Memory/Experience Procedural knowledge reused across agents Success Pattern Recognition : Identify effective strategies and understand their underlying principles Failure Analysis Learning : Learn from mistakes and avoid repeating the same issues Comparative Patterns : Different sampling trajectories provide more valuable memories through comparison Validation Patterns : Confirm the effectiveness of extracted memories through validation modules Learn more about how to use task memory from task memory Personal Memory Contextualized memory for specific users","title":"Architecture Design"},{"location":"#tool-memory","text":"Data-driven tool selection and usage optimization Historical Performance Tracking : Success rates, execution times, and token costs from real usage LLM-as-Judge Evaluation : Qualitative insights on why tools succeed or fail Parameter Optimization : Learn optimal parameter configurations from successful calls Dynamic Guidelines : Transform static tool descriptions into living, learned manuals Learn more about how to use tool memory from tool memory Learn more about how to use personal memory from [ personal memory ]( personal_memory / personal_memory . md )","title":"\ud83d\udd27 Tool Memory"},{"location":"#installation","text":"","title":"Installation"},{"location":"#install-from-pypi-recommended","text":"pip install reme-ai","title":"Install from PyPI (Recommended)"},{"location":"#install-from-source","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe pip install .","title":"Install from Source"},{"location":"#environment-configuration","text":"Copy example.env to .env and modify the corresponding parameters: FLOW_APP_NAME = ReMe FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1","title":"Environment Configuration"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#http-service-startup","text":"reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"HTTP Service Startup"},{"location":"#mcp-server-support","text":"reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"MCP Server Support"},{"location":"#core-api-usage","text":"","title":"Core API Usage"},{"location":"#task-memory-management","text":"import requests # Experience Summarizer: Learn from execution trajectories response = requests . post ( \"http://localhost:8002/summary_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Help me create a project plan\" }], \"score\" : 1.0 } ] }) # Retriever: Get relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"How to efficiently manage project progress?\" , \"top_k\" : 1 }) curl version # Experience Summarizer: Learn from execution trajectories curl -X POST http://localhost:8002/summary_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [{\"role\": \"user\", \"content\": \"Help me create a project plan\"}], \"score\": 1.0} ] }' # Retriever: Get relevant memories curl -X POST http://localhost:8002/retrieve_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"How to efficiently manage project progress?\", \"top_k\": 1 }' Node.js version // Experience Summarizer: Learn from execution trajectories fetch ( \"http://localhost:8002/summary_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [{ role : \"user\" , content : \"Help me create a project plan\" }], score : 1.0 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retriever: Get relevant memories fetch ( \"http://localhost:8002/retrieve_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"How to efficiently manage project progress?\" , top_k : 1 }) }) . then ( response => response . json ()) . then ( data => console . log ( data ));","title":"Task Memory Management"},{"location":"#personal-memory-management","text":"# Memory Integration: Learn from user interactions response = requests . post ( \"http://localhost:8002/summary_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"I like to drink coffee while working in the morning\" }, { \"role\" : \"assistant\" , \"content\" : \"I understand, you prefer to start your workday with coffee to stay energized\" } ] } ] }) # Memory Retrieval: Get personal memory fragments response = requests . post ( \"http://localhost:8002/retrieve_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"What are the user's work habits?\" , \"top_k\" : 5 }) curl version # Memory Integration: Learn from user interactions curl -X POST http://localhost:8002/summary_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [ {\"role\": \"user\", \"content\": \"I like to drink coffee while working in the morning\"}, {\"role\": \"assistant\", \"content\": \"I understand, you prefer to start your workday with coffee to stay energized\"} ]} ] }' # Memory Retrieval: Get personal memory fragments curl -X POST http://localhost:8002/retrieve_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"What are the user' s work habits? \", \" top_k \": 5 }' Node.js version // Memory Integration: Learn from user interactions fetch ( \"http://localhost:8002/summary_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [ { role : \"user\" , content : \"I like to drink coffee while working in the morning\" }, { role : \"assistant\" , content : \"I understand, you prefer to start your workday with coffee to stay energized\" } ]} ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Memory Retrieval: Get personal memory fragments fetch ( \"http://localhost:8002/retrieve_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"What are the user's work habits?\" , top_k : 5 }) }) . then ( response => response . json ()) . then ( data => console . log ( data ));","title":"Personal Memory Management"},{"location":"#tool-memory-management","text":"import requests # Record tool execution results response = requests . post ( \"http://localhost:8002/add_tool_call_result\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_call_results\" : [ { \"create_time\" : \"2025-10-21 10:30:00\" , \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio tutorial\" , \"max_results\" : 10 }, \"output\" : \"Found 10 relevant results...\" , \"token_cost\" : 150 , \"success\" : True , \"time_cost\" : 2.3 } ] }) # Generate usage guidelines from history response = requests . post ( \"http://localhost:8002/summary_tool_memory\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_names\" : \"web_search\" }) # Retrieve tool guidelines before use response = requests . post ( \"http://localhost:8002/retrieve_tool_memory\" , json = { \"workspace_id\" : \"tool_workspace\" , \"tool_names\" : \"web_search\" }) curl version # Record tool execution results curl -X POST http://localhost:8002/add_tool_call_result \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_call_results\": [ { \"create_time\": \"2025-10-21 10:30:00\", \"tool_name\": \"web_search\", \"input\": {\"query\": \"Python asyncio tutorial\", \"max_results\": 10}, \"output\": \"Found 10 relevant results...\", \"token_cost\": 150, \"success\": true, \"time_cost\": 2.3 } ] }' # Generate usage guidelines from history curl -X POST http://localhost:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_names\": \"web_search\" }' # Retrieve tool guidelines before use curl -X POST http://localhost:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"tool_workspace\", \"tool_names\": \"web_search\" }' Node.js version // Record tool execution results fetch ( \"http://localhost:8002/add_tool_call_result\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_call_results : [ { create_time : \"2025-10-21 10:30:00\" , tool_name : \"web_search\" , input : { query : \"Python asyncio tutorial\" , max_results : 10 }, output : \"Found 10 relevant results...\" , token_cost : 150 , success : true , time_cost : 2.3 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Generate usage guidelines from history fetch ( \"http://localhost:8002/summary_tool_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_names : \"web_search\" }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retrieve tool guidelines before use fetch ( \"http://localhost:8002/retrieve_tool_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"tool_workspace\" , tool_names : \"web_search\" }) }) . then ( response => response . json ()) . then ( data => console . log ( data ));","title":"Tool Memory Management"},{"location":"#ready-to-use-libraries","text":"ReMe provides pre-built memory libraries that agents can immediately use with verified best practices:","title":"\ud83d\udce6 Ready-to-Use Libraries"},{"location":"#available-libraries","text":"appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls","title":"Available Libraries"},{"location":"#quick-usage","text":"# Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 })","title":"Quick Usage"},{"location":"#experiments","text":"","title":"\ud83e\uddea Experiments"},{"location":"#appworld-experiment","text":"We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83c\udf0d Appworld Experiment"},{"location":"#frozenlake-experiment","text":"without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83e\uddca Frozenlake Experiment"},{"location":"#bfcl-v3-experiment","text":"We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%)","title":"\ud83d\udd27 BFCL-V3 Experiment"},{"location":"#tool-memory-benchmark","text":"We evaluated Tool Memory effectiveness using a controlled benchmark with three mock search tools using Qwen3-30B-Instruct: Scenario Avg Score Improvement Train (No Memory) 0.650 - Test (No Memory) 0.672 Baseline Test (With Memory) 0.772 +14.88% Key Findings: - Tool Memory enables data-driven tool selection based on historical performance - Success rates improved by ~15% with learned parameter configurations You can find more details in tool_bench.md and the implementation at run_reme_tool_bench.py .","title":"\ud83d\udee0\ufe0f Tool Memory Benchmark"},{"location":"#resources","text":"Quick Start : Get started quickly with practical examples Tool Memory Demo : Complete lifecycle demonstration of tool memory Tool Memory Benchmark : Evaluate tool memory effectiveness Vector Storage Setup : Configure local/vector databases and usage MCP Guide : Create MCP services Personal Memory , Task Memory & Tool Memory : Operators used in personal memory, task memory and tool memory. You can modify the config to customize the pipelines. Example Collection : Real use cases and best practices","title":"\ud83d\udcda Resources"},{"location":"#citation","text":"@software { ReMe2025 , title = {ReMe: Memory Management Framework for Agents} , author = {Li Yu, Jiaji Deng, Zouying Cao} , url = {https://github.com/modelscope/ReMe} , year = {2025} }","title":"Citation"},{"location":"contribution/","text":"Our community thrives on the diverse ideas and contributions of its members. Whether you're fixing a bug, adding a new feature, improving the documentation, or adding examples, your help is welcome. Here's how you can contribute: Report Bugs and Ask For New Features? Did you find a bug or have a feature request? Please first check the issue tracker to see if it has already been reported. If not, feel free to open a new issue. Include as much detail as possible: - A descriptive title - Clear description of the issue - Steps to reproduce the problem - Version of the ReMe you are using - Any relevant code snippets or error messages Contribute to Codebase Fork and Clone the Repository To work on an issue or a new feature, start by forking the ReMe repository and then cloning your fork locally. git clone https://github.com/your-username/ReMe.git cd ReMe Create a New Branch Create a new branch for your work. This helps keep proposed changes organized and separate from the main branch. git checkout -b your-feature-branch-name Making Changes With your new branch checked out, you can now make your changes to the code. Remember to keep your changes as focused as possible. If you're addressing multiple issues or features, it's better to create separate branches and pull requests for each. Commit Your Changes Once you've made your changes, it's time to commit them. Write clear and concise commit messages that explain your changes. git add -A git commit -m \"A brief description of the changes\" Submit a Pull Request When you're ready for feedback, submit a pull request to the ReMe main branch. In your pull request description, explain the changes you've made and any other relevant context. We will review your pull request. This process might involve some discussion, additional changes on your part, or both. Code Review Wait for us to review your pull request. We may suggest some changes or improvements. Keep an eye on your GitHub notifications and be responsive to any feedback.","title":"Contribution Guide"},{"location":"contribution/#report-bugs-and-ask-for-new-features","text":"Did you find a bug or have a feature request? Please first check the issue tracker to see if it has already been reported. If not, feel free to open a new issue. Include as much detail as possible: - A descriptive title - Clear description of the issue - Steps to reproduce the problem - Version of the ReMe you are using - Any relevant code snippets or error messages","title":"Report Bugs and Ask For New Features?"},{"location":"contribution/#contribute-to-codebase","text":"","title":"Contribute to Codebase"},{"location":"contribution/#fork-and-clone-the-repository","text":"To work on an issue or a new feature, start by forking the ReMe repository and then cloning your fork locally. git clone https://github.com/your-username/ReMe.git cd ReMe","title":"Fork and Clone the Repository"},{"location":"contribution/#create-a-new-branch","text":"Create a new branch for your work. This helps keep proposed changes organized and separate from the main branch. git checkout -b your-feature-branch-name","title":"Create a New Branch"},{"location":"contribution/#making-changes","text":"With your new branch checked out, you can now make your changes to the code. Remember to keep your changes as focused as possible. If you're addressing multiple issues or features, it's better to create separate branches and pull requests for each.","title":"Making Changes"},{"location":"contribution/#commit-your-changes","text":"Once you've made your changes, it's time to commit them. Write clear and concise commit messages that explain your changes. git add -A git commit -m \"A brief description of the changes\"","title":"Commit Your Changes"},{"location":"contribution/#submit-a-pull-request","text":"When you're ready for feedback, submit a pull request to the ReMe main branch. In your pull request description, explain the changes you've made and any other relevant context. We will review your pull request. This process might involve some discussion, additional changes on your part, or both.","title":"Submit a Pull Request"},{"location":"contribution/#code-review","text":"Wait for us to review your pull request. We may suggest some changes or improvements. Keep an eye on your GitHub notifications and be responsive to any feedback.","title":"Code Review"},{"location":"future_work/","text":"Planned Features 1. Automatic Tool Exploration Mode Add an automatic tool exploration mode that generates tool memory by: - Automatically discovering and testing available tools Learning tool usage patterns and best practices Building a comprehensive tool memory database from exploration results 2. Desktop Pet Personal Assistant Build a desktop pet personal assistant with: - Interactive desktop companion interface Personalized assistance capabilities Integration with ReMe's memory system 3. Task Memory Research Implementation We are currently working on implementing features based on task memory research papers. Coming soon. 4. Mem-Agent Exploration We are exploring mem-agent to implement agentic memory pathways: - Investigating agent-driven memory management Developing autonomous memory retrieval and storage mechanisms Building more intelligent memory update strategies","title":"Future work"},{"location":"future_work/#planned-features","text":"","title":"Planned Features"},{"location":"future_work/#1-automatic-tool-exploration-mode","text":"Add an automatic tool exploration mode that generates tool memory by: - Automatically discovering and testing available tools Learning tool usage patterns and best practices Building a comprehensive tool memory database from exploration results","title":"1. Automatic Tool Exploration Mode"},{"location":"future_work/#2-desktop-pet-personal-assistant","text":"Build a desktop pet personal assistant with: - Interactive desktop companion interface Personalized assistance capabilities Integration with ReMe's memory system","title":"2. Desktop Pet Personal Assistant"},{"location":"future_work/#3-task-memory-research-implementation","text":"We are currently working on implementing features based on task memory research papers. Coming soon.","title":"3. Task Memory Research Implementation"},{"location":"future_work/#4-mem-agent-exploration","text":"We are exploring mem-agent to implement agentic memory pathways: - Investigating agent-driven memory management Developing autonomous memory retrieval and storage mechanisms Building more intelligent memory update strategies","title":"4. Mem-Agent Exploration"},{"location":"mcp_quick_start/","text":"This guide will help you get started with ReMe using the Model Context Protocol (MCP) interface for seamless integration with MCP-compatible clients. \ud83d\ude80 What You'll Learn How to set up and configure ReMe MCP server How to connect to the server using Python MCP clients How to use task memory operations through MCP How to build memory-enhanced agents with MCP integration \ud83d\udccb Prerequisites Python 3.12+ LLM API access (OpenAI or compatible) Embedding model API access MCP-compatible client (Claude Desktop, or custom MCP client) \ud83d\udee0\ufe0f Installation Option 1: Install from PyPI (Recommended) pip install reme-ai Option 2: Install from Source git clone https://github.com/modelscope/ReMe.git cd ReMe pip install . \u2699\ufe0f Environment Setup Create a .env file in your project directory: FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 \ud83d\ude80 Building an MCP Server with ReMe ReMe provides a flexible framework for building MCP servers that can communicate using either STDIO or SSE (Server-Sent Events) transport protocols. Starting the MCP Server Option 1: STDIO Transport (Recommended for MCP clients) reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Option 2: SSE Transport (Server-Sent Events) reme \\ backend = mcp \\ mcp.transport = sse \\ http_service.port = 8001 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local The SSE server will start on http://localhost:8002/sse Configuring MCP Server for Claude Desktop To integrate with Claude Desktop, add the following configuration to your claude_desktop_config.json : { \"mcpServers\" : { \"reme\" : { \"command\" : \"reme\" , \"args\" : [ \"backend=mcp\" , \"mcp.transport=stdio\" , \"llm.default.model_name=qwen3-30b-a3b-thinking-2507\" , \"embedding_model.default.model_name=text-embedding-v4\" , \"vector_store.default.backend=local_file\" ] } } } This configuration: Registers a new MCP server named \"reme\" Specifies the command to launch the server ( reme ) Configures the server to use STDIO transport Sets the LLM and embedding models to use Configures the vector store backend Advanced Server Configuration Options For more advanced use cases, you can configure the server with additional parameters: # Full configuration example reme \\ backend = mcp \\ mcp.transport = stdio \\ http_service.host = 0 .0.0.0 \\ http_service.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = elasticsearch \\ \ud83d\udd0c Using Python Client to Call MCP Services The ReMe framework provides a Python client for interacting with MCP services. This section focuses specifically on using the summary_task_memory and retrieve_task_memory tools. Setting Up the Python MCP Client First, install the required packages: pip install fastmcp dotenv Then, create a basic client connection: import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # MCP server URL (for SSE transport) MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"my_workspace\" async def main (): async with Client ( MCP_URL ) as client : # Your MCP operations will go here pass if __name__ == \"__main__\" : asyncio . run ( main ()) Using the Task Memory Summarizer The summary_task_memory tool transforms conversation trajectories into valuable task memories: async def run_summary ( client , messages ): \"\"\" Generate a summary of conversation messages and create task memories Args: client: MCP client instance messages: List of message objects from a conversation Returns: None \"\"\" try : result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract memory list from response memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) print ( f \"Created memories: { memory_list } \" ) # Optionally save memories to file with open ( \"task_memory.jsonl\" , \"w\" ) as f : f . write ( json . dumps ( memory_list , indent = 2 , ensure_ascii = False )) except Exception as e : print ( f \"Error running summary: { e } \" ) Using the Task Memory Retriever The retrieve_task_memory tool allows you to retrieve relevant memories based on a query: async def run_retrieve ( client , query ): \"\"\" Retrieve relevant task memories based on a query Args: client: MCP client instance query: The query to retrieve relevant memories Returns: String containing the retrieved memory answer \"\"\" try : result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"query\" : query , } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract and return the answer answer = response_data . get ( \"answer\" , \"\" ) print ( f \"Retrieved memory: { answer } \" ) return answer except Exception as e : print ( f \"Error retrieving memory: { e } \" ) return \"\" Complete Memory-Augmented Agent Example Here's a complete example showing how to build a memory-augmented agent using the MCP client: import json import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # API configuration MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"test_workspace\" async def run_agent ( client , query ): \"\"\"Run the agent with a specific query\"\"\" result = await client . call_tool ( \"react\" , arguments = { \"query\" : query } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) messages = response_data . get ( \"messages\" , []) return messages async def run_summary ( client , messages ): \"\"\"Generate task memories from conversation\"\"\" result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) response_data = json . loads ( result . content ) memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) return memory_list async def run_retrieve ( client , query ): \"\"\"Retrieve relevant task memories\"\"\" result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query , } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) return answer async def memory_augmented_workflow (): \"\"\"Complete memory-augmented agent workflow\"\"\" query1 = \"Analyze Xiaomi Corporation\" query2 = \"Analyze the company Tesla.\" async with Client ( MCP_URL ) as client : # Step 1: Build initial memories with query2 print ( f \"Building memories with: ' { query2 } '\" ) messages = await run_agent ( client , query = query2 ) # Step 2: Summarize conversation to create memories print ( \"Creating memories from conversation\" ) memory_list = await run_summary ( client , messages ) print ( f \"Created { len ( memory_list ) } memories\" ) # Step 3: Retrieve relevant memories for query1 print ( f \"Retrieving memories for: ' { query1 } '\" ) retrieved_memory = await run_retrieve ( client , query1 ) # Step 4: Run agent with memory-augmented query print ( \"Running memory-augmented agent\" ) augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query1 } \" final_messages = await run_agent ( client , query = augmented_query ) # Extract the agent's final answer final_answer = \"\" for msg in final_messages : if msg . get ( \"role\" ) == \"assistant\" and msg . get ( \"content\" ): final_answer = msg . get ( \"content\" ) break print ( f \"Memory-augmented response: { final_answer } \" ) # Run the workflow if __name__ == \"__main__\" : asyncio . run ( memory_augmented_workflow ()) Managing Vector Store with MCP You can also manage your vector store through MCP: async def manage_vector_store ( client ): # Delete a workspace await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" , } ) # Dump memories to disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./backups/\" , } ) # Load memories from disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./backups/\" , } ) \ud83d\udc1b Common Issues and Troubleshooting MCP Server Won't Start Check if the required ports are available (for SSE transport) Verify your API keys in .env file Ensure Python version is 3.12+ Check MCP transport configuration MCP Client Connection Issues For STDIO: Ensure the command path is correct in your MCP client config For SSE: Verify the server URL and port accessibility Check firewall settings for SSE connections No Memories Retrieved Make sure you've run the summarizer tool first to create memories Check if workspace_id matches between operations Verify vector store backend is properly configured API Connection Errors Confirm LLM_BASE_URL and API keys are correct Test API access independently Check network connectivity","title":"MCP"},{"location":"mcp_quick_start/#what-youll-learn","text":"How to set up and configure ReMe MCP server How to connect to the server using Python MCP clients How to use task memory operations through MCP How to build memory-enhanced agents with MCP integration","title":"\ud83d\ude80 What You'll Learn"},{"location":"mcp_quick_start/#prerequisites","text":"Python 3.12+ LLM API access (OpenAI or compatible) Embedding model API access MCP-compatible client (Claude Desktop, or custom MCP client)","title":"\ud83d\udccb Prerequisites"},{"location":"mcp_quick_start/#installation","text":"","title":"\ud83d\udee0\ufe0f Installation"},{"location":"mcp_quick_start/#option-1-install-from-pypi-recommended","text":"pip install reme-ai","title":"Option 1: Install from PyPI (Recommended)"},{"location":"mcp_quick_start/#option-2-install-from-source","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe pip install .","title":"Option 2: Install from Source"},{"location":"mcp_quick_start/#environment-setup","text":"Create a .env file in your project directory: FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1","title":"\u2699\ufe0f Environment Setup"},{"location":"mcp_quick_start/#building-an-mcp-server-with-reme","text":"ReMe provides a flexible framework for building MCP servers that can communicate using either STDIO or SSE (Server-Sent Events) transport protocols.","title":"\ud83d\ude80 Building an MCP Server with ReMe"},{"location":"mcp_quick_start/#starting-the-mcp-server","text":"","title":"Starting the MCP Server"},{"location":"mcp_quick_start/#option-1-stdio-transport-recommended-for-mcp-clients","text":"reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"Option 1: STDIO Transport (Recommended for MCP clients)"},{"location":"mcp_quick_start/#option-2-sse-transport-server-sent-events","text":"reme \\ backend = mcp \\ mcp.transport = sse \\ http_service.port = 8001 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local The SSE server will start on http://localhost:8002/sse","title":"Option 2: SSE Transport (Server-Sent Events)"},{"location":"mcp_quick_start/#configuring-mcp-server-for-claude-desktop","text":"To integrate with Claude Desktop, add the following configuration to your claude_desktop_config.json : { \"mcpServers\" : { \"reme\" : { \"command\" : \"reme\" , \"args\" : [ \"backend=mcp\" , \"mcp.transport=stdio\" , \"llm.default.model_name=qwen3-30b-a3b-thinking-2507\" , \"embedding_model.default.model_name=text-embedding-v4\" , \"vector_store.default.backend=local_file\" ] } } } This configuration: Registers a new MCP server named \"reme\" Specifies the command to launch the server ( reme ) Configures the server to use STDIO transport Sets the LLM and embedding models to use Configures the vector store backend","title":"Configuring MCP Server for Claude Desktop"},{"location":"mcp_quick_start/#advanced-server-configuration-options","text":"For more advanced use cases, you can configure the server with additional parameters: # Full configuration example reme \\ backend = mcp \\ mcp.transport = stdio \\ http_service.host = 0 .0.0.0 \\ http_service.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = elasticsearch \\","title":"Advanced Server Configuration Options"},{"location":"mcp_quick_start/#using-python-client-to-call-mcp-services","text":"The ReMe framework provides a Python client for interacting with MCP services. This section focuses specifically on using the summary_task_memory and retrieve_task_memory tools.","title":"\ud83d\udd0c Using Python Client to Call MCP Services"},{"location":"mcp_quick_start/#setting-up-the-python-mcp-client","text":"First, install the required packages: pip install fastmcp dotenv Then, create a basic client connection: import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # MCP server URL (for SSE transport) MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"my_workspace\" async def main (): async with Client ( MCP_URL ) as client : # Your MCP operations will go here pass if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Setting Up the Python MCP Client"},{"location":"mcp_quick_start/#using-the-task-memory-summarizer","text":"The summary_task_memory tool transforms conversation trajectories into valuable task memories: async def run_summary ( client , messages ): \"\"\" Generate a summary of conversation messages and create task memories Args: client: MCP client instance messages: List of message objects from a conversation Returns: None \"\"\" try : result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract memory list from response memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) print ( f \"Created memories: { memory_list } \" ) # Optionally save memories to file with open ( \"task_memory.jsonl\" , \"w\" ) as f : f . write ( json . dumps ( memory_list , indent = 2 , ensure_ascii = False )) except Exception as e : print ( f \"Error running summary: { e } \" )","title":"Using the Task Memory Summarizer"},{"location":"mcp_quick_start/#using-the-task-memory-retriever","text":"The retrieve_task_memory tool allows you to retrieve relevant memories based on a query: async def run_retrieve ( client , query ): \"\"\" Retrieve relevant task memories based on a query Args: client: MCP client instance query: The query to retrieve relevant memories Returns: String containing the retrieved memory answer \"\"\" try : result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"query\" : query , } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract and return the answer answer = response_data . get ( \"answer\" , \"\" ) print ( f \"Retrieved memory: { answer } \" ) return answer except Exception as e : print ( f \"Error retrieving memory: { e } \" ) return \"\"","title":"Using the Task Memory Retriever"},{"location":"mcp_quick_start/#complete-memory-augmented-agent-example","text":"Here's a complete example showing how to build a memory-augmented agent using the MCP client: import json import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # API configuration MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"test_workspace\" async def run_agent ( client , query ): \"\"\"Run the agent with a specific query\"\"\" result = await client . call_tool ( \"react\" , arguments = { \"query\" : query } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) messages = response_data . get ( \"messages\" , []) return messages async def run_summary ( client , messages ): \"\"\"Generate task memories from conversation\"\"\" result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) response_data = json . loads ( result . content ) memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) return memory_list async def run_retrieve ( client , query ): \"\"\"Retrieve relevant task memories\"\"\" result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query , } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) return answer async def memory_augmented_workflow (): \"\"\"Complete memory-augmented agent workflow\"\"\" query1 = \"Analyze Xiaomi Corporation\" query2 = \"Analyze the company Tesla.\" async with Client ( MCP_URL ) as client : # Step 1: Build initial memories with query2 print ( f \"Building memories with: ' { query2 } '\" ) messages = await run_agent ( client , query = query2 ) # Step 2: Summarize conversation to create memories print ( \"Creating memories from conversation\" ) memory_list = await run_summary ( client , messages ) print ( f \"Created { len ( memory_list ) } memories\" ) # Step 3: Retrieve relevant memories for query1 print ( f \"Retrieving memories for: ' { query1 } '\" ) retrieved_memory = await run_retrieve ( client , query1 ) # Step 4: Run agent with memory-augmented query print ( \"Running memory-augmented agent\" ) augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query1 } \" final_messages = await run_agent ( client , query = augmented_query ) # Extract the agent's final answer final_answer = \"\" for msg in final_messages : if msg . get ( \"role\" ) == \"assistant\" and msg . get ( \"content\" ): final_answer = msg . get ( \"content\" ) break print ( f \"Memory-augmented response: { final_answer } \" ) # Run the workflow if __name__ == \"__main__\" : asyncio . run ( memory_augmented_workflow ())","title":"Complete Memory-Augmented Agent Example"},{"location":"mcp_quick_start/#managing-vector-store-with-mcp","text":"You can also manage your vector store through MCP: async def manage_vector_store ( client ): # Delete a workspace await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" , } ) # Dump memories to disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./backups/\" , } ) # Load memories from disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./backups/\" , } )","title":"Managing Vector Store with MCP"},{"location":"mcp_quick_start/#common-issues-and-troubleshooting","text":"","title":"\ud83d\udc1b Common Issues and Troubleshooting"},{"location":"mcp_quick_start/#mcp-server-wont-start","text":"Check if the required ports are available (for SSE transport) Verify your API keys in .env file Ensure Python version is 3.12+ Check MCP transport configuration","title":"MCP Server Won't Start"},{"location":"mcp_quick_start/#mcp-client-connection-issues","text":"For STDIO: Ensure the command path is correct in your MCP client config For SSE: Verify the server URL and port accessibility Check firewall settings for SSE connections","title":"MCP Client Connection Issues"},{"location":"mcp_quick_start/#no-memories-retrieved","text":"Make sure you've run the summarizer tool first to create memories Check if workspace_id matches between operations Verify vector store backend is properly configured","title":"No Memories Retrieved"},{"location":"mcp_quick_start/#api-connection-errors","text":"Confirm LLM_BASE_URL and API keys are correct Test API access independently Check network connectivity","title":"API Connection Errors"},{"location":"vector_store_api_guide/","text":"This guide covers the vector store implementations available in flowllm, their APIs, and how to use them effectively. \ud83d\udccb Overview flowllm provides multiple vector store backends for different use cases: LocalVectorStore ( backend=local ) - \ud83d\udcc1 Simple file-based storage for development and small datasets ChromaVectorStore ( backend=chroma ) - \ud83d\udd2e Embedded vector database for moderate scale EsVectorStore ( backend=elasticsearch ) - \ud83d\udd0d Elasticsearch-based storage for production and large scale MemoryVectorStore ( backend=memory ) - \u26a1 In-memory storage for ultra-fast access and testing All vector stores implement the BaseVectorStore interface, providing a consistent API across implementations. \ud83d\udcca Comparison Table Feature LocalVectorStore ChromaVectorStore EsVectorStore MemoryVectorStore Storage File (JSONL) Embedded DB Elasticsearch In-Memory Performance Medium Good Excellent Ultra-Fast Scalability < 10K vectors < 1M vectors > 1M vectors < 1M vectors Persistence \u2705 Auto \u2705 Auto \u2705 Auto \u26a0\ufe0f Manual Setup Complexity \ud83d\udfe2 Simple \ud83d\udfe1 Medium \ud83d\udd34 Complex \ud83d\udfe2 Simple Dependencies None ChromaDB Elasticsearch None Filtering \u274c Basic \u2705 Metadata \u2705 Advanced \u274c Basic Concurrency \u274c Limited \u2705 Good \u2705 Excellent \u274c Single Process Best For Development Local Apps Production Testing \ud83d\udd04 Common API Methods All vector store implementations share these core methods: \ud83d\udd04 Async Support All vector stores provide both synchronous and asynchronous versions of every method: # Synchronous methods store . search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) store . insert ( nodes , workspace_id = \"workspace\" ) # Asynchronous methods (with async_ prefix) await store . async_search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) await store . async_insert ( nodes , workspace_id = \"workspace\" ) Workspace Management # Check if workspace exists store . exist_workspace ( workspace_id : str ) -> bool # Create a new workspace store . create_workspace ( workspace_id : str , ** kwargs ) # Delete a workspace store . delete_workspace ( workspace_id : str , ** kwargs ) # Copy a workspace store . copy_workspace ( src_workspace_id : str , dest_workspace_id : str , ** kwargs ) Data Operations # Insert nodes (single or list) store . insert ( nodes : VectorNode | List [ VectorNode ], workspace_id : str , ** kwargs ) # Delete nodes by ID store . delete ( node_ids : str | List [ str ], workspace_id : str , ** kwargs ) # Search for similar nodes store . search ( query : str , workspace_id : str , top_k : int = 1 , ** kwargs ) -> List [ VectorNode ] # Iterate through workspace nodes for node in store . iter_workspace_nodes ( workspace_id : str , ** kwargs ): # Process each node Import/Export # Export workspace to file store . dump_workspace ( workspace_id : str , path : str | Path = \"\" , callback_fn = None , ** kwargs ) # Import workspace from file store . load_workspace ( workspace_id : str , path : str | Path = \"\" , nodes : List [ VectorNode ] = None , callback_fn = None , ** kwargs ) \u26a1 Vector Store Implementations 1. \ud83d\udcc1 LocalVectorStore ( backend=local ) A simple file-based vector store that saves data to local JSONL files. \ud83d\udca1 When to Use Development and testing - No external dependencies required \ud83d\udee0\ufe0f Small datasets - Suitable for datasets with < 10,000 vectors \ud83d\udcca Single-user applications - Limited concurrent access support \ud83d\udc64 \u2699\ufe0f Configuration from flowllm.storage.vector_store import LocalVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables (for API keys) load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./file_vector_store\" , # Directory to store JSONL files batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode # Create workspace workspace_id = \"my_workspace\" vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Artificial intelligence is revolutionizing technology\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article1\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Machine learning enables data-driven insights\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article2\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"What is AI?\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) 2. \ud83d\udd2e ChromaVectorStore ( backend=chroma ) An embedded vector database that provides persistent storage with advanced features. \ud83d\udca1 When to Use Local development with persistence requirements \ud83c\udfe0 Medium-scale applications (10K - 1M vectors) \ud83d\udcc8 Applications requiring metadata filtering \ud83d\udd0d \u2699\ufe0f Configuration from flowllm.storage.vector_store import ChromaVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = ChromaVectorStore ( embedding_model = embedding_model , store_dir = \"./chroma_vector_store\" , # Directory for Chroma database batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode workspace_id = \"chroma_workspace\" # Check if workspace exists and create if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with metadata nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Deep learning models require large datasets\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"advanced\" , \"topic\" : \"deep_learning\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"intermediate\" , \"topic\" : \"transformers\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"deep learning\" , workspace_id , top_k = 5 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) 3. \ud83d\udd0d EsVectorStore ( backend=elasticsearch ) Production-grade vector search using Elasticsearch with advanced filtering and scaling capabilities. \ud83d\udca1 When to Use Production environments requiring high availability \ud83c\udfed Large-scale applications (1M+ vectors) \ud83d\ude80 Complex filtering requirements on metadata \ud83c\udfaf \ud83d\udee0\ufe0f Setup Elasticsearch Before using EsVectorStore, set up Elasticsearch: Option 1: Docker Run # Pull the latest Elasticsearch image docker pull docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 # Run Elasticsearch container docker run -p 9200 :9200 \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"xpack.license.self_generated.type=trial\" \\ -e \"http.host=0.0.0.0\" \\ docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 Environment Configuration export FLOW_ES_HOSTS = http://localhost:9200 \u2699\ufe0f Configuration from flowllm.storage.vector_store import EsVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env import os # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = EsVectorStore ( embedding_model = embedding_model , hosts = os . getenv ( \"FLOW_ES_HOSTS\" , \"http://localhost:9200\" ), # Elasticsearch hosts basic_auth = None , # (\"username\", \"password\") for auth batch_size = 1024 # Batch size for bulk operations ) \ud83c\udfaf Advanced Filtering EsVectorStore supports advanced filtering capabilities through the filter_dict parameter: # Term filters (exact match) term_filter = { \"category\" : \"technology\" , \"author\" : \"research_team\" } # Range filters (numeric and date ranges) range_filter = { \"score\" : { \"gte\" : 0.8 }, # Score >= 0.8 \"confidence\" : { \"gte\" : 0.5 , \"lte\" : 0.9 }, # Between 0.5 and 0.9 \"timestamp\" : { \"gte\" : \"2024-01-01\" , \"lte\" : \"2024-12-31\" } } # Combined filters (filters are combined with AND logic) combined_filter = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } # Search with filters applied results = vector_store . search ( \"machine learning\" , workspace_id , top_k = 10 , filter_dict = combined_filter ) \u26a1 Performance Optimization # Refresh index for immediate availability (useful after bulk inserts) vector_store . insert ( nodes , workspace_id , refresh = True ) # Auto-refresh vector_store . refresh ( workspace_id ) # Manual refresh # Bulk operations with custom batch size vector_store . insert ( large_node_list , workspace_id , refresh = False ) # Skip refresh for speed vector_store . refresh ( workspace_id ) # Refresh once after all inserts \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode # Define workspace workspace_id = \"production_workspace\" # Create workspace if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with rich metadata nodes = [ VectorNode ( unique_id = \"doc1\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"subcategory\" : \"NLP\" , \"author\" : \"research_team\" , \"timestamp\" : \"2024-01-15\" , \"confidence\" : 0.95 , \"tags\" : [ \"transformer\" , \"nlp\" , \"attention\" ] } ) ] # Insert with refresh for immediate availability vector_store . insert ( nodes , workspace_id , refresh = True ) # Advanced search with filters filter_dict = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } results = vector_store . search ( \"transformer models\" , workspace_id , top_k = 5 , filter_dict = filter_dict ) for result in results : print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) 4. \u26a1 MemoryVectorStore ( backend=memory ) An ultra-fast in-memory vector store that keeps all data in RAM for maximum performance. \ud83d\udca1 When to Use Testing and development - Fastest possible operations for unit tests \ud83e\uddea Small to medium datasets that fit in memory (< 1M vectors) \ud83d\udcbe Applications requiring ultra-low latency search operations \u26a1 Temporary workspaces that don't need persistence \ud83d\ude80 \u2699\ufe0f Configuration from flowllm.storage.vector_store import MemoryVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = MemoryVectorStore ( embedding_model = embedding_model , store_dir = \"./memory_vector_store\" , # Directory for backup/restore operations batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode workspace_id = \"memory_workspace\" # Create workspace in memory vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"mem_node1\" , workspace_id = workspace_id , content = \"Memory stores provide ultra-fast access to data\" , metadata = { \"category\" : \"performance\" , \"type\" : \"memory\" , \"speed\" : \"ultra_fast\" } ), VectorNode ( unique_id = \"mem_node2\" , workspace_id = workspace_id , content = \"In-memory databases excel at low-latency operations\" , metadata = { \"category\" : \"performance\" , \"type\" : \"database\" , \"latency\" : \"low\" } ) ] # Insert nodes (stored in memory) vector_store . insert ( nodes , workspace_id ) # Ultra-fast search results = vector_store . search ( \"fast memory access\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) # Optional: Save to disk for backup vector_store . dump_workspace ( workspace_id , path = \"./backup\" ) # Optional: Load from disk to memory vector_store . load_workspace ( workspace_id , path = \"./backup\" ) \u26a1 Performance Benefits Zero I/O latency - All operations happen in RAM Instant search results - No disk or network overhead Perfect for testing - Fast setup and teardown Memory efficient - Only stores what you need \ud83d\udea8 Important Notes Data is volatile - Lost when process ends unless explicitly saved Memory usage - Entire dataset must fit in available RAM No persistence - Use dump_workspace() to save to disk Single process - Not suitable for distributed applications \ud83d\udcdd Working with VectorNode The VectorNode class is the fundamental data unit for all vector stores: from flowllm.schema.vector_node import VectorNode # Create a node node = VectorNode ( unique_id = \"unique_identifier\" , # Unique ID for the node (required) workspace_id = \"my_workspace\" , # Workspace ID (required) content = \"Text content to embed\" , # Content to be embedded (required) metadata = { # Optional metadata \"source\" : \"document1\" , \"category\" : \"technology\" , \"timestamp\" : \"2024-08-29\" }, vector = None # Vector will be generated automatically if None ) \ud83d\udd04 Import/Export Example Export and import workspaces for backup or transfer: # Export workspace to file vector_store . dump_workspace ( workspace_id = \"my_workspace\" , path = \"./backup_data\" # Directory to store the exported data ) # Import workspace from file vector_store . load_workspace ( workspace_id = \"new_workspace\" , path = \"./backup_data\" # Directory containing the exported data ) # Copy workspace within the same store vector_store . copy_workspace ( src_workspace_id = \"original_workspace\" , dest_workspace_id = \"copied_workspace\" ) \ud83e\udde9 Integration with Embedding Models All vector stores require an embedding model to function: from flowllm.embedding_model import OpenAICompatibleEmbeddingModel # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , # Embedding dimensions model_name = \"text-embedding-v4\" , # Model name batch_size = 32 # Batch size for embedding generation ) # Pass to vector store (example with LocalVectorStore) # You can also use: ChromaVectorStore, EsVectorStore, or MemoryVectorStore vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./vector_store\" ) \ud83c\udf89 This guide provides everything you need to work with vector stores in flowllm. Choose the implementation that best fits your use case and scale up as needed! \u2728","title":"Vector Store"},{"location":"vector_store_api_guide/#overview","text":"flowllm provides multiple vector store backends for different use cases: LocalVectorStore ( backend=local ) - \ud83d\udcc1 Simple file-based storage for development and small datasets ChromaVectorStore ( backend=chroma ) - \ud83d\udd2e Embedded vector database for moderate scale EsVectorStore ( backend=elasticsearch ) - \ud83d\udd0d Elasticsearch-based storage for production and large scale MemoryVectorStore ( backend=memory ) - \u26a1 In-memory storage for ultra-fast access and testing All vector stores implement the BaseVectorStore interface, providing a consistent API across implementations.","title":"\ud83d\udccb Overview"},{"location":"vector_store_api_guide/#comparison-table","text":"Feature LocalVectorStore ChromaVectorStore EsVectorStore MemoryVectorStore Storage File (JSONL) Embedded DB Elasticsearch In-Memory Performance Medium Good Excellent Ultra-Fast Scalability < 10K vectors < 1M vectors > 1M vectors < 1M vectors Persistence \u2705 Auto \u2705 Auto \u2705 Auto \u26a0\ufe0f Manual Setup Complexity \ud83d\udfe2 Simple \ud83d\udfe1 Medium \ud83d\udd34 Complex \ud83d\udfe2 Simple Dependencies None ChromaDB Elasticsearch None Filtering \u274c Basic \u2705 Metadata \u2705 Advanced \u274c Basic Concurrency \u274c Limited \u2705 Good \u2705 Excellent \u274c Single Process Best For Development Local Apps Production Testing","title":"\ud83d\udcca Comparison Table"},{"location":"vector_store_api_guide/#common-api-methods","text":"All vector store implementations share these core methods:","title":"\ud83d\udd04 Common API Methods"},{"location":"vector_store_api_guide/#async-support","text":"All vector stores provide both synchronous and asynchronous versions of every method: # Synchronous methods store . search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) store . insert ( nodes , workspace_id = \"workspace\" ) # Asynchronous methods (with async_ prefix) await store . async_search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) await store . async_insert ( nodes , workspace_id = \"workspace\" )","title":"\ud83d\udd04 Async Support"},{"location":"vector_store_api_guide/#workspace-management","text":"# Check if workspace exists store . exist_workspace ( workspace_id : str ) -> bool # Create a new workspace store . create_workspace ( workspace_id : str , ** kwargs ) # Delete a workspace store . delete_workspace ( workspace_id : str , ** kwargs ) # Copy a workspace store . copy_workspace ( src_workspace_id : str , dest_workspace_id : str , ** kwargs )","title":"Workspace Management"},{"location":"vector_store_api_guide/#data-operations","text":"# Insert nodes (single or list) store . insert ( nodes : VectorNode | List [ VectorNode ], workspace_id : str , ** kwargs ) # Delete nodes by ID store . delete ( node_ids : str | List [ str ], workspace_id : str , ** kwargs ) # Search for similar nodes store . search ( query : str , workspace_id : str , top_k : int = 1 , ** kwargs ) -> List [ VectorNode ] # Iterate through workspace nodes for node in store . iter_workspace_nodes ( workspace_id : str , ** kwargs ): # Process each node","title":"Data Operations"},{"location":"vector_store_api_guide/#importexport","text":"# Export workspace to file store . dump_workspace ( workspace_id : str , path : str | Path = \"\" , callback_fn = None , ** kwargs ) # Import workspace from file store . load_workspace ( workspace_id : str , path : str | Path = \"\" , nodes : List [ VectorNode ] = None , callback_fn = None , ** kwargs )","title":"Import/Export"},{"location":"vector_store_api_guide/#vector-store-implementations","text":"","title":"\u26a1 Vector Store Implementations"},{"location":"vector_store_api_guide/#1-localvectorstore-backendlocal","text":"A simple file-based vector store that saves data to local JSONL files.","title":"1. \ud83d\udcc1 LocalVectorStore (backend=local)"},{"location":"vector_store_api_guide/#when-to-use","text":"Development and testing - No external dependencies required \ud83d\udee0\ufe0f Small datasets - Suitable for datasets with < 10,000 vectors \ud83d\udcca Single-user applications - Limited concurrent access support \ud83d\udc64","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration","text":"from flowllm.storage.vector_store import LocalVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables (for API keys) load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./file_vector_store\" , # Directory to store JSONL files batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage","text":"from flowllm.schema.vector_node import VectorNode # Create workspace workspace_id = \"my_workspace\" vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Artificial intelligence is revolutionizing technology\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article1\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Machine learning enables data-driven insights\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article2\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"What is AI?\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#2-chromavectorstore-backendchroma","text":"An embedded vector database that provides persistent storage with advanced features.","title":"2. \ud83d\udd2e ChromaVectorStore (backend=chroma)"},{"location":"vector_store_api_guide/#when-to-use_1","text":"Local development with persistence requirements \ud83c\udfe0 Medium-scale applications (10K - 1M vectors) \ud83d\udcc8 Applications requiring metadata filtering \ud83d\udd0d","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration_1","text":"from flowllm.storage.vector_store import ChromaVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = ChromaVectorStore ( embedding_model = embedding_model , store_dir = \"./chroma_vector_store\" , # Directory for Chroma database batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage_1","text":"from flowllm.schema.vector_node import VectorNode workspace_id = \"chroma_workspace\" # Check if workspace exists and create if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with metadata nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Deep learning models require large datasets\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"advanced\" , \"topic\" : \"deep_learning\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"intermediate\" , \"topic\" : \"transformers\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"deep learning\" , workspace_id , top_k = 5 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#3-esvectorstore-backendelasticsearch","text":"Production-grade vector search using Elasticsearch with advanced filtering and scaling capabilities.","title":"3. \ud83d\udd0d EsVectorStore (backend=elasticsearch)"},{"location":"vector_store_api_guide/#when-to-use_2","text":"Production environments requiring high availability \ud83c\udfed Large-scale applications (1M+ vectors) \ud83d\ude80 Complex filtering requirements on metadata \ud83c\udfaf","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#setup-elasticsearch","text":"Before using EsVectorStore, set up Elasticsearch:","title":"\ud83d\udee0\ufe0f Setup Elasticsearch"},{"location":"vector_store_api_guide/#option-1-docker-run","text":"# Pull the latest Elasticsearch image docker pull docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 # Run Elasticsearch container docker run -p 9200 :9200 \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"xpack.license.self_generated.type=trial\" \\ -e \"http.host=0.0.0.0\" \\ docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0","title":"Option 1: Docker Run"},{"location":"vector_store_api_guide/#environment-configuration","text":"export FLOW_ES_HOSTS = http://localhost:9200","title":"Environment Configuration"},{"location":"vector_store_api_guide/#configuration_2","text":"from flowllm.storage.vector_store import EsVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env import os # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = EsVectorStore ( embedding_model = embedding_model , hosts = os . getenv ( \"FLOW_ES_HOSTS\" , \"http://localhost:9200\" ), # Elasticsearch hosts basic_auth = None , # (\"username\", \"password\") for auth batch_size = 1024 # Batch size for bulk operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#advanced-filtering","text":"EsVectorStore supports advanced filtering capabilities through the filter_dict parameter: # Term filters (exact match) term_filter = { \"category\" : \"technology\" , \"author\" : \"research_team\" } # Range filters (numeric and date ranges) range_filter = { \"score\" : { \"gte\" : 0.8 }, # Score >= 0.8 \"confidence\" : { \"gte\" : 0.5 , \"lte\" : 0.9 }, # Between 0.5 and 0.9 \"timestamp\" : { \"gte\" : \"2024-01-01\" , \"lte\" : \"2024-12-31\" } } # Combined filters (filters are combined with AND logic) combined_filter = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } # Search with filters applied results = vector_store . search ( \"machine learning\" , workspace_id , top_k = 10 , filter_dict = combined_filter )","title":"\ud83c\udfaf Advanced Filtering"},{"location":"vector_store_api_guide/#performance-optimization","text":"# Refresh index for immediate availability (useful after bulk inserts) vector_store . insert ( nodes , workspace_id , refresh = True ) # Auto-refresh vector_store . refresh ( workspace_id ) # Manual refresh # Bulk operations with custom batch size vector_store . insert ( large_node_list , workspace_id , refresh = False ) # Skip refresh for speed vector_store . refresh ( workspace_id ) # Refresh once after all inserts","title":"\u26a1 Performance Optimization"},{"location":"vector_store_api_guide/#example-usage_2","text":"from flowllm.schema.vector_node import VectorNode # Define workspace workspace_id = \"production_workspace\" # Create workspace if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with rich metadata nodes = [ VectorNode ( unique_id = \"doc1\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"subcategory\" : \"NLP\" , \"author\" : \"research_team\" , \"timestamp\" : \"2024-01-15\" , \"confidence\" : 0.95 , \"tags\" : [ \"transformer\" , \"nlp\" , \"attention\" ] } ) ] # Insert with refresh for immediate availability vector_store . insert ( nodes , workspace_id , refresh = True ) # Advanced search with filters filter_dict = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } results = vector_store . search ( \"transformer models\" , workspace_id , top_k = 5 , filter_dict = filter_dict ) for result in results : print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#4-memoryvectorstore-backendmemory","text":"An ultra-fast in-memory vector store that keeps all data in RAM for maximum performance.","title":"4. \u26a1 MemoryVectorStore (backend=memory)"},{"location":"vector_store_api_guide/#when-to-use_3","text":"Testing and development - Fastest possible operations for unit tests \ud83e\uddea Small to medium datasets that fit in memory (< 1M vectors) \ud83d\udcbe Applications requiring ultra-low latency search operations \u26a1 Temporary workspaces that don't need persistence \ud83d\ude80","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration_3","text":"from flowllm.storage.vector_store import MemoryVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = MemoryVectorStore ( embedding_model = embedding_model , store_dir = \"./memory_vector_store\" , # Directory for backup/restore operations batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage_3","text":"from flowllm.schema.vector_node import VectorNode workspace_id = \"memory_workspace\" # Create workspace in memory vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"mem_node1\" , workspace_id = workspace_id , content = \"Memory stores provide ultra-fast access to data\" , metadata = { \"category\" : \"performance\" , \"type\" : \"memory\" , \"speed\" : \"ultra_fast\" } ), VectorNode ( unique_id = \"mem_node2\" , workspace_id = workspace_id , content = \"In-memory databases excel at low-latency operations\" , metadata = { \"category\" : \"performance\" , \"type\" : \"database\" , \"latency\" : \"low\" } ) ] # Insert nodes (stored in memory) vector_store . insert ( nodes , workspace_id ) # Ultra-fast search results = vector_store . search ( \"fast memory access\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) # Optional: Save to disk for backup vector_store . dump_workspace ( workspace_id , path = \"./backup\" ) # Optional: Load from disk to memory vector_store . load_workspace ( workspace_id , path = \"./backup\" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#performance-benefits","text":"Zero I/O latency - All operations happen in RAM Instant search results - No disk or network overhead Perfect for testing - Fast setup and teardown Memory efficient - Only stores what you need","title":"\u26a1 Performance Benefits"},{"location":"vector_store_api_guide/#important-notes","text":"Data is volatile - Lost when process ends unless explicitly saved Memory usage - Entire dataset must fit in available RAM No persistence - Use dump_workspace() to save to disk Single process - Not suitable for distributed applications","title":"\ud83d\udea8 Important Notes"},{"location":"vector_store_api_guide/#working-with-vectornode","text":"The VectorNode class is the fundamental data unit for all vector stores: from flowllm.schema.vector_node import VectorNode # Create a node node = VectorNode ( unique_id = \"unique_identifier\" , # Unique ID for the node (required) workspace_id = \"my_workspace\" , # Workspace ID (required) content = \"Text content to embed\" , # Content to be embedded (required) metadata = { # Optional metadata \"source\" : \"document1\" , \"category\" : \"technology\" , \"timestamp\" : \"2024-08-29\" }, vector = None # Vector will be generated automatically if None )","title":"\ud83d\udcdd Working with VectorNode"},{"location":"vector_store_api_guide/#importexport-example","text":"Export and import workspaces for backup or transfer: # Export workspace to file vector_store . dump_workspace ( workspace_id = \"my_workspace\" , path = \"./backup_data\" # Directory to store the exported data ) # Import workspace from file vector_store . load_workspace ( workspace_id = \"new_workspace\" , path = \"./backup_data\" # Directory containing the exported data ) # Copy workspace within the same store vector_store . copy_workspace ( src_workspace_id = \"original_workspace\" , dest_workspace_id = \"copied_workspace\" )","title":"\ud83d\udd04 Import/Export Example"},{"location":"vector_store_api_guide/#integration-with-embedding-models","text":"All vector stores require an embedding model to function: from flowllm.embedding_model import OpenAICompatibleEmbeddingModel # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , # Embedding dimensions model_name = \"text-embedding-v4\" , # Model name batch_size = 32 # Batch size for embedding generation ) # Pass to vector store (example with LocalVectorStore) # You can also use: ChromaVectorStore, EsVectorStore, or MemoryVectorStore vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./vector_store\" ) \ud83c\udf89 This guide provides everything you need to work with vector stores in flowllm. Choose the implementation that best fits your use case and scale up as needed! \u2728","title":"\ud83e\udde9 Integration with Embedding Models"},{"location":"cookbook/experiment_overview/","text":"\ud83c\udf0d Appworld Experiment We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md . \ud83e\uddca Frozenlake Experiment without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md . \ud83d\udd27 BFCL-V3 Experiment We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%)","title":"Overview"},{"location":"cookbook/experiment_overview/#appworld-experiment","text":"We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83c\udf0d Appworld Experiment"},{"location":"cookbook/experiment_overview/#frozenlake-experiment","text":"without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83e\uddca Frozenlake Experiment"},{"location":"cookbook/experiment_overview/#bfcl-v3-experiment","text":"We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%)","title":"\ud83d\udd27 BFCL-V3 Experiment"},{"location":"cookbook/appworld/quickstart/","text":"This guide helps you quickly set up and run AppWorld experiments with ReMe integration. Env Setup 1. Clone the Repository git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/appworld 2. Appworld Environment Setup Create a new conda environment with Python 3.12: conda create -p ./appworld-env python == 3 .12 conda activate ./appworld-env Install required Python packages: pip install -r requirements.txt Install AppWorld and download the dataset: pip install appworld appworld install appworld download data Note : The AppWorld data will be saved in the current directory. 3. Start ReMe Service Install ReMe (if not already installed) If you haven't installed the ReMe environment yet, follow these steps: # Go back to the project root cd ../.. # Create ReMe environment conda create -p ./reme-env python == 3 .12 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-latest \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local add memories for appworld: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"appworld\", \"action\": \"load\", \"path\": \"./docs/library\" }' Now you have loaded the ReMe memory library to enable memory-based agent! 4. Common Issues AppWorld data not found : Ensure appworld download data completed successfully pydantic version issue : AppWorld depends on an older version of pydantic, which is why a separate environment is needed. If you encounter issues running the experiments, try pip install appworld to override the dependencies. Run Experiments 1. Test: With Memory vs Without Memory Run the main experiment script to compare performance with and without memory: python run_appworld.py What this does: - Runs AppWorld tasks on the development dataset - Compares agent performance with ReMe memory ( use_memory=True ) vs without memory - Uses multiple workers for parallel processing - Runs each task multiple times for statistical significance - Results are automatically saved to ./exp_result/ directory Configuration options in run_appworld.py : - max_workers : Number of parallel workers (default: 6) - num_runs : Number of times each task is repeated (default: 4) - use_memory : Whether to use ReMe memory library 2. View Experiment Results After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv Metrics explained: - best@k : Takes groups of k runs per task, finds the maximum score in each group, then averages these maximums - Higher k values show potential performance, lower k values show consistency Output Files ./exp_result/*.jsonl : Raw experiment results for each configuration ./exp_result/experiment_summary.csv : Statistical summary table Console output: Real-time progress and summary statistics Understanding Results The experiment compares: 1. Baseline : Agent without memory library 2. With Memory : Agent enhanced with ReMe memory library Key metrics to look for: - best@1 : Average performance across all single runs - best@k : Performance when taking the best of k attempts - Improvement percentage when using memory vs baseline","title":"AppWorld"},{"location":"cookbook/appworld/quickstart/#env-setup","text":"","title":"Env Setup"},{"location":"cookbook/appworld/quickstart/#1-clone-the-repository","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/appworld","title":"1. Clone the Repository"},{"location":"cookbook/appworld/quickstart/#2-appworld-environment-setup","text":"Create a new conda environment with Python 3.12: conda create -p ./appworld-env python == 3 .12 conda activate ./appworld-env Install required Python packages: pip install -r requirements.txt Install AppWorld and download the dataset: pip install appworld appworld install appworld download data Note : The AppWorld data will be saved in the current directory.","title":"2. Appworld Environment Setup"},{"location":"cookbook/appworld/quickstart/#3-start-reme-service","text":"Install ReMe (if not already installed) If you haven't installed the ReMe environment yet, follow these steps: # Go back to the project root cd ../.. # Create ReMe environment conda create -p ./reme-env python == 3 .12 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-latest \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local add memories for appworld: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"appworld\", \"action\": \"load\", \"path\": \"./docs/library\" }' Now you have loaded the ReMe memory library to enable memory-based agent!","title":"3. Start ReMe Service"},{"location":"cookbook/appworld/quickstart/#4-common-issues","text":"AppWorld data not found : Ensure appworld download data completed successfully pydantic version issue : AppWorld depends on an older version of pydantic, which is why a separate environment is needed. If you encounter issues running the experiments, try pip install appworld to override the dependencies.","title":"4. Common Issues"},{"location":"cookbook/appworld/quickstart/#run-experiments","text":"","title":"Run Experiments"},{"location":"cookbook/appworld/quickstart/#1-test-with-memory-vs-without-memory","text":"Run the main experiment script to compare performance with and without memory: python run_appworld.py What this does: - Runs AppWorld tasks on the development dataset - Compares agent performance with ReMe memory ( use_memory=True ) vs without memory - Uses multiple workers for parallel processing - Runs each task multiple times for statistical significance - Results are automatically saved to ./exp_result/ directory Configuration options in run_appworld.py : - max_workers : Number of parallel workers (default: 6) - num_runs : Number of times each task is repeated (default: 4) - use_memory : Whether to use ReMe memory library","title":"1. Test: With Memory vs Without Memory"},{"location":"cookbook/appworld/quickstart/#2-view-experiment-results","text":"After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv Metrics explained: - best@k : Takes groups of k runs per task, finds the maximum score in each group, then averages these maximums - Higher k values show potential performance, lower k values show consistency Output Files ./exp_result/*.jsonl : Raw experiment results for each configuration ./exp_result/experiment_summary.csv : Statistical summary table Console output: Real-time progress and summary statistics","title":"2. View Experiment Results"},{"location":"cookbook/appworld/quickstart/#understanding-results","text":"The experiment compares: 1. Baseline : Agent without memory library 2. With Memory : Agent enhanced with ReMe memory library Key metrics to look for: - best@1 : Average performance across all single runs - best@k : Performance when taking the best of k attempts - Improvement percentage when using memory vs baseline","title":"Understanding Results"},{"location":"cookbook/bfcl/quickstart/","text":"This guide helps you quickly set up and run BFCL experiments with ReMe integration. Env Setup 1. BFCL installation clone the repository git clone https://github.com/ShishirPatil/gorilla.git Change directory to the berkeley-function-call-leaderboard cd gorilla/berkeley-function-call-leaderboard Install the package in editable mode conda create -n bfcl-env python == 3 .12 conda activate bfcl-env pip install -e . pip install -r requirements.txt Move the dataset to the data folder under bfcl cp -r bfcl_eval/data { /path/to/bfcl/data } Note : The original BFCL data is designed as a benchmark dataset and does not have a train/validation split, you can use split_into_trainval.py to split JSONL file into train and validation sets. 2. Collect agent trajectories on training data set Run the main experiment script to collect agent trajectories on training data set without task memory( use_memory=False ): python run_bfcl.py Note : - max_workers : Number of parallel workers (default: 4 ) - num_runs : Number of times each task is repeated (default: 1 ) - model_name : LLM model name (default: qwen3-8b ) - enable_thinking : Control the model's thinking mode (default: False ) - data_path : Path to the training dataset (default: ./data/multiturn_data_base_train.jsonl ) - answer_path : Path to the possible answer, which are used to evaluate the model's output function (default: ./data/possible_answer ) - Results are automatically saved to ./exp_result/{model_name}/{no_think/with_think} directory 3. Start ReMe Service and Init the task memory pool After collecting trajectories, Launch the ReMe service (make sure you have installed ReMe environment, if not please follow the steps in the ReMe Installation Guide to install): reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local and then init the task memory pool: python init_exp_pool.py Configuration options in init_exp_pool.py : - jsonl_file : Path to the collloaded trajectories - service_url : ReMe service URL (default: http://localhost:8002 ) - workspace_id : Workspace ID for the task memory pool (default: bfcl_test ) - n_threads : Number of threads for processing (default: 4 ) - output_file : Output file to save results (optional) Now you have inited the task memory pool using local backend (start on http://localhost:8002 ). Then, use local_file_to_library.py script to convert the local file to the memory library or run the following curl command: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"dump\", \"path\": \"./library\" }' to dump the memory library (default in ./library/bfcl_test.jsonl ). Next time, you can import this previously exported task memory data to populate the new started workspace with existing knowledge: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"load\", \"path\": \"./library\" }' 4. Run Experiments on Validation Set Run you can compare agent performance on the validation set with task memory ( use_memory=True ) and without task memory: # remember to change the configuration options, e.g., `data_path=./data/multiturn_data_base_val.jsonl` python run_bfcl.py After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv","title":"BFCL"},{"location":"cookbook/bfcl/quickstart/#env-setup","text":"","title":"Env Setup"},{"location":"cookbook/bfcl/quickstart/#1-bfcl-installation","text":"","title":"1. BFCL installation"},{"location":"cookbook/bfcl/quickstart/#clone-the-repository","text":"git clone https://github.com/ShishirPatil/gorilla.git","title":"clone the repository"},{"location":"cookbook/bfcl/quickstart/#change-directory-to-the-berkeley-function-call-leaderboard","text":"cd gorilla/berkeley-function-call-leaderboard","title":"Change directory to the berkeley-function-call-leaderboard"},{"location":"cookbook/bfcl/quickstart/#install-the-package-in-editable-mode","text":"conda create -n bfcl-env python == 3 .12 conda activate bfcl-env pip install -e . pip install -r requirements.txt","title":"Install the package in editable mode"},{"location":"cookbook/bfcl/quickstart/#move-the-dataset-to-the-data-folder-under-bfcl","text":"cp -r bfcl_eval/data { /path/to/bfcl/data } Note : The original BFCL data is designed as a benchmark dataset and does not have a train/validation split, you can use split_into_trainval.py to split JSONL file into train and validation sets.","title":"Move the dataset to the data folder under bfcl"},{"location":"cookbook/bfcl/quickstart/#2-collect-agent-trajectories-on-training-data-set","text":"Run the main experiment script to collect agent trajectories on training data set without task memory( use_memory=False ): python run_bfcl.py Note : - max_workers : Number of parallel workers (default: 4 ) - num_runs : Number of times each task is repeated (default: 1 ) - model_name : LLM model name (default: qwen3-8b ) - enable_thinking : Control the model's thinking mode (default: False ) - data_path : Path to the training dataset (default: ./data/multiturn_data_base_train.jsonl ) - answer_path : Path to the possible answer, which are used to evaluate the model's output function (default: ./data/possible_answer ) - Results are automatically saved to ./exp_result/{model_name}/{no_think/with_think} directory","title":"2. Collect agent trajectories on training data set"},{"location":"cookbook/bfcl/quickstart/#3-start-reme-service-and-init-the-task-memory-pool","text":"After collecting trajectories, Launch the ReMe service (make sure you have installed ReMe environment, if not please follow the steps in the ReMe Installation Guide to install): reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local and then init the task memory pool: python init_exp_pool.py Configuration options in init_exp_pool.py : - jsonl_file : Path to the collloaded trajectories - service_url : ReMe service URL (default: http://localhost:8002 ) - workspace_id : Workspace ID for the task memory pool (default: bfcl_test ) - n_threads : Number of threads for processing (default: 4 ) - output_file : Output file to save results (optional) Now you have inited the task memory pool using local backend (start on http://localhost:8002 ). Then, use local_file_to_library.py script to convert the local file to the memory library or run the following curl command: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"dump\", \"path\": \"./library\" }' to dump the memory library (default in ./library/bfcl_test.jsonl ). Next time, you can import this previously exported task memory data to populate the new started workspace with existing knowledge: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"load\", \"path\": \"./library\" }'","title":"3. Start ReMe Service and Init the task memory pool"},{"location":"cookbook/bfcl/quickstart/#4-run-experiments-on-validation-set","text":"Run you can compare agent performance on the validation set with task memory ( use_memory=True ) and without task memory: # remember to change the configuration options, e.g., `data_path=./data/multiturn_data_base_val.jsonl` python run_bfcl.py After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv","title":"4. Run Experiments on Validation Set"},{"location":"cookbook/frozenlake/quickstart/","text":"This guide helps you quickly set up and run FrozenLake experiments with ReMe integration. The FrozenLake experiment demonstrates how task memory can improve an agent's performance in a navigation task. Environment Setup 1. Clone the Repository git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/frozenlake 2. FrozenLake Environment Setup Install Gymnasium for FrozenLake environment: pip install gymnasium This will install: - gymnasium - for the FrozenLake environment - ray - for parallel execution - openai - for LLM API access - other dependencies 3. Start ReMe Service If you haven't installed ReMe yet, follow these steps: # Go back to the project root cd ../.. # Create a virtual environment (optional) conda create -p ./reme-env python == 3 .10 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Add your api key for agent: export OPENAI_API_KEY = \"xxx\" export OPENAI_BASE_URL = \"xxx\" Run Experiments 1. Quick Test: Performance Evaluation Only (Default) Run the main experiment script to test agent performance using existing memory: cd cookbook/frozenlake python run_frozenlake.py What this does: - Tests the agent on randomly generated FrozenLake maps - Uses the default memory library ( frozenlake_no_slippery ) - Evaluates performance with multiple runs for statistical significance - Results are automatically saved to ./exp_result/ directory 2. Advanced: Training + Testing (Memory Generation) To create new memories through training and then test performance: You can modify the experiment parameters directly in the run_frozenlake.py file. The main parameters are in the main() function: def main (): experiment_name = \"frozenlake_no_slippery\" # Name of the experiment max_workers = 4 # Number of parallel workers training_runs = 4 # Runs per training map num_training_maps = 50 # Number of maps for training test_runs = 1 # Runs per test configuration num_test_maps = 100 # Number of test maps is_slippery = False # Enable slippery mode Key parameters to consider: - experiment_name : Used as the workspace ID for task memory - is_slippery : When True, agent movement becomes stochastic (harder) - max_workers : Increase for faster execution on multi-core systems 3. View Experiment Results After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates success rates and performance metrics - Generates a summary table showing performance comparisons - Analyzes the effect of task memory on performance - Saves results to frozenlake_summary.csv Understanding the Implementation Key Components FrozenLakeReactAgent ( frozenlake_react_agent.py ) Implements a ReAct agent that interacts with the FrozenLake environment Handles task memory retrieval and storage Uses LLM (via OpenAI API) for decision making Experiment Runner ( run_frozenlake.py ) Manages the overall experiment flow Handles training and testing phases Uses Ray for parallel execution Map Manager ( map_manager.py ) Generates and manages test maps Ensures consistent evaluation across experiments Statistics Analyzer ( run_exp_statistic.py ) Processes experiment results Calculates performance metrics Generates comparative analysis Output Files ./exp_result/*_training.jsonl : Results from training phase ./exp_result/*_test_no_memory.jsonl : Test results without task memory ./exp_result/*_test_with_memory.jsonl : Test results with task memory ./exp_result/frozenlake_summary.csv : Statistical summary Task Memory Mechanism The task memory system works as follows: Memory Creation : During training, successful trajectories are sent to the ReMe service Memory Retrieval : During testing, the agent queries relevant memories based on the current map Memory Application : The agent uses retrieved memories to guide its decision-making The experiment demonstrates how task memory can significantly improve performance, especially in challenging environments like the slippery FrozenLake.","title":"FrozenLake"},{"location":"cookbook/frozenlake/quickstart/#environment-setup","text":"","title":"Environment Setup"},{"location":"cookbook/frozenlake/quickstart/#1-clone-the-repository","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/frozenlake","title":"1. Clone the Repository"},{"location":"cookbook/frozenlake/quickstart/#2-frozenlake-environment-setup","text":"Install Gymnasium for FrozenLake environment: pip install gymnasium This will install: - gymnasium - for the FrozenLake environment - ray - for parallel execution - openai - for LLM API access - other dependencies","title":"2. FrozenLake Environment Setup"},{"location":"cookbook/frozenlake/quickstart/#3-start-reme-service","text":"If you haven't installed ReMe yet, follow these steps: # Go back to the project root cd ../.. # Create a virtual environment (optional) conda create -p ./reme-env python == 3 .10 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Add your api key for agent: export OPENAI_API_KEY = \"xxx\" export OPENAI_BASE_URL = \"xxx\"","title":"3. Start ReMe Service"},{"location":"cookbook/frozenlake/quickstart/#run-experiments","text":"","title":"Run Experiments"},{"location":"cookbook/frozenlake/quickstart/#1-quick-test-performance-evaluation-only-default","text":"Run the main experiment script to test agent performance using existing memory: cd cookbook/frozenlake python run_frozenlake.py What this does: - Tests the agent on randomly generated FrozenLake maps - Uses the default memory library ( frozenlake_no_slippery ) - Evaluates performance with multiple runs for statistical significance - Results are automatically saved to ./exp_result/ directory","title":"1. Quick Test: Performance Evaluation Only (Default)"},{"location":"cookbook/frozenlake/quickstart/#2-advanced-training-testing-memory-generation","text":"To create new memories through training and then test performance: You can modify the experiment parameters directly in the run_frozenlake.py file. The main parameters are in the main() function: def main (): experiment_name = \"frozenlake_no_slippery\" # Name of the experiment max_workers = 4 # Number of parallel workers training_runs = 4 # Runs per training map num_training_maps = 50 # Number of maps for training test_runs = 1 # Runs per test configuration num_test_maps = 100 # Number of test maps is_slippery = False # Enable slippery mode Key parameters to consider: - experiment_name : Used as the workspace ID for task memory - is_slippery : When True, agent movement becomes stochastic (harder) - max_workers : Increase for faster execution on multi-core systems","title":"2. Advanced: Training + Testing (Memory Generation)"},{"location":"cookbook/frozenlake/quickstart/#3-view-experiment-results","text":"After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates success rates and performance metrics - Generates a summary table showing performance comparisons - Analyzes the effect of task memory on performance - Saves results to frozenlake_summary.csv","title":"3. View Experiment Results"},{"location":"cookbook/frozenlake/quickstart/#understanding-the-implementation","text":"","title":"Understanding the Implementation"},{"location":"cookbook/frozenlake/quickstart/#key-components","text":"FrozenLakeReactAgent ( frozenlake_react_agent.py ) Implements a ReAct agent that interacts with the FrozenLake environment Handles task memory retrieval and storage Uses LLM (via OpenAI API) for decision making Experiment Runner ( run_frozenlake.py ) Manages the overall experiment flow Handles training and testing phases Uses Ray for parallel execution Map Manager ( map_manager.py ) Generates and manages test maps Ensures consistent evaluation across experiments Statistics Analyzer ( run_exp_statistic.py ) Processes experiment results Calculates performance metrics Generates comparative analysis","title":"Key Components"},{"location":"cookbook/frozenlake/quickstart/#output-files","text":"./exp_result/*_training.jsonl : Results from training phase ./exp_result/*_test_no_memory.jsonl : Test results without task memory ./exp_result/*_test_with_memory.jsonl : Test results with task memory ./exp_result/frozenlake_summary.csv : Statistical summary","title":"Output Files"},{"location":"cookbook/frozenlake/quickstart/#task-memory-mechanism","text":"The task memory system works as follows: Memory Creation : During training, successful trajectories are sent to the ReMe service Memory Retrieval : During testing, the agent queries relevant memories based on the current map Memory Application : The agent uses retrieved memories to guide its decision-making The experiment demonstrates how task memory can significantly improve performance, especially in challenging environments like the slippery FrozenLake.","title":"Task Memory Mechanism"},{"location":"library/library/","text":"Clear Showing 0 of 0 items Loading memory library\u2026 \u26a0\ufe0f Failed to load memory library. Try again \u2190 Back to Libraries Libraries \u2190 Prev Next \u2192 \ud83d\udd0e No results found. Try changing your search. \u2715 When to use Memory Metadata Author Created Memory ID Workspace Close :root { --ml-radius: .75rem; --ml-gap: 1rem; --ml-shadow: 0 6px 24px rgba(0,0,0,.08); } .ml-prose-container { display: grid; gap: var(--ml-gap); } .ml-card { background: var(--background, #fff); color: var(--foreground, #0a0a0a); border: 1px solid var(--border, rgba(0,0,0,.08)); border-radius: var(--ml-radius); padding: 1rem; box-shadow: var(--shadow, 0 1px 0 rgba(0,0,0,.02)); } /* general card/grid */ .ml-grid { display: grid; gap: var(--ml-gap); grid-template-columns: repeat(1, minmax(0,1fr)); } @media (min-width: 640px){ .ml-grid{ grid-template-columns: repeat(2, minmax(0,1fr)); } } @media (min-width: 1024px){ .ml-grid{ grid-template-columns: repeat(3, minmax(0,1fr)); } } /* libraries stacked (categories vertical, libraries 1 per row) */ .ml-stacked { display: grid; gap: 1.25rem; } .ml-section{ display:grid; gap:.5rem; } .ml-section h3{ margin:.25rem 0; font-size:1.05rem; font-weight:700; opacity:.85; display:flex; gap:.5rem; align-items:center; } .ml-card-item{ background: var(--card, var(--background, #fff)); border: 1px solid var(--border, rgba(0,0,0,.08)); border-radius: var(--ml-radius); padding: 1rem; transition: transform .18s ease, box-shadow .18s ease, border-color .18s ease; cursor: pointer; } .ml-card-item:hover{ transform: translateY(-2px); box-shadow: var(--ml-shadow); border-color: var(--primary, #3b82f6); } .ml-card-head{ display:flex; align-items:flex-start; justify-content:space-between; gap:.75rem; margin-bottom:.5rem; } .ml-card-title{ font-weight: 650; font-size: 1rem; } .ml-card-sub{ font-size: .85rem; opacity: .7; } .ml-card-sample{ margin-top:.5rem; font-size:.92rem; line-height:1.5; opacity:.9; display:-webkit-box; -webkit-line-clamp:3; -webkit-box-orient:vertical; overflow:hidden; } .ml-card-foot{ display:flex; justify-content:space-between; align-items:center; border-top:1px solid var(--border, rgba(0,0,0,.08)); padding-top:.5rem; margin-top:.75rem; font-size:.85rem; opacity:.8; } /* toolbar */ .ml-toolbar{ display:flex; gap:.75rem; align-items:center; justify-content:space-between; flex-wrap:wrap; } .ml-input-wrap{ position:relative; flex:1; min-width: 260px; } .ml-input-wrap input{ width:100%; padding:.6rem .9rem .6rem 2.2rem; border-radius:.6rem; border:1px solid var(--border, rgba(0,0,0,.12)); background: var(--muted, rgba(0,0,0,.02)); color: var(--foreground, #0a0a0a); outline:none; } .ml-input-wrap input:focus{ border-color: var(--primary, #3b82f6); box-shadow: 0 0 0 3px color-mix(in srgb, var(--primary, #3b82f6) 22%, transparent); background: var(--background, #fff); } .ml-icon{ position:absolute; left:.6rem; top:50%; transform:translateY(-50%); width:1.1rem; height:1.1rem; opacity:.6; } .ml-btn{ border:1px solid var(--border, rgba(0,0,0,.12)); background: var(--accent, var(--background, #fff)); color: var(--foreground, #0a0a0a); padding:.55rem .9rem; border-radius:.55rem; cursor:pointer; } .ml-btn.secondary{ background: var(--muted, rgba(0,0,0,.03)); } .ml-btn:hover{ border-color: var(--primary, #3b82f6); } /* stats/breadcrumb */ .ml-stats{ margin-top:.5rem; font-size:.9rem; opacity:.8; } .ml-crumb{ display:flex; align-items:center; gap:.75rem; } .ml-link{ background:none; border:none; color: var(--primary, #3b82f6); cursor:pointer; padding:.25rem .5rem; border-radius:.4rem; } .ml-link:hover{ text-decoration: underline; } .ml-crumb-title{ font-weight:600; opacity:.8; } /* states */ .ml-loading, .ml-error, .ml-empty{ display:grid; justify-items:center; gap:.5rem; padding:3rem 1rem; } .ml-spinner{ width:38px; height:38px; border-radius:999px; border:3px solid color-mix(in srgb, var(--foreground,#000) 12%, transparent); border-top-color: var(--primary,#3b82f6); animation: ml-spin 1s linear infinite; } @keyframes ml-spin{ to{ transform: rotate(360deg); } } .ml-muted{ opacity:.7; } .ml-error-icon{ font-size:1.4rem; } /* chips */ .ml-chip{ display:inline-block; padding:.25rem .55rem; border-radius:999px; font-size:.78rem; background: color-mix(in srgb, var(--primary,#3b82f6) 12%, transparent); color: var(--primary,#3b82f6); } .ml-chip.success{ background: color-mix(in srgb, #16a34a 14%, transparent); color: #16a34a; } .ml-chip.beta{ background: color-mix(in srgb, #f59e0b 14%, transparent); color: #b45309; } .ml-chip.contribute { background: color-mix(in srgb, #3b82f6 14%, transparent); color: #1d4ed8; } /* code/note */ .ml-code{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", monospace; background: var(--muted, rgba(0,0,0,.04)); border:1px solid var(--border, rgba(0,0,0,.08)); padding:.75rem; border-radius:.6rem; white-space:pre-wrap; } .ml-note{ background: color-mix(in srgb, #f59e0b 9%, transparent); border:1px solid color-mix(in srgb, #f59e0b 28%, transparent); padding:.75rem; border-radius:.6rem; } /* meta */ .ml-meta{ display:grid; grid-template-columns: repeat(1, minmax(0,1fr)); gap:.5rem; } @media (min-width: 640px){ .ml-meta{ grid-template-columns: repeat(2, minmax(0,1fr)); } } .ml-meta > div{ display:flex; justify-content:space-between; align-items:center; padding:.5rem .75rem; border:1px dashed var(--border, rgba(0,0,0,.12)); border-radius:.5rem; background: var(--background, #fff); } .ml-meta span{ opacity:.7; } .mono{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace; } /* modal */ .ml-modal{ padding:0; border:none; background: transparent; } .ml-modal[open]{ display:grid; align-items:center; justify-items:center; } .ml-modal::backdrop{ background: rgba(0,0,0,.45); } .ml-modal-card{ width:min(100%, 960px); max-height: 85vh; overflow:auto; background: var(--background, #fff); color: var(--foreground,#0a0a0a); border:1px solid var(--border, rgba(0,0,0,.1)); border-radius: var(--ml-radius); padding: 1rem; box-shadow: var(--ml-shadow); } .ml-modal-header{ display:flex; justify-content:space-between; align-items:center; gap:.75rem; margin-bottom:.5rem; } .ml-close{ border:none; background:none; font-size:1.1rem; cursor:pointer; opacity:.6; } .ml-close:hover{ opacity:1; } .ml-modal-section{ display:grid; gap:.35rem; margin-top:.75rem; } .ml-section-title{ font-weight:650; opacity:.85; } .ml-modal-footer{ display:flex; justify-content:flex-end; margin-top:1rem; } /* pagination */ .ml-pagination{ display:flex; justify-content:space-between; align-items:center; padding:.5rem .25rem; } .ml-page-controls{ display:flex; gap:.5rem; } .ml-page-info{ font-size:.9rem; opacity:.8; } (() => { // \u2014\u2014 State let ALL = []; let GROUPED = {}; let VIEW = \"libraries\"; // \"libraries\" | \"memories\" let CURR = null; // pagination state for memories let PAGE = 1; const PAGE_SIZE = 30; let CURRENT_MEM_LIST = []; // \u2014\u2014 DOM const $ = (id) => document.getElementById(id); const elLoading = $(\"ml-loading\"); const elError = $(\"ml-error\"); const elRetry = $(\"ml-retry\"); const elLibraries = $(\"ml-libraries\"); const elMemories = $(\"ml-memories\"); const elPagination = $(\"ml-pagination\"); const elPageRange = $(\"ml-page-range\"); const elPrev = $(\"ml-prev\"); const elNext = $(\"ml-next\"); const elEmpty = $(\"ml-empty\"); const elSearch = $(\"ml-search\"); const elClear = $(\"ml-clear\"); const elStats = $(\"ml-stats\"); const elCount = $(\"ml-count\"); const elTotal = $(\"ml-total\"); const elType = $(\"ml-type\"); const elCrumb = $(\"ml-crumb\"); const elBack = $(\"ml-back\"); const elCrumbTitle = $(\"ml-crumb-title\"); const dlg = $(\"ml-modal\"); const mLib = $(\"ml-modal-lib\"); const mScore = $(\"ml-modal-score\"); const mWhen = $(\"ml-modal-when\"); const mCont = $(\"ml-modal-content\"); const mAuth = $(\"ml-modal-author\"); const mCreated = $(\"ml-modal-created\"); const mId = $(\"ml-modal-id\"); const mWs = $(\"ml-modal-ws\"); // \u2014\u2014 Config\uff1aJSONL \u6587\u4ef6\u4f4d\u4e8e\u672c\u9875\u540c\u7ea7\u76ee\u5f55\uff08docs/library/\uff09 const BASE = \"..\"; // \u2014\u2014 Categories const CATEGORY_MAP = { \"Academic Datasets\": [\"appworld\", \"bfcl_v3\"], \"Finance\": [\"research_plan\", \"research_tips\"], \"Medical/Law/Education\": [] // header only if empty }; const FILES = Array.from(new Set( Object.values(CATEGORY_MAP).flat().map(n => `${n}.jsonl`) )); // \u2014\u2014 Utils function show(el){ el.hidden = false; } function hide(el){ el.hidden = true; } function setLoading(on){ on ? (show(elLoading), [elError, elLibraries, elMemories, elEmpty, elStats, elCrumb, elPagination].forEach(hide)) : hide(elLoading); } function setError(on){ on ? (show(elError), [elLoading].forEach(hide)) : hide(elError); } function clampTxt(s, n){ if(!s) return \"\"; return s.length<=n? s : s.slice(0,n)+\"\u2026\"; } const fmtDate = (t)=> t ? new Date(t).toLocaleDateString() : \"Unknown\"; function debounce(fn, ms=250){ let t; return (...a)=>{ clearTimeout(t); t=setTimeout(()=>fn(...a), ms); }; } function fileBase(name){ return name.replace(/\\.jsonl$/,\"\"); } // \u2014\u2014 Data Loading async function loadAll(){ setLoading(true); setError(false); try{ const arr = await Promise.all(FILES.map(async f=>{ try{ const res = await fetch(`${BASE}/${f}`); if(!res.ok) return []; const txt = await res.text(); return txt.split(\"\\n\").filter(l=>l.trim()).map(line=>{ try{ const obj = JSON.parse(line); obj._library = fileBase(f); return obj; }catch{ return null; } }).filter(Boolean); }catch{ return []; } })); ALL = arr.flat(); if(!ALL.length) throw new Error(\"no data\"); GROUPED = ALL.reduce((acc,m)=>{ (acc[m._library] ||= []).push(m); return acc; }, {}); renderLibraries(); }catch(e){ setError(true); }finally{ setLoading(false); } } // \u2014\u2014 Render \u2014 Libraries (stacked categories) function renderLibraries(){ VIEW = \"libraries\"; CURR = null; PAGE = 1; CURRENT_MEM_LIST = []; hide(elMemories); hide(elEmpty); hide(elPagination); show(elLibraries); hide(elCrumb); elCrumbTitle.textContent = \"Libraries\"; elType.textContent = \"libraries\"; const availableLibs = Object.keys(GROUPED); const sections = Object.entries(CATEGORY_MAP).map(([cat, prefixes])=>{ // build libraries list for this category const libs = (prefixes || []).filter(p => availableLibs.includes(p)); const itemsHtml = libs.map(name=>{ const arr = GROUPED[name]; const sample = arr[0] || {}; const sampleText = sample.when_to_use || sample.content || \"No description available\"; const author = sample.author || \"Unknown\"; return ` <div class=\"ml-card-item\" data-lib=\"${name}\"> <div class=\"ml-card-head\"> <div> <div class=\"ml-card-title\">${name}</div> <div class=\"ml-card-sub\">${arr.length} memories</div> </div> <div class=\"ml-chip\">DB</div> </div> <div class=\"ml-card-sample\">${clampTxt(sampleText, 180)}</div> <div class=\"ml-card-foot\"> <span>\ud83d\udc64 ${author}</span> <span>View \u2192</span> </div> </div> `; }).join(\"\"); // Category header with Finance (beta) chip const betaChip = (cat === \"Finance\") ? `<span class=\"ml-chip beta\">beta</span>` : \"\"; const contributeChip = (cat === \"Medical/Law/Education\") ? `<span class=\"ml-chip contribute\">Feel free to contribute</span>` : \"\"; return ` <section class=\"ml-section\"> <h3>${cat} ${betaChip} ${contributeChip}</h3> <div class=\"ml-grid\"> ${itemsHtml} </div> </section> `; }).join(\"\"); elLibraries.innerHTML = sections; bindLibraryClicks(); show(elStats); const catsShown = Object.keys(CATEGORY_MAP).length; const libsShown = Object.values(CATEGORY_MAP) .reduce((acc, prefixes) => acc + prefixes.filter(p => availableLibs.includes(p)).length, 0); $(\"ml-count\").textContent = libsShown; $(\"ml-total\").textContent = libsShown; } // \u2014\u2014 Render \u2014 Memories with Pagination function renderMemories(memList){ VIEW = \"memories\"; hide(elLibraries); hide(elEmpty); show(elMemories); show(elCrumb); elType.textContent = \"memories\"; elCrumbTitle.textContent = `Exploring ${CURR}`; CURRENT_MEM_LIST = memList || []; if(!CURRENT_MEM_LIST.length){ hide(elMemories); hide(elPagination); show(elEmpty); hide(elStats); return; } const total = CURRENT_MEM_LIST.length; const pages = Math.max(1, Math.ceil(total / PAGE_SIZE)); if(PAGE > pages) PAGE = pages; const startIdx = (PAGE - 1) * PAGE_SIZE; const endIdx = Math.min(startIdx + PAGE_SIZE, total); const pageItems = CURRENT_MEM_LIST.slice(startIdx, endIdx); elMemories.innerHTML = pageItems.map((m,idxOnPage)=>` <div class=\"ml-card-item\" data-idx=\"${startIdx + idxOnPage}\"> <div class=\"ml-card-head\"> <div class=\"ml-chip\">${m._library}</div> ${(\"score\" in m && m.score !== null && m.score !== undefined) ? `<div class=\"ml-chip success\">Score: ${m.score}</div>` : \"\"} </div> <div class=\"ml-card-sample\"><b>When to use:</b> ${clampTxt(m.when_to_use || \"No specific guidance provided\", 140)}</div> <div class=\"ml-card-foot\"> <span>\ud83d\udc64 ${m.author || \"Unknown\"}</span> <span>Details \u2192</span> </div> </div> `).join(\"\"); // modal binding [...elMemories.querySelectorAll(\".ml-card-item\")].forEach(card=>{ card.addEventListener(\"click\", ()=>{ const absIdx = Number(card.getAttribute(\"data-idx\")); const m = CURRENT_MEM_LIST[absIdx]; mLib.textContent = m._library; const hasScore = \"score\" in m && m.score !== null && m.score !== undefined; if(hasScore){ mScore.textContent = `Score: ${m.score}`; mScore.hidden = false; } else { mScore.hidden = true; } mWhen.textContent = m.when_to_use || \"No specific guidance provided\"; mCont.textContent = m.content || \"No content available\"; mAuth.textContent = m.author || \"Unknown\"; mCreated.textContent = fmtDate(m.time_created); mId.textContent = m.memory_id || \"N/A\"; mWs.textContent = m.workspace_id || \"N/A\"; dlg.showModal(); }); }); // pagination controls show(elPagination); elPageRange.textContent = `Showing ${startIdx + 1}\u2013${endIdx} of ${total}`; elPrev.disabled = PAGE <= 1; elNext.disabled = PAGE >= pages; elPrev.onclick = ()=>{ if(PAGE > 1){ PAGE--; renderMemories(CURRENT_MEM_LIST); } }; elNext.onclick = ()=>{ if(PAGE < pages){ PAGE++; renderMemories(CURRENT_MEM_LIST); } }; show(elStats); elCount.textContent = pageItems.length; elTotal.textContent = total; } function bindLibraryClicks(){ [...elLibraries.querySelectorAll(\".ml-card-item[data-lib]\")].forEach(card=>{ card.addEventListener(\"click\", ()=>{ CURR = card.getAttribute(\"data-lib\"); PAGE = 1; renderMemories(GROUPED[CURR]); }); }); } // \u2014\u2014 Search function handleSearch(){ const q = elSearch.value.trim().toLowerCase(); if(!q){ if(VIEW===\"libraries\") renderLibraries(); else { PAGE = 1; renderMemories(GROUPED[CURR]); } return; } if(VIEW===\"libraries\"){ // filter categories if name matches, or any of their libs/memories match const availableLibs = Object.keys(GROUPED); const filteredEntries = Object.entries(CATEGORY_MAP).filter(([cat, prefixes])=>{ if(cat.toLowerCase().includes(q)) return true; return (prefixes || []).some(name=>{ if(!availableLibs.includes(name)) return false; const arr = GROUPED[name] || []; if(name.toLowerCase().includes(q)) return true; return arr.some(m => (m.when_to_use||\"\").toLowerCase().includes(q) || (m.content||\"\").toLowerCase().includes(q) || (m.author||\"\").toLowerCase().includes(q) ); }); }); const tmp = Object.fromEntries(filteredEntries); const backup = {...CATEGORY_MAP}; Object.keys(CATEGORY_MAP).forEach(k=> delete CATEGORY_MAP[k]); Object.assign(CATEGORY_MAP, tmp); renderLibraries(); Object.keys(CATEGORY_MAP).forEach(k=> delete CATEGORY_MAP[k]); Object.assign(CATEGORY_MAP, backup); }else{ const arr = GROUPED[CURR] || []; const filtered = arr.filter(m => (m.when_to_use||\"\").toLowerCase().includes(q) || (m.content||\"\").toLowerCase().includes(q) || (m.author||\"\").toLowerCase().includes(q) ); PAGE = 1; renderMemories(filtered); } } // \u2014\u2014 Events elRetry?.addEventListener(\"click\", loadAll); elBack?.addEventListener(\"click\", ()=> renderLibraries()); elSearch?.addEventListener(\"input\", debounce(handleSearch, 250)); elClear?.addEventListener(\"click\", ()=>{ elSearch.value = \"\"; handleSearch(); }); // \u2014\u2014 Init document.addEventListener(\"DOMContentLoaded\", loadAll); })();","title":"Library Home"},{"location":"library/use_library/","text":"ReMe provides pre-built memory libraries that agents can immediately use with verified best practices: Available Libraries appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls Quick Usage # Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 })","title":"Use Library"},{"location":"library/use_library/#available-libraries","text":"appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls","title":"Available Libraries"},{"location":"library/use_library/#quick-usage","text":"# Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 })","title":"Quick Usage"},{"location":"personal_memory/personal_memory/","text":"Configuration Logic ReMe's personal memory system consists of two main components: retrieval and summarization. The configuration for these components is defined in the default.yaml file. Retrieval Configuration ( retrieve_personal_memory ) retrieve_personal_memory : flow_content : set_query_op >> (extract_time_op | (retrieve_memory_op >> semantic_rank_op)) >> fuse_rerank_op This flow performs the following operations: 1. set_query_op : Prepares the query for memory retrieval 2. Parallel paths: - extract_time_op : Extracts time-related information from the query - retrieve_memory_op >> semantic_rank_op : Retrieves memories and ranks them semantically 3. fuse_rerank_op : Combines and reranks the results for final output Summarization Configuration ( summary_personal_memory ) summary_personal_memory : flow_content : info_filter_op >> (get_observation_op | get_observation_with_time_op | load_today_memory_op) >> contra_repeat_op >> update_vector_store_op This flow performs the following operations: 1. info_filter_op : Filters incoming information to extract relevant personal details 2. Parallel paths for observation extraction: - get_observation_op : Extracts general observations - get_observation_with_time_op : Extracts observations with time context - load_today_memory_op : Loads memories from the current day 3. contra_repeat_op : Removes contradictions and repetitions 4. update_vector_store_op : Stores the processed memories in the vector database Basic Usage The following example demonstrates how to use personal memory in MemoryScope: 1. Setup import asyncio import json import aiohttp # API base URL (default is http://0.0.0.0:8002) base_url = \"http://0.0.0.0:8002\" workspace_id = \"personal_memory_demo\" 2. Clear Existing Memories async with aiohttp . ClientSession () as session : # Delete existing workspace memories async with session . post ( f \" { base_url } /vector_store\" , json = { \"action\" : \"delete\" , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () 3. Create Conversation with Personal Information # Example conversation with personal details messages = [ { \"role\" : \"user\" , \"content\" : \"My name is John Smith, I'm 28 years old\" }, { \"role\" : \"assistant\" , \"content\" : \"Nice to meet you, John!\" }, { \"role\" : \"user\" , \"content\" : \"I'm a software engineer working with Python\" }, { \"role\" : \"assistant\" , \"content\" : \"I see, you're a Python engineer.\" }, # Additional conversation messages... ] 4. Summarize Personal Memories async with session . post ( f \" { base_url } /summary_personal_memory\" , json = { \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ], \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () 5. Retrieve Personal Memories # Example queries to retrieve personal information queries = [ \"What's my name and age?\" , \"What do I do for work?\" , \"What are my hobbies?\" ] for query in queries : async with session . post ( f \" { base_url } /retrieve_personal_memory\" , json = { \"query\" : query , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () print ( f \"Query: { query } \" ) print ( f \"Answer: { result . get ( 'answer' , '' ) } \" ) Complete Example For a complete working example, refer to /cookbook/simple_demo/use_personal_memory_demo.py in the ReMe repository.","title":"Overview"},{"location":"personal_memory/personal_memory/#configuration-logic","text":"ReMe's personal memory system consists of two main components: retrieval and summarization. The configuration for these components is defined in the default.yaml file.","title":"Configuration Logic"},{"location":"personal_memory/personal_memory/#retrieval-configuration-retrieve_personal_memory","text":"retrieve_personal_memory : flow_content : set_query_op >> (extract_time_op | (retrieve_memory_op >> semantic_rank_op)) >> fuse_rerank_op This flow performs the following operations: 1. set_query_op : Prepares the query for memory retrieval 2. Parallel paths: - extract_time_op : Extracts time-related information from the query - retrieve_memory_op >> semantic_rank_op : Retrieves memories and ranks them semantically 3. fuse_rerank_op : Combines and reranks the results for final output","title":"Retrieval Configuration (retrieve_personal_memory)"},{"location":"personal_memory/personal_memory/#summarization-configuration-summary_personal_memory","text":"summary_personal_memory : flow_content : info_filter_op >> (get_observation_op | get_observation_with_time_op | load_today_memory_op) >> contra_repeat_op >> update_vector_store_op This flow performs the following operations: 1. info_filter_op : Filters incoming information to extract relevant personal details 2. Parallel paths for observation extraction: - get_observation_op : Extracts general observations - get_observation_with_time_op : Extracts observations with time context - load_today_memory_op : Loads memories from the current day 3. contra_repeat_op : Removes contradictions and repetitions 4. update_vector_store_op : Stores the processed memories in the vector database","title":"Summarization Configuration (summary_personal_memory)"},{"location":"personal_memory/personal_memory/#basic-usage","text":"The following example demonstrates how to use personal memory in MemoryScope:","title":"Basic Usage"},{"location":"personal_memory/personal_memory/#1-setup","text":"import asyncio import json import aiohttp # API base URL (default is http://0.0.0.0:8002) base_url = \"http://0.0.0.0:8002\" workspace_id = \"personal_memory_demo\"","title":"1. Setup"},{"location":"personal_memory/personal_memory/#2-clear-existing-memories","text":"async with aiohttp . ClientSession () as session : # Delete existing workspace memories async with session . post ( f \" { base_url } /vector_store\" , json = { \"action\" : \"delete\" , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json ()","title":"2. Clear Existing Memories"},{"location":"personal_memory/personal_memory/#3-create-conversation-with-personal-information","text":"# Example conversation with personal details messages = [ { \"role\" : \"user\" , \"content\" : \"My name is John Smith, I'm 28 years old\" }, { \"role\" : \"assistant\" , \"content\" : \"Nice to meet you, John!\" }, { \"role\" : \"user\" , \"content\" : \"I'm a software engineer working with Python\" }, { \"role\" : \"assistant\" , \"content\" : \"I see, you're a Python engineer.\" }, # Additional conversation messages... ]","title":"3. Create Conversation with Personal Information"},{"location":"personal_memory/personal_memory/#4-summarize-personal-memories","text":"async with session . post ( f \" { base_url } /summary_personal_memory\" , json = { \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ], \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json ()","title":"4. Summarize Personal Memories"},{"location":"personal_memory/personal_memory/#5-retrieve-personal-memories","text":"# Example queries to retrieve personal information queries = [ \"What's my name and age?\" , \"What do I do for work?\" , \"What are my hobbies?\" ] for query in queries : async with session . post ( f \" { base_url } /retrieve_personal_memory\" , json = { \"query\" : query , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () print ( f \"Query: { query } \" ) print ( f \"Answer: { result . get ( 'answer' , '' ) } \" )","title":"5. Retrieve Personal Memories"},{"location":"personal_memory/personal_memory/#complete-example","text":"For a complete working example, refer to /cookbook/simple_demo/use_personal_memory_demo.py in the ReMe repository.","title":"Complete Example"},{"location":"personal_memory/personal_retrieve_ops/","text":"SetQueryOp Functionality SetQueryOp prepares the query for memory retrieval by setting the query and its associated timestamp into the context. It's the first operation in the personal memory retrieval flow. Parameters op.set_query_op.params.timestamp : (Optional) Integer timestamp to use instead of the current time. If not provided, the current timestamp will be used. Implementation Details The operation: 1. Takes the query from the context (which is guaranteed to exist as a flow input requirement) 2. Sets a timestamp (either current time or from parameters) 3. Stores the query and timestamp as a tuple in the context for downstream operations ExtractTimeOp Functionality ExtractTimeOp identifies and extracts time-related information from the query. It uses an LLM to analyze the query text and determine any temporal references or constraints. Parameters op.extract_time_op.params.language : Language for time extraction (defaults to \"en\") Implementation Details The operation: 1. Checks if the query contains datetime keywords 2. If time-related words are found, it prepares a prompt for the LLM with: - System instructions - Few-shot examples - The user's query and current time 3. Parses the LLM response to extract time information (year, month, day, etc.) 4. Stores the extracted time dictionary in the context for downstream operations RetrieveMemoryOp Functionality RetrieveMemoryOp retrieves memories from the vector store based on the query. It extends the RecallVectorStoreOp class to provide memory retrieval functionality. Parameters op.retrieve_memory_op.params.recall_key : Key in the context to use as the query (default: \"query\") op.retrieve_memory_op.params.top_k : Maximum number of memories to retrieve (default: 3) op.retrieve_memory_op.params.threshold_score : (Optional) Minimum similarity score for memories (filters out memories below this threshold) Implementation Details The operation: 1. Retrieves the query from the context 2. Searches the vector store for relevant memories based on the query 3. Removes duplicate memories 4. Filters memories by threshold score if specified 5. Stores the retrieved memories in the context for downstream operations SemanticRankOp Functionality SemanticRankOp ranks memories based on their semantic relevance to the query using an LLM. This improves the quality of retrieved memories by considering deeper semantic relationships beyond vector similarity. Parameters op.semantic_rank_op.params.enable_ranker : Whether to enable semantic ranking (default: true) op.semantic_rank_op.params.output_memory_max_count : Maximum number of memories to output (default: 10) Implementation Details The operation: 1. Retrieves the memory list from the context 2. If ranking is enabled and there are more memories than the output limit: - Removes duplicates based on content - Formats memories for LLM ranking - Asks the LLM to rank memories by relevance on a scale of 0.0 to 1.0 - Parses the ranking results and applies scores to memories 3. Sorts memories by score 4. Stores the ranked memories in the context for downstream operations FuseRerankOp Functionality FuseRerankOp performs the final reranking of memories by combining multiple factors: semantic scores, memory types, and temporal relevance. It also formats the final output. Parameters op.fuse_rerank_op.params.fuse_score_threshold : Minimum score threshold for memories (default: 0.1) op.fuse_rerank_op.params.fuse_ratio_dict : Dictionary of memory type to score multiplier ratios (default: {\"conversation\": 0.5, \"observation\": 1, \"obs_customized\": 1.2, \"insight\": 2.0}) op.fuse_rerank_op.params.fuse_time_ratio : Score multiplier for time-relevant memories (default: 2.0) op.fuse_rerank_op.params.output_memory_max_count : Maximum number of memories to output (default: 5) Implementation Details The operation: 1. Retrieves extracted time information and memory list from the context 2. For each memory: - Checks if the memory score is above the threshold - Applies a type-based adjustment factor based on the memory type - Determines time relevance by matching memory time metadata with extracted time - Calculates the final score by multiplying the original score by type and time factors 3. Sorts memories by the reranked scores 4. Selects the top-K memories based on the output limit 5. Formats memories for output with timestamps if available 6. Stores both the formatted output and the memory list in the context PrintMemoryOp Functionality PrintMemoryOp formats the retrieved memories for display to the user. It provides a clean, structured representation of the memory content. Parameters No specific parameters for this operation. Implementation Details The operation: 1. Retrieves the memory list from the context 2. Formats each memory with: - Memory index - When to use information - Content - Additional metadata (if available) 3. Joins the formatted memories into a single string 4. Stores the formatted string in the context as the response answer ReadMessageOp Functionality ReadMessageOp fetches unmemorized chat messages from the context. This is useful for retrieving recent conversations that haven't been processed into memories yet. Parameters op.read_message_op.params.contextual_msg_max_count : Maximum number of contextual messages to retrieve (default: 10) Implementation Details The operation: 1. Retrieves chat messages from the context 2. Filters for messages that: - Are not marked as memorized - Contain the target name 3. Flattens the messages into a single list 4. Sorts messages by creation time if available 5. Stores the filtered messages back in the context","title":"Retrieve Ops"},{"location":"personal_memory/personal_retrieve_ops/#setqueryop","text":"","title":"SetQueryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality","text":"SetQueryOp prepares the query for memory retrieval by setting the query and its associated timestamp into the context. It's the first operation in the personal memory retrieval flow.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters","text":"op.set_query_op.params.timestamp : (Optional) Integer timestamp to use instead of the current time. If not provided, the current timestamp will be used.","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details","text":"The operation: 1. Takes the query from the context (which is guaranteed to exist as a flow input requirement) 2. Sets a timestamp (either current time or from parameters) 3. Stores the query and timestamp as a tuple in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#extracttimeop","text":"","title":"ExtractTimeOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_1","text":"ExtractTimeOp identifies and extracts time-related information from the query. It uses an LLM to analyze the query text and determine any temporal references or constraints.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_1","text":"op.extract_time_op.params.language : Language for time extraction (defaults to \"en\")","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_1","text":"The operation: 1. Checks if the query contains datetime keywords 2. If time-related words are found, it prepares a prompt for the LLM with: - System instructions - Few-shot examples - The user's query and current time 3. Parses the LLM response to extract time information (year, month, day, etc.) 4. Stores the extracted time dictionary in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#retrievememoryop","text":"","title":"RetrieveMemoryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_2","text":"RetrieveMemoryOp retrieves memories from the vector store based on the query. It extends the RecallVectorStoreOp class to provide memory retrieval functionality.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_2","text":"op.retrieve_memory_op.params.recall_key : Key in the context to use as the query (default: \"query\") op.retrieve_memory_op.params.top_k : Maximum number of memories to retrieve (default: 3) op.retrieve_memory_op.params.threshold_score : (Optional) Minimum similarity score for memories (filters out memories below this threshold)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_2","text":"The operation: 1. Retrieves the query from the context 2. Searches the vector store for relevant memories based on the query 3. Removes duplicate memories 4. Filters memories by threshold score if specified 5. Stores the retrieved memories in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#semanticrankop","text":"","title":"SemanticRankOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_3","text":"SemanticRankOp ranks memories based on their semantic relevance to the query using an LLM. This improves the quality of retrieved memories by considering deeper semantic relationships beyond vector similarity.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_3","text":"op.semantic_rank_op.params.enable_ranker : Whether to enable semantic ranking (default: true) op.semantic_rank_op.params.output_memory_max_count : Maximum number of memories to output (default: 10)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_3","text":"The operation: 1. Retrieves the memory list from the context 2. If ranking is enabled and there are more memories than the output limit: - Removes duplicates based on content - Formats memories for LLM ranking - Asks the LLM to rank memories by relevance on a scale of 0.0 to 1.0 - Parses the ranking results and applies scores to memories 3. Sorts memories by score 4. Stores the ranked memories in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#fusererankop","text":"","title":"FuseRerankOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_4","text":"FuseRerankOp performs the final reranking of memories by combining multiple factors: semantic scores, memory types, and temporal relevance. It also formats the final output.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_4","text":"op.fuse_rerank_op.params.fuse_score_threshold : Minimum score threshold for memories (default: 0.1) op.fuse_rerank_op.params.fuse_ratio_dict : Dictionary of memory type to score multiplier ratios (default: {\"conversation\": 0.5, \"observation\": 1, \"obs_customized\": 1.2, \"insight\": 2.0}) op.fuse_rerank_op.params.fuse_time_ratio : Score multiplier for time-relevant memories (default: 2.0) op.fuse_rerank_op.params.output_memory_max_count : Maximum number of memories to output (default: 5)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_4","text":"The operation: 1. Retrieves extracted time information and memory list from the context 2. For each memory: - Checks if the memory score is above the threshold - Applies a type-based adjustment factor based on the memory type - Determines time relevance by matching memory time metadata with extracted time - Calculates the final score by multiplying the original score by type and time factors 3. Sorts memories by the reranked scores 4. Selects the top-K memories based on the output limit 5. Formats memories for output with timestamps if available 6. Stores both the formatted output and the memory list in the context","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#printmemoryop","text":"","title":"PrintMemoryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_5","text":"PrintMemoryOp formats the retrieved memories for display to the user. It provides a clean, structured representation of the memory content.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_5","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_5","text":"The operation: 1. Retrieves the memory list from the context 2. Formats each memory with: - Memory index - When to use information - Content - Additional metadata (if available) 3. Joins the formatted memories into a single string 4. Stores the formatted string in the context as the response answer","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#readmessageop","text":"","title":"ReadMessageOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_6","text":"ReadMessageOp fetches unmemorized chat messages from the context. This is useful for retrieving recent conversations that haven't been processed into memories yet.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_6","text":"op.read_message_op.params.contextual_msg_max_count : Maximum number of contextual messages to retrieve (default: 10)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_6","text":"The operation: 1. Retrieves chat messages from the context 2. Filters for messages that: - Are not marked as memorized - Contain the target name 3. Flattens the messages into a single list 4. Sorts messages by creation time if available 5. Stores the filtered messages back in the context","title":"Implementation Details"},{"location":"personal_memory/personal_summary_ops/","text":"InfoFilterOp Purpose Filters messages based on information content scores, retaining only those that include significant information about the user. Parameters op.info_filter_op.params.preserved_scores : Comma-separated string of scores to preserve (default: \"2,3\") op.info_filter_op.params.info_filter_msg_max_size : Maximum size of messages to process (default: 200) Description This operation analyzes messages to determine which ones contain valuable personal information. It uses an LLM to score each message on a scale of 0-3: - 0: No user information - 1: Hypothetical or fictional content - 2: General or time-sensitive information - 3: Clear, important information or explicitly requested records Only messages with scores specified in preserved_scores are retained. Messages are also filtered to exclude those already memorized and to only include messages from the user. GetObservationOp Purpose Extracts general observations about the user from messages that don't contain time-related information. Parameters No specific parameters for this operation. Description This operation processes messages that don't contain time-related keywords. It uses an LLM to extract meaningful observations about the user from these messages. Each observation includes: - Content: The actual observation text - Keywords: Tags that indicate when this observation might be relevant - Source message: The original message that led to this observation The operation creates PersonalMemory objects with observation type \"personal_info\" for each extracted observation. GetObservationWithTimeOp Purpose Extracts observations with time context from messages that contain time-related information. Parameters No specific parameters for this operation. Description This operation is the counterpart to GetObservationOp but focuses specifically on messages containing time-related keywords. It extracts observations while preserving the time context, which is important for memories related to schedules, appointments, or time-specific preferences. The operation creates PersonalMemory objects with observation type \"personal_info_with_time\" for each extracted observation, including the time information in the metadata. LoadTodayMemoryOp Purpose Loads memories created today from the vector store to prevent duplication and enable updating of recent memories. Parameters op.load_today_memory_op.params.top_k : Maximum number of memories to retrieve (default: 50) Description This operation retrieves memories created on the current day using vector store search with date filtering. It converts vector nodes to memory objects and makes them available for deduplication in subsequent operations. This helps ensure that new observations don't create redundant memories for information already captured earlier in the day. ContraRepeatOp Purpose Identifies and removes contradictory or repetitive information from the collected memories. Parameters op.contra_repeat_op.params.contra_repeat_max_count : Maximum number of memories to process (default: 50) op.contra_repeat_op.params.enable_contra_repeat : Whether to enable contradiction/repetition checking (default: true) Description This operation analyzes the combined memories from previous operations (observation_memories, observation_memories_with_time, today_memories) to identify contradictions or redundancies. It uses an LLM to evaluate each memory and mark it as: - \"Contradiction\": Contradicts other memories - \"Contained\": Redundant as the information is already contained in other memories - \"None\": Unique and should be kept Memories marked as contradictory or contained are filtered out, and their IDs are tracked for deletion from the vector store. LongContraRepeatOp Purpose Performs more sophisticated contradiction and redundancy analysis for longer-term memory management. Parameters op.long_contra_repeat_op.params.long_contra_repeat_max_count : Maximum number of memories to process (default: 50) op.long_contra_repeat_op.params.enable_long_contra_repeat : Whether to enable this operation (default: true) Description This operation extends the basic contradiction analysis of ContraRepeatOp with the ability to resolve conflicts by modifying contradictory memories rather than simply removing them. It's particularly useful for managing long-term personal memories where information might evolve over time. For contradictory memories, it can either: - Modify the content to resolve the contradiction - Remove the memory if it's completely invalidated - Keep the most accurate/recent information UpdateInsightOp Purpose Updates existing insight values based on new observations. Parameters op.update_insight_op.params.update_insight_threshold : Minimum relevance score threshold (default: 0.3) op.update_insight_op.params.update_insight_max_count : Maximum number of insights to update (default: 5) Description This operation integrates new observations into existing insights about the user. It: 1. Scores insight memories based on relevance to new observations 2. Selects the top insights that meet the relevance threshold 3. Updates each selected insight using an LLM to incorporate the new information 4. Creates updated insight memories with the original ID but new content This helps maintain accurate and up-to-date insights as new information about the user becomes available. GetReflectionSubjectOp Purpose Generates reflection subjects (topics) from personal memories for insight extraction. Parameters op.get_reflection_subject_op.params.reflect_obs_cnt_threshold : Minimum number of memories required for reflection (default: 10) op.get_reflection_subject_op.params.reflect_num_questions : Maximum number of new subjects to generate (default: 3) Description This operation analyzes a collection of personal memories to identify potential topics for reflection and insight generation. It: 1. Checks if there are sufficient memories for meaningful reflection 2. Extracts existing insight subjects to avoid duplication 3. Uses an LLM to generate new reflection subjects based on memory content 4. Creates insight memory objects for these new subjects The generated subjects serve as focal points for organizing and synthesizing personal information about the user. UpdateVectorStoreOp Purpose Stores the processed memories in the vector database and removes deleted memories. Parameters No specific parameters for this operation. Description This operation is the final step in the personal memory summarization flow. It: 1. Deletes memories that were marked for removal (contradictory or redundant) 2. Inserts new or updated memories into the vector store 3. Records the number of deleted and inserted memories This ensures that the vector store remains up-to-date with the latest processed memories.","title":"Summary Ops"},{"location":"personal_memory/personal_summary_ops/#infofilterop","text":"","title":"InfoFilterOp"},{"location":"personal_memory/personal_summary_ops/#purpose","text":"Filters messages based on information content scores, retaining only those that include significant information about the user.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters","text":"op.info_filter_op.params.preserved_scores : Comma-separated string of scores to preserve (default: \"2,3\") op.info_filter_op.params.info_filter_msg_max_size : Maximum size of messages to process (default: 200)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description","text":"This operation analyzes messages to determine which ones contain valuable personal information. It uses an LLM to score each message on a scale of 0-3: - 0: No user information - 1: Hypothetical or fictional content - 2: General or time-sensitive information - 3: Clear, important information or explicitly requested records Only messages with scores specified in preserved_scores are retained. Messages are also filtered to exclude those already memorized and to only include messages from the user.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getobservationop","text":"","title":"GetObservationOp"},{"location":"personal_memory/personal_summary_ops/#purpose_1","text":"Extracts general observations about the user from messages that don't contain time-related information.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_1","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_1","text":"This operation processes messages that don't contain time-related keywords. It uses an LLM to extract meaningful observations about the user from these messages. Each observation includes: - Content: The actual observation text - Keywords: Tags that indicate when this observation might be relevant - Source message: The original message that led to this observation The operation creates PersonalMemory objects with observation type \"personal_info\" for each extracted observation.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getobservationwithtimeop","text":"","title":"GetObservationWithTimeOp"},{"location":"personal_memory/personal_summary_ops/#purpose_2","text":"Extracts observations with time context from messages that contain time-related information.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_2","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_2","text":"This operation is the counterpart to GetObservationOp but focuses specifically on messages containing time-related keywords. It extracts observations while preserving the time context, which is important for memories related to schedules, appointments, or time-specific preferences. The operation creates PersonalMemory objects with observation type \"personal_info_with_time\" for each extracted observation, including the time information in the metadata.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#loadtodaymemoryop","text":"","title":"LoadTodayMemoryOp"},{"location":"personal_memory/personal_summary_ops/#purpose_3","text":"Loads memories created today from the vector store to prevent duplication and enable updating of recent memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_3","text":"op.load_today_memory_op.params.top_k : Maximum number of memories to retrieve (default: 50)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_3","text":"This operation retrieves memories created on the current day using vector store search with date filtering. It converts vector nodes to memory objects and makes them available for deduplication in subsequent operations. This helps ensure that new observations don't create redundant memories for information already captured earlier in the day.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#contrarepeatop","text":"","title":"ContraRepeatOp"},{"location":"personal_memory/personal_summary_ops/#purpose_4","text":"Identifies and removes contradictory or repetitive information from the collected memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_4","text":"op.contra_repeat_op.params.contra_repeat_max_count : Maximum number of memories to process (default: 50) op.contra_repeat_op.params.enable_contra_repeat : Whether to enable contradiction/repetition checking (default: true)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_4","text":"This operation analyzes the combined memories from previous operations (observation_memories, observation_memories_with_time, today_memories) to identify contradictions or redundancies. It uses an LLM to evaluate each memory and mark it as: - \"Contradiction\": Contradicts other memories - \"Contained\": Redundant as the information is already contained in other memories - \"None\": Unique and should be kept Memories marked as contradictory or contained are filtered out, and their IDs are tracked for deletion from the vector store.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#longcontrarepeatop","text":"","title":"LongContraRepeatOp"},{"location":"personal_memory/personal_summary_ops/#purpose_5","text":"Performs more sophisticated contradiction and redundancy analysis for longer-term memory management.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_5","text":"op.long_contra_repeat_op.params.long_contra_repeat_max_count : Maximum number of memories to process (default: 50) op.long_contra_repeat_op.params.enable_long_contra_repeat : Whether to enable this operation (default: true)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_5","text":"This operation extends the basic contradiction analysis of ContraRepeatOp with the ability to resolve conflicts by modifying contradictory memories rather than simply removing them. It's particularly useful for managing long-term personal memories where information might evolve over time. For contradictory memories, it can either: - Modify the content to resolve the contradiction - Remove the memory if it's completely invalidated - Keep the most accurate/recent information","title":"Description"},{"location":"personal_memory/personal_summary_ops/#updateinsightop","text":"","title":"UpdateInsightOp"},{"location":"personal_memory/personal_summary_ops/#purpose_6","text":"Updates existing insight values based on new observations.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_6","text":"op.update_insight_op.params.update_insight_threshold : Minimum relevance score threshold (default: 0.3) op.update_insight_op.params.update_insight_max_count : Maximum number of insights to update (default: 5)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_6","text":"This operation integrates new observations into existing insights about the user. It: 1. Scores insight memories based on relevance to new observations 2. Selects the top insights that meet the relevance threshold 3. Updates each selected insight using an LLM to incorporate the new information 4. Creates updated insight memories with the original ID but new content This helps maintain accurate and up-to-date insights as new information about the user becomes available.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getreflectionsubjectop","text":"","title":"GetReflectionSubjectOp"},{"location":"personal_memory/personal_summary_ops/#purpose_7","text":"Generates reflection subjects (topics) from personal memories for insight extraction.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_7","text":"op.get_reflection_subject_op.params.reflect_obs_cnt_threshold : Minimum number of memories required for reflection (default: 10) op.get_reflection_subject_op.params.reflect_num_questions : Maximum number of new subjects to generate (default: 3)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_7","text":"This operation analyzes a collection of personal memories to identify potential topics for reflection and insight generation. It: 1. Checks if there are sufficient memories for meaningful reflection 2. Extracts existing insight subjects to avoid duplication 3. Uses an LLM to generate new reflection subjects based on memory content 4. Creates insight memory objects for these new subjects The generated subjects serve as focal points for organizing and synthesizing personal information about the user.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#updatevectorstoreop","text":"","title":"UpdateVectorStoreOp"},{"location":"personal_memory/personal_summary_ops/#purpose_8","text":"Stores the processed memories in the vector database and removes deleted memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_8","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_8","text":"This operation is the final step in the personal memory summarization flow. It: 1. Deletes memories that were marked for removal (contradictory or redundant) 2. Inserts new or updated memories into the vector store 3. Records the number of deleted and inserted memories This ensures that the vector store remains up-to-date with the latest processed memories.","title":"Description"},{"location":"sop_memory/making_sop_memories/","text":"1. Background In LLM application development, we often need to combine multiple basic operations (atomic operations) into more complex workflows. These workflows can handle complex tasks such as data retrieval, code generation, multi-turn dialogues, and more. By combining these atomic operations into Standard Operating Procedures (SOPs), we can: Improve code reusability Simplify implementation of complex tasks Standardize common workflows Reduce development and maintenance costs This document introduces how to combine atomic operations (Ops) to form new composite operation tools using the FlowLLM framework. 2. Technical Solution 2.1 Atomic Operation Definition Each operation (Op) needs to define the following core attributes: class BaseAsyncToolOp : description : str # Description of the operation input_schema : Dict [ str , ParamAttr ] # Input parameter schema definition output_schema : Dict [ str , ParamAttr ] # Output parameter schema definition Where ParamAttr defines parameter type, whether it's required, and other attributes: class ParamAttr : type : Type # Parameter type, such as str, int, Dict, etc. required : bool = True # Whether it must be provided default : Any = None # Default value description : str = \"\" # Parameter description 2.2 SOP Composition Process Step 1: Create Atomic Operation Instances First, instantiate the required atomic operations: from flowllm.op.gallery.mock_op import MockOp from flowllm.op.search.tavily_search_op import TavilySearchOp from flowllm.op.agent.react_v2_op import ReactV2Op # Create atomic operation instances search_op = TavilySearchOp () react_op = ReactV2Op () summary_op = MockOp ( description = \"Summarize search results\" , input_schema = { \"search_results\" : ParamAttr ( type = str , description = \"Search results to summarize\" )}, output_schema = { \"summary\" : ParamAttr ( type = str , description = \"Summarized content\" )} ) Step 2: Define Data Flow Between Operations Set up input-output relationships between operations, defining how data flows between them: # Set input parameter sources react_op . set_input ( \"context\" , \"search_summary\" ) # react_op's context parameter is retrieved from search_summary in memory # Set output parameter destinations search_op . set_output ( \"results\" , \"search_results\" ) # search_op's results output to search_results in memory summary_op . set_output ( \"summary\" , \"search_summary\" ) # summary_op's summary output to search_summary in memory Step 3: Build Operation Flow Graph Use operators to build the operation flow graph, defining execution order and parallel relationships: # Build operation flow graph flow = search_op >> summary_op >> react_op # Or more complex flows # Parallel operations use the | operator, sequential operations use the >> operator complex_flow = ( search_op >> summary_op ) | ( another_search_op >> another_summary_op ) >> react_op Operator explanation: >> : Sequential execution, execute the next operation after the previous one completes | : Parallel execution, execute multiple operations simultaneously Step 4: Create Composite Operation Class Encapsulate the built operation flow into a new composite operation class: class SearchAndReactOp ( BaseToolOp ): description = \"Search for information and generate a response based on search results\" input_schema = ... output_schema = ... def build_flow ( self ): search_op = TavilySearchOp () summary_op = MockOp () react_op = ReactV2Op () # Set data flow search_op . set_output ( \"results\" , \"search_results\" ) summary_op . set_input ( \"search_results\" , \"search_results\" ) summary_op . set_output ( \"summary\" , \"search_summary\" ) react_op . set_input ( \"context\" , \"search_summary\" ) react_op . set_output ( \"response\" , \"response\" ) # Build operation flow graph return search_op >> summary_op >> react_op async def execute ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # Execute operation flow return await self . flow . execute ( inputs )","title":"Making SOP Memories"},{"location":"sop_memory/making_sop_memories/#1-background","text":"In LLM application development, we often need to combine multiple basic operations (atomic operations) into more complex workflows. These workflows can handle complex tasks such as data retrieval, code generation, multi-turn dialogues, and more. By combining these atomic operations into Standard Operating Procedures (SOPs), we can: Improve code reusability Simplify implementation of complex tasks Standardize common workflows Reduce development and maintenance costs This document introduces how to combine atomic operations (Ops) to form new composite operation tools using the FlowLLM framework.","title":"1. Background"},{"location":"sop_memory/making_sop_memories/#2-technical-solution","text":"","title":"2. Technical Solution"},{"location":"sop_memory/making_sop_memories/#21-atomic-operation-definition","text":"Each operation (Op) needs to define the following core attributes: class BaseAsyncToolOp : description : str # Description of the operation input_schema : Dict [ str , ParamAttr ] # Input parameter schema definition output_schema : Dict [ str , ParamAttr ] # Output parameter schema definition Where ParamAttr defines parameter type, whether it's required, and other attributes: class ParamAttr : type : Type # Parameter type, such as str, int, Dict, etc. required : bool = True # Whether it must be provided default : Any = None # Default value description : str = \"\" # Parameter description","title":"2.1 Atomic Operation Definition"},{"location":"sop_memory/making_sop_memories/#22-sop-composition-process","text":"","title":"2.2 SOP Composition Process"},{"location":"sop_memory/making_sop_memories/#step-1-create-atomic-operation-instances","text":"First, instantiate the required atomic operations: from flowllm.op.gallery.mock_op import MockOp from flowllm.op.search.tavily_search_op import TavilySearchOp from flowllm.op.agent.react_v2_op import ReactV2Op # Create atomic operation instances search_op = TavilySearchOp () react_op = ReactV2Op () summary_op = MockOp ( description = \"Summarize search results\" , input_schema = { \"search_results\" : ParamAttr ( type = str , description = \"Search results to summarize\" )}, output_schema = { \"summary\" : ParamAttr ( type = str , description = \"Summarized content\" )} )","title":"Step 1: Create Atomic Operation Instances"},{"location":"sop_memory/making_sop_memories/#step-2-define-data-flow-between-operations","text":"Set up input-output relationships between operations, defining how data flows between them: # Set input parameter sources react_op . set_input ( \"context\" , \"search_summary\" ) # react_op's context parameter is retrieved from search_summary in memory # Set output parameter destinations search_op . set_output ( \"results\" , \"search_results\" ) # search_op's results output to search_results in memory summary_op . set_output ( \"summary\" , \"search_summary\" ) # summary_op's summary output to search_summary in memory","title":"Step 2: Define Data Flow Between Operations"},{"location":"sop_memory/making_sop_memories/#step-3-build-operation-flow-graph","text":"Use operators to build the operation flow graph, defining execution order and parallel relationships: # Build operation flow graph flow = search_op >> summary_op >> react_op # Or more complex flows # Parallel operations use the | operator, sequential operations use the >> operator complex_flow = ( search_op >> summary_op ) | ( another_search_op >> another_summary_op ) >> react_op Operator explanation: >> : Sequential execution, execute the next operation after the previous one completes | : Parallel execution, execute multiple operations simultaneously","title":"Step 3: Build Operation Flow Graph"},{"location":"sop_memory/making_sop_memories/#step-4-create-composite-operation-class","text":"Encapsulate the built operation flow into a new composite operation class: class SearchAndReactOp ( BaseToolOp ): description = \"Search for information and generate a response based on search results\" input_schema = ... output_schema = ... def build_flow ( self ): search_op = TavilySearchOp () summary_op = MockOp () react_op = ReactV2Op () # Set data flow search_op . set_output ( \"results\" , \"search_results\" ) summary_op . set_input ( \"search_results\" , \"search_results\" ) summary_op . set_output ( \"summary\" , \"search_summary\" ) react_op . set_input ( \"context\" , \"search_summary\" ) react_op . set_output ( \"response\" , \"response\" ) # Build operation flow graph return search_op >> summary_op >> react_op async def execute ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # Execute operation flow return await self . flow . execute ( inputs )","title":"Step 4: Create Composite Operation Class"},{"location":"task_memory/task_memory/","text":"Task Memory is a key component of ReMe that allows AI agents to learn from memories and improve their performance on similar tasks in the future. This document explains how task memory works and how to use it in your applications. What is Task Memory? Task Memory represents knowledge extracted from previous task executions, including: - Successful approaches to solving problems - Common pitfalls and failures to avoid - Comparative insights between different approaches Each task memory contains: - when_to_use : Conditions that indicate when this memory is relevant - content : The actual knowledge or memory to be applied - Metadata about the memory's source and utility Configuration Logic Task Memory in ReMe is configured through two main flows: 1. Summary Task Memory The summary_task_memory flow processes conversation trajectories to extract meaningful memories: summary_task_memory : flow_content : trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> update_vector_store_op description : \"Summarizes conversation trajectories or messages into structured memory representations for long-term storage\" This flow: 1. Preprocesses trajectories ( trajectory_preprocess_op ) 2. Extracts memories based on success/failure/comparative analysis 3. Validates memories ( memory_validation_op ) 4. Updates the vector store ( update_vector_store_op ) A simplified version ( summary_task_memory_simple ) is also available for less complex use cases. 2. Retrieve Task Memory The retrieve_task_memory flow fetches relevant memories based on a query: retrieve_task_memory : flow_content : build_query_op >> recall_vector_store_op >> rerank_memory_op >> rewrite_memory_op description : \"Retrieves the most relevant top-k memory from historical data based on the current query to enhance task-solving capabilities\" This flow: 1. Builds a query from the input ( build_query_op ) 2. Recalls relevant memories from the vector store ( recall_vector_store_op ) 3. Reranks memories by relevance ( rerank_memory_op ) 4. Rewrites memories for better context integration ( rewrite_memory_op ) A simplified version ( retrieve_task_memory_simple ) is also available. Basic Usage Here's how to use Task Memory in your application: Step 1: Set Up Your Environment import requests # API configuration BASE_URL = \"http://0.0.0.0:8002/\" WORKSPACE_ID = \"your_workspace_id\" Step 2: Run an Agent and Generate Memories # Run the agent with a query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : \"Your query here\" } ) messages = response . json () . get ( \"messages\" , []) # Summarize the conversation to create task memories response = requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) Step 3: Retrieve Relevant Memories for a New Task # Retrieve memories relevant to a new query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : \"Your new query here\" } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) Step 4: Use Retrieved Memories to Enhance Agent Performance # Augment a new query with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { your_query } \" # Run agent with the augmented query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : augmented_query } ) Complete Example Here's a complete example workflow that demonstrates how to use task memory: def run_agent_with_memory ( query_first , query_second ): # Run agent with second query to build initial memories messages = run_agent ( query = query_second ) # Summarize conversation to create memories requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Retrieve relevant memories for the first query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query_first } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) # Run agent with first query augmented with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query_first } \" return run_agent ( query = augmented_query ) Managing Task Memories Delete a Workspace response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" } ) Dump Memories to Disk response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./\" } ) Load Memories from Disk response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./\" } ) Advanced Features ReMe also provides additional task memory operations: record_task_memory : Update frequency and utility attributes of retrieved memories delete_task_memory : Delete memories based on utility/frequency thresholds For more detailed examples, see the use_task_memory_demo.py file in the cookbook directory of the ReMe project.","title":"Overview"},{"location":"task_memory/task_memory/#what-is-task-memory","text":"Task Memory represents knowledge extracted from previous task executions, including: - Successful approaches to solving problems - Common pitfalls and failures to avoid - Comparative insights between different approaches Each task memory contains: - when_to_use : Conditions that indicate when this memory is relevant - content : The actual knowledge or memory to be applied - Metadata about the memory's source and utility","title":"What is Task Memory?"},{"location":"task_memory/task_memory/#configuration-logic","text":"Task Memory in ReMe is configured through two main flows:","title":"Configuration Logic"},{"location":"task_memory/task_memory/#1-summary-task-memory","text":"The summary_task_memory flow processes conversation trajectories to extract meaningful memories: summary_task_memory : flow_content : trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> update_vector_store_op description : \"Summarizes conversation trajectories or messages into structured memory representations for long-term storage\" This flow: 1. Preprocesses trajectories ( trajectory_preprocess_op ) 2. Extracts memories based on success/failure/comparative analysis 3. Validates memories ( memory_validation_op ) 4. Updates the vector store ( update_vector_store_op ) A simplified version ( summary_task_memory_simple ) is also available for less complex use cases.","title":"1. Summary Task Memory"},{"location":"task_memory/task_memory/#2-retrieve-task-memory","text":"The retrieve_task_memory flow fetches relevant memories based on a query: retrieve_task_memory : flow_content : build_query_op >> recall_vector_store_op >> rerank_memory_op >> rewrite_memory_op description : \"Retrieves the most relevant top-k memory from historical data based on the current query to enhance task-solving capabilities\" This flow: 1. Builds a query from the input ( build_query_op ) 2. Recalls relevant memories from the vector store ( recall_vector_store_op ) 3. Reranks memories by relevance ( rerank_memory_op ) 4. Rewrites memories for better context integration ( rewrite_memory_op ) A simplified version ( retrieve_task_memory_simple ) is also available.","title":"2. Retrieve Task Memory"},{"location":"task_memory/task_memory/#basic-usage","text":"Here's how to use Task Memory in your application:","title":"Basic Usage"},{"location":"task_memory/task_memory/#step-1-set-up-your-environment","text":"import requests # API configuration BASE_URL = \"http://0.0.0.0:8002/\" WORKSPACE_ID = \"your_workspace_id\"","title":"Step 1: Set Up Your Environment"},{"location":"task_memory/task_memory/#step-2-run-an-agent-and-generate-memories","text":"# Run the agent with a query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : \"Your query here\" } ) messages = response . json () . get ( \"messages\" , []) # Summarize the conversation to create task memories response = requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } )","title":"Step 2: Run an Agent and Generate Memories"},{"location":"task_memory/task_memory/#step-3-retrieve-relevant-memories-for-a-new-task","text":"# Retrieve memories relevant to a new query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : \"Your new query here\" } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" )","title":"Step 3: Retrieve Relevant Memories for a New Task"},{"location":"task_memory/task_memory/#step-4-use-retrieved-memories-to-enhance-agent-performance","text":"# Augment a new query with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { your_query } \" # Run agent with the augmented query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : augmented_query } )","title":"Step 4: Use Retrieved Memories to Enhance Agent Performance"},{"location":"task_memory/task_memory/#complete-example","text":"Here's a complete example workflow that demonstrates how to use task memory: def run_agent_with_memory ( query_first , query_second ): # Run agent with second query to build initial memories messages = run_agent ( query = query_second ) # Summarize conversation to create memories requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Retrieve relevant memories for the first query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query_first } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) # Run agent with first query augmented with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query_first } \" return run_agent ( query = augmented_query )","title":"Complete Example"},{"location":"task_memory/task_memory/#managing-task-memories","text":"","title":"Managing Task Memories"},{"location":"task_memory/task_memory/#delete-a-workspace","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" } )","title":"Delete a Workspace"},{"location":"task_memory/task_memory/#dump-memories-to-disk","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./\" } )","title":"Dump Memories to Disk"},{"location":"task_memory/task_memory/#load-memories-from-disk","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./\" } )","title":"Load Memories from Disk"},{"location":"task_memory/task_memory/#advanced-features","text":"ReMe also provides additional task memory operations: record_task_memory : Update frequency and utility attributes of retrieved memories delete_task_memory : Delete memories based on utility/frequency thresholds For more detailed examples, see the use_task_memory_demo.py file in the cookbook directory of the ReMe project.","title":"Advanced Features"},{"location":"task_memory/task_retrieve_ops/","text":"BuildQueryOp Purpose Constructs a query for memory retrieval either from a direct query input or by analyzing conversation messages. Functionality If a direct query is provided in the context, it uses that query If messages are provided in the context, it can: Use an LLM to generate a query based on the conversation context Or create a simple query from recent messages without using an LLM Parameters op.build_query_op.params.enable_llm_build (boolean, default: true ): When true , uses an LLM to generate a query from conversation messages When false , creates a simple query by concatenating recent messages RerankMemoryOp Purpose Reranks and filters recalled memories to ensure the most relevant memories are prioritized. Functionality Reranks memories using LLM-based analysis (optional) Filters memories based on quality scores (optional) Returns the top-k most relevant memories Parameters op.rerank_memory_op.params.enable_llm_rerank (boolean, default: true ): When true , uses an LLM to rerank memories based on their relevance to the query op.rerank_memory_op.params.enable_score_filter (boolean, default: false ): When true , filters memories based on their quality scores op.rerank_memory_op.params.min_score_threshold (float, default: 0.3 ): Minimum score threshold for filtering memories when enable_score_filter is true op.rerank_memory_op.params.top_k (integer, default: 5 ): Number of top memories to retain after reranking RewriteMemoryOp Purpose Rewrites and formats the retrieved memories to make them more relevant and actionable for the current context. Functionality Formats retrieved memories into a structured format Can use an LLM to rewrite memories to better fit the current context (optional) Generates a cohesive context message from multiple memories Parameters op.rewrite_memory_op.params.enable_llm_rewrite (boolean, default: true ): When true , uses an LLM to rewrite the memories to make them more relevant and actionable When false , simply formats the memories without LLM-based rewriting MergeMemoryOp Purpose An alternative to RewriteMemoryOp that merges multiple memories into a single response without using an LLM. Functionality Collects the content from all memories in the memory list Formats them into a single response with a standard structure Adds a prompt to consider the helpful parts when answering the question","title":"Retrieve Ops"},{"location":"task_memory/task_retrieve_ops/#buildqueryop","text":"","title":"BuildQueryOp"},{"location":"task_memory/task_retrieve_ops/#purpose","text":"Constructs a query for memory retrieval either from a direct query input or by analyzing conversation messages.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality","text":"If a direct query is provided in the context, it uses that query If messages are provided in the context, it can: Use an LLM to generate a query based on the conversation context Or create a simple query from recent messages without using an LLM","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters","text":"op.build_query_op.params.enable_llm_build (boolean, default: true ): When true , uses an LLM to generate a query from conversation messages When false , creates a simple query by concatenating recent messages","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#rerankmemoryop","text":"","title":"RerankMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_1","text":"Reranks and filters recalled memories to ensure the most relevant memories are prioritized.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_1","text":"Reranks memories using LLM-based analysis (optional) Filters memories based on quality scores (optional) Returns the top-k most relevant memories","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters_1","text":"op.rerank_memory_op.params.enable_llm_rerank (boolean, default: true ): When true , uses an LLM to rerank memories based on their relevance to the query op.rerank_memory_op.params.enable_score_filter (boolean, default: false ): When true , filters memories based on their quality scores op.rerank_memory_op.params.min_score_threshold (float, default: 0.3 ): Minimum score threshold for filtering memories when enable_score_filter is true op.rerank_memory_op.params.top_k (integer, default: 5 ): Number of top memories to retain after reranking","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#rewritememoryop","text":"","title":"RewriteMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_2","text":"Rewrites and formats the retrieved memories to make them more relevant and actionable for the current context.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_2","text":"Formats retrieved memories into a structured format Can use an LLM to rewrite memories to better fit the current context (optional) Generates a cohesive context message from multiple memories","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters_2","text":"op.rewrite_memory_op.params.enable_llm_rewrite (boolean, default: true ): When true , uses an LLM to rewrite the memories to make them more relevant and actionable When false , simply formats the memories without LLM-based rewriting","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#mergememoryop","text":"","title":"MergeMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_3","text":"An alternative to RewriteMemoryOp that merges multiple memories into a single response without using an LLM.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_3","text":"Collects the content from all memories in the memory list Formats them into a single response with a standard structure Adds a prompt to consider the helpful parts when answering the question","title":"Functionality"},{"location":"task_memory/task_summary_ops/","text":"TrajectoryPreprocessOp Purpose Preprocesses trajectories by validating and classifying them based on their score. Functionality Validates and classifies trajectories as success or failure based on a threshold Modifies tool calls in messages to ensure consistent format Sets context for downstream operators with classified trajectories Parameters op.trajectory_preprocess_op.params.success_threshold (float, default: 1.0 ): The threshold score that determines if a trajectory is considered successful Trajectories with scores greater than or equal to this value are classified as successful TrajectorySegmentationOp Purpose Segments trajectories into meaningful step sequences to enable more granular memory extraction. Functionality Uses LLM to identify logical break points in trajectories Adds segmentation information to trajectory metadata Enables more focused memory extraction from specific parts of conversations Parameters op.trajectory_segmentation_op.params.segment_target (string, default: \"all\" ): Determines which trajectories to segment Options: \"all\" , \"success\" , \"failure\" SuccessExtractionOp Purpose Extracts task memories from successful trajectories. Functionality Processes successful trajectories to identify valuable memories Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions Parameters No specific parameters beyond the LLM configuration. FailureExtractionOp Purpose Extracts task memories from failed trajectories to capture lessons learned from unsuccessful attempts. Functionality Processes failed trajectories to identify pitfalls and mistakes Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions Parameters No specific parameters beyond the LLM configuration. ComparativeExtractionOp Purpose Extracts comparative task memories by comparing different scoring trajectories. Functionality Performs \"soft comparison\" between highest and lowest scoring trajectories Can perform \"hard comparison\" between success and failure trajectories using similarity search Identifies key differences that contributed to success or failure Parameters op.comparative_extraction_op.params.enable_soft_comparison (boolean, default: true ): When true , enables comparison between highest and lowest scoring trajectories op.comparative_extraction_op.params.enable_similarity_comparison (boolean, default: false ): When true , enables similarity-based comparison between success and failure trajectories op.comparative_extraction_op.params.similarity_threshold (float, default: 0.3 ): The threshold for considering two trajectories similar op.comparative_extraction_op.params.max_similarity_sequences (integer, default: 5 ): Maximum number of sequences to compare to avoid computational overload op.comparative_extraction_op.params.max_similarity_pairs (integer, default: 3 ): Maximum number of similar pairs to process MemoryValidationOp Purpose Validates the quality of extracted task memories to ensure they are useful and relevant. Functionality Uses LLM to validate each extracted memory Scores memories based on quality and relevance Filters out low-quality memories based on validation threshold Parameters op.memory_validation_op.params.validation_threshold (float, default: 0.5 ): The minimum score for a memory to be considered valid MemoryDeduplicationOp Purpose Removes duplicate task memories to avoid redundancy in the vector store. Functionality Compares new memories with existing memories in the vector store Uses embedding similarity to identify duplicates Ensures only unique memories are stored Parameters op.memory_deduplication_op.params.similarity_threshold (float, default: 0.5 ): The threshold for considering two memories similar op.memory_deduplication_op.params.max_existing_task_memories (integer, default: 1000 ): Maximum number of existing memories to check against SimpleSummaryOp Purpose A simplified version of memory extraction that processes entire trajectories in one step. Functionality Classifies trajectories as success or failure based on score threshold Extracts memories directly from complete trajectories Useful for simpler use cases where detailed segmentation is not required Parameters op.simple_summary_op.params.success_score_threshold (float, default: 0.9 ): The threshold score that determines if a trajectory is considered successful SimpleComparativeSummaryOp Purpose A simplified version of comparative memory extraction. Functionality Groups trajectories by task ID Compares the highest and lowest scoring trajectories for each task Extracts comparative insights without complex segmentation Parameters No specific parameters beyond the LLM configuration.","title":"Summary Ops"},{"location":"task_memory/task_summary_ops/#trajectorypreprocessop","text":"","title":"TrajectoryPreprocessOp"},{"location":"task_memory/task_summary_ops/#purpose","text":"Preprocesses trajectories by validating and classifying them based on their score.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality","text":"Validates and classifies trajectories as success or failure based on a threshold Modifies tool calls in messages to ensure consistent format Sets context for downstream operators with classified trajectories","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters","text":"op.trajectory_preprocess_op.params.success_threshold (float, default: 1.0 ): The threshold score that determines if a trajectory is considered successful Trajectories with scores greater than or equal to this value are classified as successful","title":"Parameters"},{"location":"task_memory/task_summary_ops/#trajectorysegmentationop","text":"","title":"TrajectorySegmentationOp"},{"location":"task_memory/task_summary_ops/#purpose_1","text":"Segments trajectories into meaningful step sequences to enable more granular memory extraction.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_1","text":"Uses LLM to identify logical break points in trajectories Adds segmentation information to trajectory metadata Enables more focused memory extraction from specific parts of conversations","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_1","text":"op.trajectory_segmentation_op.params.segment_target (string, default: \"all\" ): Determines which trajectories to segment Options: \"all\" , \"success\" , \"failure\"","title":"Parameters"},{"location":"task_memory/task_summary_ops/#successextractionop","text":"","title":"SuccessExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_2","text":"Extracts task memories from successful trajectories.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_2","text":"Processes successful trajectories to identify valuable memories Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_2","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"},{"location":"task_memory/task_summary_ops/#failureextractionop","text":"","title":"FailureExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_3","text":"Extracts task memories from failed trajectories to capture lessons learned from unsuccessful attempts.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_3","text":"Processes failed trajectories to identify pitfalls and mistakes Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_3","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"},{"location":"task_memory/task_summary_ops/#comparativeextractionop","text":"","title":"ComparativeExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_4","text":"Extracts comparative task memories by comparing different scoring trajectories.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_4","text":"Performs \"soft comparison\" between highest and lowest scoring trajectories Can perform \"hard comparison\" between success and failure trajectories using similarity search Identifies key differences that contributed to success or failure","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_4","text":"op.comparative_extraction_op.params.enable_soft_comparison (boolean, default: true ): When true , enables comparison between highest and lowest scoring trajectories op.comparative_extraction_op.params.enable_similarity_comparison (boolean, default: false ): When true , enables similarity-based comparison between success and failure trajectories op.comparative_extraction_op.params.similarity_threshold (float, default: 0.3 ): The threshold for considering two trajectories similar op.comparative_extraction_op.params.max_similarity_sequences (integer, default: 5 ): Maximum number of sequences to compare to avoid computational overload op.comparative_extraction_op.params.max_similarity_pairs (integer, default: 3 ): Maximum number of similar pairs to process","title":"Parameters"},{"location":"task_memory/task_summary_ops/#memoryvalidationop","text":"","title":"MemoryValidationOp"},{"location":"task_memory/task_summary_ops/#purpose_5","text":"Validates the quality of extracted task memories to ensure they are useful and relevant.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_5","text":"Uses LLM to validate each extracted memory Scores memories based on quality and relevance Filters out low-quality memories based on validation threshold","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_5","text":"op.memory_validation_op.params.validation_threshold (float, default: 0.5 ): The minimum score for a memory to be considered valid","title":"Parameters"},{"location":"task_memory/task_summary_ops/#memorydeduplicationop","text":"","title":"MemoryDeduplicationOp"},{"location":"task_memory/task_summary_ops/#purpose_6","text":"Removes duplicate task memories to avoid redundancy in the vector store.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_6","text":"Compares new memories with existing memories in the vector store Uses embedding similarity to identify duplicates Ensures only unique memories are stored","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_6","text":"op.memory_deduplication_op.params.similarity_threshold (float, default: 0.5 ): The threshold for considering two memories similar op.memory_deduplication_op.params.max_existing_task_memories (integer, default: 1000 ): Maximum number of existing memories to check against","title":"Parameters"},{"location":"task_memory/task_summary_ops/#simplesummaryop","text":"","title":"SimpleSummaryOp"},{"location":"task_memory/task_summary_ops/#purpose_7","text":"A simplified version of memory extraction that processes entire trajectories in one step.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_7","text":"Classifies trajectories as success or failure based on score threshold Extracts memories directly from complete trajectories Useful for simpler use cases where detailed segmentation is not required","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_7","text":"op.simple_summary_op.params.success_score_threshold (float, default: 0.9 ): The threshold score that determines if a trajectory is considered successful","title":"Parameters"},{"location":"task_memory/task_summary_ops/#simplecomparativesummaryop","text":"","title":"SimpleComparativeSummaryOp"},{"location":"task_memory/task_summary_ops/#purpose_8","text":"A simplified version of comparative memory extraction.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_8","text":"Groups trajectories by task ID Compares the highest and lowest scoring trajectories for each task Extracts comparative insights without complex segmentation","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_8","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"},{"location":"tool_memory/tool_bench/","text":"Overview This benchmark evaluates Tool Memory effectiveness by comparing agent performance with and without tool memory across multiple epochs. The experiment uses mock search tools with varying performance characteristics for different query complexities. Experimental Setup Mock Search Tools Three LLM-based mock search tools with different performance profiles: Tool Simple Queries Medium Queries Complex Queries SearchToolA \u2b50\u2b50\u2b50 Fast, high success (90%) \u274c Poor (20% success) \u26a0\ufe0f Weak (50% success) SearchToolB \u26a0\ufe0f Over-engineered (30%) \u2b50\u2b50\u2b50 Optimal (90% success) \u26a0\ufe0f Limited (50% success) SearchToolC \u26a0\ufe0f Overkill (30%) \u26a0\ufe0f Excessive (40%) \u2b50\u2b50\u2b50 Best (90% success) Performance Characteristics: - success_rate : Probability of successful execution (vs \"Service busy\" error) - relevance_ratio : Probability of returning relevant results (vs random content) - extra_time : Simulated latency (currently 0 in implementation) Each tool uses LLM to classify query complexity and generate appropriate responses. Query Dataset Source: cookbook/tool_memory/query.json Train Set : 20 queries per complexity \u00d7 3 levels = 60 queries Test Set : 20 queries per complexity \u00d7 3 levels = 60 queries Complexity Levels : simple, moderate, complex Benchmark Workflow Single Epoch Process Each epoch consists of 5 steps: Step 1: Train without Memory # Execute all train queries on TRAIN_WORKSPACE # Agent selects tools without historical guidance run_use_mock_search ( TRAIN_WORKSPACE , train_queries , prompt_template ) # Add results to memory and get scored results train_scored_results = add_tool_call_results ( TRAIN_WORKSPACE , train_results ) Step 2: Test without Memory # Execute all test queries on TEST_WORKSPACE (fresh workspace) # Baseline performance without tool memory run_use_mock_search ( TEST_WORKSPACE , test_queries , prompt_template ) # Add results to memory (will be cleared in Step 4) test_scored_results = add_tool_call_results ( TEST_WORKSPACE , test_results ) Step 3: Summarize Tool Memory # Summarize tool performance from TRAIN_WORKSPACE summarize_tool_memory ( TRAIN_WORKSPACE , \"SearchToolA,SearchToolB,SearchToolC\" ) # Retrieve formatted tool memory content memories = retrieve_tool_memory ( TRAIN_WORKSPACE , tool_names ) The summarization produces memory content including: - Best/worst use cases per tool - Statistical metrics (avg score, success rate, token cost, time cost) - Usage recommendations Step 4: Test with Memory # Clear TEST_WORKSPACE to start fresh delete_workspace ( TEST_WORKSPACE ) # Inject tool memory into prompt prompt_with_memory = f \"Tool Information \\n { memories } \\n Must select one tool to answer \\n Query \\n { query } \" # Execute test queries with memory guidance run_use_mock_search ( TEST_WORKSPACE , test_queries , prompt_with_memory ) # Add results and get scored results test_scored_results_with_memory = add_tool_call_results ( TEST_WORKSPACE , test_results ) Step 5: Compare Results # Generate comparison table print_comparison_table ([ train_no_memory_stats , test_no_memory_stats , test_with_memory_stats ]) # Calculate improvements (baseline: test without memory) improvements = calculate_improvements ( test_no_memory_stats , test_with_memory_stats ) print_improvements ( improvements ) Multi-Epoch Execution # Run benchmark with 3 epochs python cookbook/tool_memory/run_reme_tool_bench.py # Test mode (5 queries per complexity level) main ( test_mode = True, run_epoch = 3 ) # Full mode (20 queries per complexity level) main ( test_mode = False, run_epoch = 3 ) Key Components 1. Tool Selection: UseMockSearchOp # Agent uses LLM to select appropriate tool tool_call = await self . select_tool ( query , [ SearchToolA (), SearchToolB (), SearchToolC ()]) # Execute selected tool and record results result = ToolCallResult ( create_time = timestamp , tool_name = tool_call . name , input = { \"query\" : query }, output = content , token_cost = token_cost , success = success , time_cost = time_cost ) 2. Tool Call Result Evaluation Results are automatically evaluated and scored: - score : 0.0 (failure/irrelevant) or 1.0 (complete success) - success : Tool execution status - summary : Brief description - evaluation : Detailed assessment 3. Tool Memory Schema ToolMemory ( workspace_id = \"workspace_id\" , memory_type = \"tool\" , when_to_use = \"Brief usage scenario description\" , content = \"Detailed performance analysis and recommendations\" , score = 0.85 , tool_call_results = [ list of ToolCallResult ], metadata = { \"tool_name\" : \"SearchToolA\" } ) Evaluation Metrics Per-Scenario Metrics Avg Score : Average quality score (0.0-1.0) Total Calls : Number of tool invocations Success Rate : Percentage of successful executions Improvement Calculation improvement_percentage = (( with_memory_score - without_memory_score ) / without_memory_score ) * 100 Expected Results Hypothesis Tool Memory should enable the agent to: 1. Select optimal tools based on query complexity 2. Improve average score by 10-30% on test set 3. Increase consistency across multiple epochs Sample Output ================================================================================================== BENCHMARK RESULTS COMPARISON ================================================================================================== Note: Avg Score = average quality score +---------------------------+--------------+-----------+ | Scenario | Total Calls | Avg Score | +===========================+==============+===========+ | Epoch1 - Train (No Memory)| 60 | 0.650 | +---------------------------+--------------+-----------+ | Epoch1 - Test (No Memory) | 60 | 0.633 | +---------------------------+--------------+-----------+ | Epoch1 - Test (With Memory)| 60 | 0.817 | +---------------------------+--------------+-----------+ ================================================================================================== IMPROVEMENTS WITH TOOL MEMORY (Baseline: Test without memory) ================================================================================================== Average Score : +29.07% \u2191 ================================================================================================== Running the Benchmark Prerequisites pip install requests python-dotenv loguru tabulate Start API Server # Start ReMe API server python reme_ai/app.py --port 8002 Execute Benchmark # Full benchmark (3 epochs, 60+60 queries per epoch) python cookbook/tool_memory/run_reme_tool_bench.py # Quick test (3 epochs, 15+15 queries per epoch) # Modify main() call: main(test_mode=True, run_epoch=3) Output Files tool_memory_benchmark_results.json : Complete benchmark results Console output: Real-time progress and comparison tables API Endpoints Used /use_mock_search : Execute tool selection and search Input: workspace_id , query Output: ToolCallResult JSON /add_tool_call_result : Add results to memory and get evaluation scores Input: workspace_id , tool_call_results (list) Output: memory_list with scored results /summary_tool_memory : Summarize tool performance Input: workspace_id , tool_names (comma-separated) Output: Updated ToolMemory with content /retrieve_tool_memory : Retrieve formatted tool memory Input: workspace_id , tool_names Output: Markdown-formatted memory content /vector_store : Delete workspace Input: workspace_id , action: \"delete\" Concurrency Control Max workers : 4 parallel queries Rate limiting : 1 second delay between submissions Timeout : 120 seconds per API call References Tool Memory Schema: reme_ai/schema/memory.py Mock Tools Implementation: reme_ai/agent/tools/mock_search_tools.py LLM-based Search Op: reme_ai/agent/tools/llm_mock_search_op.py Tool Selection Op: reme_ai/agent/tools/use_mock_search_op.py","title":"Benchmark"},{"location":"tool_memory/tool_bench/#overview","text":"This benchmark evaluates Tool Memory effectiveness by comparing agent performance with and without tool memory across multiple epochs. The experiment uses mock search tools with varying performance characteristics for different query complexities.","title":"Overview"},{"location":"tool_memory/tool_bench/#experimental-setup","text":"","title":"Experimental Setup"},{"location":"tool_memory/tool_bench/#mock-search-tools","text":"Three LLM-based mock search tools with different performance profiles: Tool Simple Queries Medium Queries Complex Queries SearchToolA \u2b50\u2b50\u2b50 Fast, high success (90%) \u274c Poor (20% success) \u26a0\ufe0f Weak (50% success) SearchToolB \u26a0\ufe0f Over-engineered (30%) \u2b50\u2b50\u2b50 Optimal (90% success) \u26a0\ufe0f Limited (50% success) SearchToolC \u26a0\ufe0f Overkill (30%) \u26a0\ufe0f Excessive (40%) \u2b50\u2b50\u2b50 Best (90% success) Performance Characteristics: - success_rate : Probability of successful execution (vs \"Service busy\" error) - relevance_ratio : Probability of returning relevant results (vs random content) - extra_time : Simulated latency (currently 0 in implementation) Each tool uses LLM to classify query complexity and generate appropriate responses.","title":"Mock Search Tools"},{"location":"tool_memory/tool_bench/#query-dataset","text":"Source: cookbook/tool_memory/query.json Train Set : 20 queries per complexity \u00d7 3 levels = 60 queries Test Set : 20 queries per complexity \u00d7 3 levels = 60 queries Complexity Levels : simple, moderate, complex","title":"Query Dataset"},{"location":"tool_memory/tool_bench/#benchmark-workflow","text":"","title":"Benchmark Workflow"},{"location":"tool_memory/tool_bench/#single-epoch-process","text":"Each epoch consists of 5 steps:","title":"Single Epoch Process"},{"location":"tool_memory/tool_bench/#step-1-train-without-memory","text":"# Execute all train queries on TRAIN_WORKSPACE # Agent selects tools without historical guidance run_use_mock_search ( TRAIN_WORKSPACE , train_queries , prompt_template ) # Add results to memory and get scored results train_scored_results = add_tool_call_results ( TRAIN_WORKSPACE , train_results )","title":"Step 1: Train without Memory"},{"location":"tool_memory/tool_bench/#step-2-test-without-memory","text":"# Execute all test queries on TEST_WORKSPACE (fresh workspace) # Baseline performance without tool memory run_use_mock_search ( TEST_WORKSPACE , test_queries , prompt_template ) # Add results to memory (will be cleared in Step 4) test_scored_results = add_tool_call_results ( TEST_WORKSPACE , test_results )","title":"Step 2: Test without Memory"},{"location":"tool_memory/tool_bench/#step-3-summarize-tool-memory","text":"# Summarize tool performance from TRAIN_WORKSPACE summarize_tool_memory ( TRAIN_WORKSPACE , \"SearchToolA,SearchToolB,SearchToolC\" ) # Retrieve formatted tool memory content memories = retrieve_tool_memory ( TRAIN_WORKSPACE , tool_names ) The summarization produces memory content including: - Best/worst use cases per tool - Statistical metrics (avg score, success rate, token cost, time cost) - Usage recommendations","title":"Step 3: Summarize Tool Memory"},{"location":"tool_memory/tool_bench/#step-4-test-with-memory","text":"# Clear TEST_WORKSPACE to start fresh delete_workspace ( TEST_WORKSPACE ) # Inject tool memory into prompt prompt_with_memory = f \"Tool Information \\n { memories } \\n Must select one tool to answer \\n Query \\n { query } \" # Execute test queries with memory guidance run_use_mock_search ( TEST_WORKSPACE , test_queries , prompt_with_memory ) # Add results and get scored results test_scored_results_with_memory = add_tool_call_results ( TEST_WORKSPACE , test_results )","title":"Step 4: Test with Memory"},{"location":"tool_memory/tool_bench/#step-5-compare-results","text":"# Generate comparison table print_comparison_table ([ train_no_memory_stats , test_no_memory_stats , test_with_memory_stats ]) # Calculate improvements (baseline: test without memory) improvements = calculate_improvements ( test_no_memory_stats , test_with_memory_stats ) print_improvements ( improvements )","title":"Step 5: Compare Results"},{"location":"tool_memory/tool_bench/#multi-epoch-execution","text":"# Run benchmark with 3 epochs python cookbook/tool_memory/run_reme_tool_bench.py # Test mode (5 queries per complexity level) main ( test_mode = True, run_epoch = 3 ) # Full mode (20 queries per complexity level) main ( test_mode = False, run_epoch = 3 )","title":"Multi-Epoch Execution"},{"location":"tool_memory/tool_bench/#key-components","text":"","title":"Key Components"},{"location":"tool_memory/tool_bench/#1-tool-selection-usemocksearchop","text":"# Agent uses LLM to select appropriate tool tool_call = await self . select_tool ( query , [ SearchToolA (), SearchToolB (), SearchToolC ()]) # Execute selected tool and record results result = ToolCallResult ( create_time = timestamp , tool_name = tool_call . name , input = { \"query\" : query }, output = content , token_cost = token_cost , success = success , time_cost = time_cost )","title":"1. Tool Selection: UseMockSearchOp"},{"location":"tool_memory/tool_bench/#2-tool-call-result-evaluation","text":"Results are automatically evaluated and scored: - score : 0.0 (failure/irrelevant) or 1.0 (complete success) - success : Tool execution status - summary : Brief description - evaluation : Detailed assessment","title":"2. Tool Call Result Evaluation"},{"location":"tool_memory/tool_bench/#3-tool-memory-schema","text":"ToolMemory ( workspace_id = \"workspace_id\" , memory_type = \"tool\" , when_to_use = \"Brief usage scenario description\" , content = \"Detailed performance analysis and recommendations\" , score = 0.85 , tool_call_results = [ list of ToolCallResult ], metadata = { \"tool_name\" : \"SearchToolA\" } )","title":"3. Tool Memory Schema"},{"location":"tool_memory/tool_bench/#evaluation-metrics","text":"","title":"Evaluation Metrics"},{"location":"tool_memory/tool_bench/#per-scenario-metrics","text":"Avg Score : Average quality score (0.0-1.0) Total Calls : Number of tool invocations Success Rate : Percentage of successful executions","title":"Per-Scenario Metrics"},{"location":"tool_memory/tool_bench/#improvement-calculation","text":"improvement_percentage = (( with_memory_score - without_memory_score ) / without_memory_score ) * 100","title":"Improvement Calculation"},{"location":"tool_memory/tool_bench/#expected-results","text":"","title":"Expected Results"},{"location":"tool_memory/tool_bench/#hypothesis","text":"Tool Memory should enable the agent to: 1. Select optimal tools based on query complexity 2. Improve average score by 10-30% on test set 3. Increase consistency across multiple epochs","title":"Hypothesis"},{"location":"tool_memory/tool_bench/#sample-output","text":"================================================================================================== BENCHMARK RESULTS COMPARISON ================================================================================================== Note: Avg Score = average quality score +---------------------------+--------------+-----------+ | Scenario | Total Calls | Avg Score | +===========================+==============+===========+ | Epoch1 - Train (No Memory)| 60 | 0.650 | +---------------------------+--------------+-----------+ | Epoch1 - Test (No Memory) | 60 | 0.633 | +---------------------------+--------------+-----------+ | Epoch1 - Test (With Memory)| 60 | 0.817 | +---------------------------+--------------+-----------+ ================================================================================================== IMPROVEMENTS WITH TOOL MEMORY (Baseline: Test without memory) ================================================================================================== Average Score : +29.07% \u2191 ==================================================================================================","title":"Sample Output"},{"location":"tool_memory/tool_bench/#running-the-benchmark","text":"","title":"Running the Benchmark"},{"location":"tool_memory/tool_bench/#prerequisites","text":"pip install requests python-dotenv loguru tabulate","title":"Prerequisites"},{"location":"tool_memory/tool_bench/#start-api-server","text":"# Start ReMe API server python reme_ai/app.py --port 8002","title":"Start API Server"},{"location":"tool_memory/tool_bench/#execute-benchmark","text":"# Full benchmark (3 epochs, 60+60 queries per epoch) python cookbook/tool_memory/run_reme_tool_bench.py # Quick test (3 epochs, 15+15 queries per epoch) # Modify main() call: main(test_mode=True, run_epoch=3)","title":"Execute Benchmark"},{"location":"tool_memory/tool_bench/#output-files","text":"tool_memory_benchmark_results.json : Complete benchmark results Console output: Real-time progress and comparison tables","title":"Output Files"},{"location":"tool_memory/tool_bench/#api-endpoints-used","text":"/use_mock_search : Execute tool selection and search Input: workspace_id , query Output: ToolCallResult JSON /add_tool_call_result : Add results to memory and get evaluation scores Input: workspace_id , tool_call_results (list) Output: memory_list with scored results /summary_tool_memory : Summarize tool performance Input: workspace_id , tool_names (comma-separated) Output: Updated ToolMemory with content /retrieve_tool_memory : Retrieve formatted tool memory Input: workspace_id , tool_names Output: Markdown-formatted memory content /vector_store : Delete workspace Input: workspace_id , action: \"delete\"","title":"API Endpoints Used"},{"location":"tool_memory/tool_bench/#concurrency-control","text":"Max workers : 4 parallel queries Rate limiting : 1 second delay between submissions Timeout : 120 seconds per API call","title":"Concurrency Control"},{"location":"tool_memory/tool_bench/#references","text":"Tool Memory Schema: reme_ai/schema/memory.py Mock Tools Implementation: reme_ai/agent/tools/mock_search_tools.py LLM-based Search Op: reme_ai/agent/tools/llm_mock_search_op.py Tool Selection Op: reme_ai/agent/tools/use_mock_search_op.py","title":"References"},{"location":"tool_memory/tool_memory/","text":"1. Background: Why Tool Memory? The MCP Tool Selection Challenge In modern AI agent systems, LLMs face a rapidly expanding ecosystem of MCP (Model Context Protocol) tools. With hundreds or thousands of available tools, a critical problem emerges: The Core Problem: Tool Description is Not Enough When an LLM faces numerous MCP tools, it relies heavily on tool descriptions to decide which tool to use and how to use it. However: Ambiguous Descriptions : Many tools have similar descriptions but different performance characteristics Hidden Complexity : Static descriptions can't capture runtime behaviors, edge cases, or failure patterns Parameter Confusion : Tools may accept similar parameters with different optimal values No Quality Signal : Descriptions don't tell you which tools are reliable, fast, or cost-effective Example: Web Search Tools Imagine an LLM choosing between three search tools: Tool A : \"Search the web for information\" Tool B : \"Perform web searches with customizable parameters\" Tool C : \"Query search engines and return results\" The descriptions are nearly identical, but in reality: - Tool A: 95% success rate, avg 2.3s, best for technical queries - Tool B: 70% success rate, avg 5.8s, often times out with >20 results - Tool C: 85% success rate, avg 3.1s, good for general queries Without historical data, the LLM can't make informed decisions. The Solution: Tool Memory as Context Enhancement Tool Memory solves this by providing learned context from historical usage , transforming static tool descriptions into dynamic, data-driven guidance: 1. Rule-Based Statistics (Objective Metrics) - Success Rate : \"This tool succeeds 92% of the time\" - Performance : \"Average execution time: 2.3s, token cost: 150\" - Usage Patterns : \"Most successful calls use max_results=10-20\" 2. LLM-as-Judge Evaluation (Qualitative Insights) - Quality Assessment : LLM evaluates each call's effectiveness - Pattern Recognition : Identifies why some calls succeed and others fail - Actionable Recommendations : Synthesizes guidelines from patterns 3. Enhanced Context for LLM Decision-Making Instead of just a tool description, the LLM now receives: Tool : web_search Static Description : \"Search the web for information\" + Tool Memory Context : \"Based on 150 historical calls: - Success rate: 92% (138 successful, 12 failed) - Avg time: 2.3s, Avg tokens: 150 - Best for: Technical documentation, tutorials (95% success) - Optimal params: max_results=5-20, language='en' - Common failures: Generic queries timeout, max_results>50 unreliable - Recommendation: Use specific multi-word queries with filter_type='technical_docs'\" This enriched context enables the LLM to: - Choose the right tool based on task requirements and reliability - Use optimal parameters learned from successful historical calls - Avoid known pitfalls that caused previous failures - Estimate costs (time and tokens) before execution The Impact: From Static Descriptions to Dynamic Intelligence Traditional Approach (Static Descriptions Only): LLM : \"I have 50 search tools, all with similar descriptions\" \u2192 Random choice or first match \u2192 Trial - and - error parameter selection \u2192 75 % success rate , repeated failures Tool Memory Approach (Description + Historical Context): LLM : \"I have 50 search tools, but Tool A has 95 % s uccess for technical queries\" \u2192 Informed choice based on data \u2192 Use proven parameter configurations \u2192 92 % success rate , optimized performance Real-World Impact: Before Tool Memory : - Success rate : 75 % - Average time cost : 5.2 s - Token cost : 200 + per call - Repeated parameter errors - Random tool selection After Tool Memory : - Success rate : 92 % ( + 17 % ) - Average time cost : 2.8 s ( - 46 % ) - Token cost : 150 per call ( - 25 % ) - Consistent best practices - Data - driven tool selection Why This Matters for MCP Ecosystem As the MCP ecosystem grows, Tool Memory becomes essential: Scalability : LLMs can navigate thousands of tools with confidence Quality Control : Tools with poor performance get flagged automatically Continuous Improvement : Every call improves the knowledge base Transfer Learning : Insights from one agent benefit all agents in the workspace Tool Memory transforms tool descriptions from static documentation into living, learned manuals that improve with every use. 2. What is Tool Memory? Tool Memory is a structured knowledge base that captures insights from tool usage history. Each Tool Memory represents accumulated wisdom about a specific tool. Data Structure ToolMemory ToolMemory is the core data structure that stores comprehensive information about a tool's usage patterns: class ToolMemory ( BaseMemory ): memory_type : str = \"tool\" # Type identifier workspace_id : str # Workspace identifier memory_id : str # Unique memory ID when_to_use : str # Tool name (serves as unique identifier) content : str # Synthesized usage guidelines score : float # Overall quality score time_created : str # Creation timestamp time_modified : str # Last modification timestamp author : str # Creator (typically LLM model name) tool_call_results : List [ ToolCallResult ] # Historical invocation records metadata : dict # Additional metadata Key Fields: - when_to_use : The tool name, used as the unique identifier for retrieval - content : Human-readable usage guidelines synthesized from historical data - tool_call_results : Complete history of tool invocations with evaluations - score : Overall quality metric for the tool's performance ToolCallResult Each tool invocation is captured as a ToolCallResult : class ToolCallResult ( BaseModel ): create_time : str # Invocation timestamp tool_name : str # Name of the tool input : dict | str # Input parameters output : str # Tool output token_cost : int # Token consumption success : bool # Whether invocation succeeded time_cost : float # Time consumed (seconds) summary : str # Brief summary of the result evaluation : str # Detailed evaluation (generated by LLM) score : float # Evaluation score (0.0 for failure, 1.0 for success) metadata : dict # Additional metadata Key Fields: - input / output : The complete I/O data for analysis - summary : LLM-generated brief summary of what happened - evaluation : LLM-generated detailed analysis of the call quality - score : Binary evaluation (0.0 = failure, 1.0 = success) - Performance metrics : time_cost , token_cost , success for statistical analysis Tool Memory Lifecycle graph LR A [ Tool Call ] --> B [ Evaluate ] B --> C [ Store Memory ] C --> D [( Vector Store )] D --> E [ Agent Retrieves ] E --> A C - . Periodic . -> F [ Summarize ] F --> C 3. How Tool Memory Works: The Complete Flow Tool Memory operates through three complementary operations that work together to create a learning loop: graph LR A [ Agent ] -->| 1 retrieve_tool_memory | B [( Vector Store )] B -->| Guidelines | A A -->| 2 Execute Tool | C [ Tool ] C -->| Result | A A -->| 3 add_tool_call_result | D [ LLM Evaluate ] D -->| Store | B B -->| Periodic | E [ summary_tool_memory ] E -->| 4 Update Guidelines | B Operation Flow 1. retrieve_tool_memory (Before Execution) - Agent queries: \"How should I use web_search tool?\" - Retrieves stored guidelines and historical patterns - Returns: Usage recommendations, parameter suggestions, common pitfalls 2. Tool Execution - Agent executes tool with informed parameters - Collects: input, output, time_cost, token_cost, success status 3. add_tool_call_result (After Execution) - Submits execution data for evaluation - LLM analyzes: Was it successful? What could be improved? - Generates: summary, evaluation, score (0.0 or 1.0) - Appends to tool's historical record in Vector Store 4. summary_tool_memory (Periodic) - Analyzes recent N tool calls (e.g., last 20-30) - Calculates statistics: success rate, avg costs, avg score - LLM synthesizes: Actionable usage guidelines - Updates the content field with comprehensive guidance Example Flow from Demo Based on use_tool_memory_demo.py , here's a typical workflow: # Step 1: Add tool call results (accumulate history) add_tool_call_results ([ { \"tool_name\" : \"web_search\" , \"input\" : { ... }, \"output\" : \"...\" , \"success\" : True }, { \"tool_name\" : \"web_search\" , \"input\" : { ... }, \"output\" : \"...\" , \"success\" : False }, # ... more results ]) # Step 2: Generate usage guidelines (periodic) summarize_tool_memory ( \"web_search\" ) # Step 3: Retrieve guidelines before next use memory = retrieve_tool_memory ( \"web_search\" ) # Returns: # \"For web_search tool: # - Use max_results=5-20 for optimal performance # - Avoid generic queries, be specific # - Language parameter 'en' has 95% success rate # Statistics: 83% success, avg 2.3s, avg 150 tokens\" # Step 4: Agent uses guidelines for better execution execute_with_recommended_parameters () 4. Operation Details: How to Use Each Component 4.1 add_tool_call_result Purpose : Evaluate and store tool call results into Tool Memory. Flow : add_tool_call_result : flow_content : parse_tool_call_result_op >> update_vector_store_op description : \"Evaluates and adds tool call results to the tool memory database\" Process : 1. Receives raw tool call results 2. Uses LLM to evaluate each call (generates summary, evaluation, score) 3. Groups results by tool name 4. Creates or updates ToolMemory objects 5. Stores in Vector Store Configuration ( default.yaml ): op : parse_tool_call_result_op : backend : parse_tool_call_result_op llm : default params : max_history_tool_call_cnt : 100 # Max calls to retain per tool evaluation_sleep_interval : 1.0 # Delay between evaluations (seconds) Usage with curl curl -X POST http://0.0.0.0:8002/add_tool_call_result \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_call_results\": [ { \"create_time\": \"2025-10-21 10:30:00\", \"tool_name\": \"web_search\", \"input\": { \"query\": \"Python asyncio tutorial\", \"max_results\": 10, \"language\": \"en\" }, \"output\": \"Found 10 relevant results including official docs and tutorials\", \"token_cost\": 150, \"success\": true, \"time_cost\": 2.3 }, { \"create_time\": \"2025-10-21 10:32:00\", \"tool_name\": \"web_search\", \"input\": { \"query\": \"test\", \"max_results\": 100, \"language\": \"unknown\" }, \"output\": \"Error: Invalid language parameter\", \"token_cost\": 50, \"success\": false, \"time_cost\": 0.5 } ] }' Response : { \"success\" : true , \"answer\" : \"Successfully evaluated and stored 2 tool call results\" , \"metadata\" : { \"memory_list\" : [ { \"when_to_use\" : \"web_search\" , \"memory_id\" : \"abc123...\" , \"tool_call_results\" : [ { \"tool_name\" : \"web_search\" , \"summary\" : \"Successfully retrieved relevant Python asyncio documentation\" , \"evaluation\" : \"Good parameter choices with appropriate max_results and language settings\" , \"score\" : 1.0 , ... }, { \"tool_name\" : \"web_search\" , \"summary\" : \"Failed due to invalid language parameter\" , \"evaluation\" : \"Query too generic and language parameter not supported\" , \"score\" : 0.0 , ... } ] } ] } } Usage with Python import requests from datetime import datetime def add_tool_call_results ( tool_call_results : list ) -> dict : \"\"\"Add tool call results to Tool Memory\"\"\" response = requests . post ( url = f \" { BASE_URL } add_tool_call_result\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_call_results\" : tool_call_results } ) return response . json () # Example: Record a tool invocation result = add_tool_call_results ([{ \"create_time\" : datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ), \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio\" , \"max_results\" : 10 }, \"output\" : \"Found 10 relevant results...\" , \"token_cost\" : 150 , \"success\" : True , \"time_cost\" : 2.3 }]) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code. 4.2 retrieve_tool_memory Purpose : Retrieve usage guidelines and historical data for specific tools. Flow : retrieve_tool_memory : flow_content : retrieve_tool_memory_op description : \"Retrieves tool memories from the vector database based on tool names\" Process : 1. Takes comma-separated tool names as input 2. Searches Vector Store for exact matches (by when_to_use field) 3. Returns complete ToolMemory objects with: - Usage guidelines ( content ) - Historical call records ( tool_call_results ) - Statistics and metadata Usage with curl # Retrieve single tool curl -X POST http://0.0.0.0:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search\" }' # Retrieve multiple tools (comma-separated) curl -X POST http://0.0.0.0:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search,database_query,file_processor\" }' Response : { \"success\" : true , \"answer\" : \"Successfully retrieved 1 tool memories\" , \"metadata\" : { \"memory_list\" : [ { \"memory_type\" : \"tool\" , \"workspace_id\" : \"my_workspace\" , \"memory_id\" : \"abc123...\" , \"when_to_use\" : \"web_search\" , \"content\" : \"## Usage Guidelines\\n\\n**Best Practices:**\\n- Use max_results between 5-20 for optimal performance\\n- Always specify language parameter (en has 95% success rate)\\n- Avoid generic single-word queries\\n\\n**Common Pitfalls:**\\n- max_results > 50 often causes timeouts\\n- Unknown language values default to 'en' with warning\\n\\n## Statistics\\n- **Success Rate**: 83.33%\\n- **Average Score**: 0.833\\n- **Average Time Cost**: 2.345s\\n- **Average Token Cost**: 156.7\" , \"score\" : 0.85 , \"time_created\" : \"2025-10-20 10:00:00\" , \"time_modified\" : \"2025-10-21 10:35:00\" , \"author\" : \"gpt-4\" , \"tool_call_results\" : [ { \"create_time\" : \"2025-10-21 10:30:00\" , \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio\" , \"max_results\" : 10 }, \"output\" : \"Found 10 results...\" , \"summary\" : \"Successfully retrieved relevant documentation\" , \"evaluation\" : \"Good parameter choices...\" , \"score\" : 1.0 , \"token_cost\" : 150 , \"success\" : true , \"time_cost\" : 2.3 } // ... more historical calls ] } ] } } Usage with Python import requests def retrieve_tool_memory ( tool_names : str ) -> dict : \"\"\"Retrieve tool memories by tool names\"\"\" response = requests . post ( url = f \" { BASE_URL } retrieve_tool_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_names\" : tool_names } ) return response . json () # Example: Retrieve and use guidelines result = retrieve_tool_memory ( \"web_search\" ) if result [ 'success' ]: memory = result [ 'metadata' ][ 'memory_list' ][ 0 ] print ( f \"Tool: { memory [ 'when_to_use' ] } \" ) print ( f \"Guidelines: \\n { memory [ 'content' ] } \" ) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code. 4.3 summary_tool_memory Purpose : Analyze historical tool calls and generate comprehensive usage guidelines. Flow : summary_tool_memory : flow_content : summary_tool_memory_op >> update_vector_store_op description : \"Analyzes tool call history and generates comprehensive usage patterns\" Process : 1. Retrieves existing ToolMemory by tool name 2. Analyzes recent N tool calls (default: 30) 3. Calculates statistics: - Success rate - Average score - Average time cost - Average token cost 4. Uses LLM to synthesize actionable guidelines from call summaries 5. Appends statistics to guidelines 6. Updates ToolMemory content in Vector Store Configuration ( default.yaml ): op : summary_tool_memory_op : backend : summary_tool_memory_op llm : default params : recent_call_count : 30 # Number of recent calls to analyze summary_sleep_interval : 1.0 # Delay between summaries (seconds) Usage with curl # Summarize single tool curl -X POST http://0.0.0.0:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search\" }' # Summarize multiple tools (comma-separated) curl -X POST http://0.0.0.0:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search,database_query,file_processor\" }' Response : { \"success\" : true , \"answer\" : \"Successfully summarized 1 tool memories\" , \"metadata\" : { \"memory_list\" : [ { \"memory_type\" : \"tool\" , \"when_to_use\" : \"web_search\" , \"content\" : \"## Usage Guidelines\\n\\n**Optimal Parameters:**\\n- Set max_results between 5-20 for best balance of coverage and speed\\n- Always specify language='en' for technical queries (95% success rate)\\n- Use filter_type='technical_docs' for development-related searches\\n\\n**Success Patterns:**\\n- Specific, multi-word queries perform significantly better than generic terms\\n- Queries with clear intent (e.g., 'Python asyncio tutorial') return high-quality results\\n- Technical terms and version numbers improve result relevance\\n\\n**Common Failures:**\\n- Generic single-word queries (e.g., 'test') return poor results\\n- max_results > 50 increases timeout risk (5 failures observed)\\n- Invalid language codes cause fallback to default with warnings\\n\\n**Performance Insights:**\\n- Typical response time: 1.5-3.5s for successful queries\\n- Timeout threshold: 10s (consider simplifying complex queries)\\n- Token cost scales with result count: ~150 tokens for 10 results\\n\\n**Recommendations:**\\n1. Always validate language parameter before calling\\n2. Start with max_results=10, adjust based on needs\\n3. For time-sensitive operations, set timeout < 5s\\n4. Monitor token costs for high-frequency usage\\n\\n## Statistics\\n- **Success Rate**: 83.33%\\n- **Average Score**: 0.833\\n- **Average Time Cost**: 2.345s\\n- **Average Token Cost**: 156.7\" , \"memory_id\" : \"abc123...\" , \"time_modified\" : \"2025-10-21 10:40:00\" , ... } ] } } Usage with Python import requests def summarize_tool_memory ( tool_names : str ) -> dict : \"\"\"Generate comprehensive usage guidelines for tools\"\"\" response = requests . post ( url = f \" { BASE_URL } summary_tool_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_names\" : tool_names } ) return response . json () # Example: Generate guidelines result = summarize_tool_memory ( \"web_search\" ) if result [ 'success' ]: memory = result [ 'metadata' ][ 'memory_list' ][ 0 ] print ( f \"Tool: { memory [ 'when_to_use' ] } \" ) print ( f \"Guidelines: \\n { memory [ 'content' ] } \" ) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code. 5. Best Practices When to Record Tool Calls Always : Record every tool invocation, including failures Include : Complete input parameters, output, and performance metrics Timing : Record immediately after tool execution completes When to Generate Summaries Initial : After accumulating 20-30 tool calls for meaningful patterns Periodic : Re-summarize every 50-100 new calls or weekly Trigger-based : When success rate drops or patterns change significantly When to Retrieve Guidelines Before first use : Always retrieve before using an unfamiliar tool Before critical operations : Check latest guidelines for important tasks After updates : Re-retrieve when tool memory has been updated Performance Tuning For High-Volume Tools (>100 calls/day): op : parse_tool_call_result_op : params : max_history_tool_call_cnt : 200 # Keep more history evaluation_sleep_interval : 0.5 # Faster evaluation summary_tool_memory_op : params : recent_call_count : 50 # Analyze more calls For Low-Volume Tools (<20 calls/day): op : parse_tool_call_result_op : params : max_history_tool_call_cnt : 50 # Less history needed evaluation_sleep_interval : 1.0 # Standard rate summary_tool_memory_op : params : recent_call_count : 20 # Analyze fewer calls Quality Maintenance Monitor Metrics : memory = retrieve_tool_memory ( \"web_search\" )[ 'metadata' ][ 'memory_list' ][ 0 ] stats = ToolMemory ( ** memory ) . statistic ( recent_frequency = 30 ) print ( f \"Success Rate: { stats [ 'success_rate' ] : .2% } \" ) print ( f \"Avg Score: { stats [ 'avg_score' ] : .2f } \" ) if stats [ 'success_rate' ] < 0.7 : print ( \"\u26a0\ufe0f Low success rate - investigate tool issues\" ) ``` 2. ** Clean Old Memories ** : - Delete tool memories for deprecated tools - Reset memories when tool behavior changes significantly 3. ** Validate Guidelines ** : - Periodically review generated guidelines for accuracy - Test recommended parameters in production scenarios ## 6. Memory Management ### Delete Workspace ``` bash curl - X POST http : // 0.0.0.0 : 8002 / vector_store \\ - H \"Content-Type: application/json\" \\ - d '{ \"workspace_id\" : \"my_workspace\" , \"action\" : \"delete\" } ' def delete_workspace ( workspace_id : str ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"delete\" } ) return response . json () Dump Memories to Disk curl -X POST http://0.0.0.0:8002/vector_store \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"action\": \"dump\", \"path\": \"./memory_backup/\" }' def dump_memory ( workspace_id : str , path : str = \"./\" ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"dump\" , \"path\" : path } ) return response . json () Load Memories from Disk curl -X POST http://0.0.0.0:8002/vector_store \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"action\": \"load\", \"path\": \"./memory_backup/\" }' def load_memory ( workspace_id : str , path : str = \"./\" ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"load\" , \"path\" : path } ) return response . json () 7. Complete Working Example For a complete, runnable example demonstrating the full Tool Memory lifecycle, see: cookbook/simple_demo/use_tool_memory_demo.py This demo includes: - Workspace management : Clean, delete, dump, and load operations - Tool call recording : Adding 30+ mock tool invocations with various scenarios - Summarization : Generating usage guidelines from historical data - Retrieval : Fetching and displaying tool memories - Statistics : Analyzing success rates, costs, and performance Run the demo: cd cookbook/simple_demo python use_tool_memory_demo.py Key Workflow Steps: 1. Clean workspace : Remove existing data 2. Add tool calls : Record 30+ invocations (success/failure scenarios) 3. Generate guidelines : LLM analyzes patterns and creates recommendations 4. Retrieve memory : Get usage guidelines for agent consumption 5. Persistence : Test dump/load operations 8. Advanced Use Cases Use Case 1: Adaptive Parameter Tuning Retrieve tool memory statistics and adapt parameters based on historical performance: - If avg_time_cost > 5s : Increase timeout - If success_rate < 80% : Enable retry logic - If avg_token_cost high: Reduce result limits Use Case 2: Multi-Tool Workflow Optimization Retrieve memories for multiple tools at once and optimize workflow order based on: - Success rates: Execute reliable tools first - Time costs: Parallelize slow operations - Token costs: Budget-aware tool selection Use Case 3: Automated Quality Monitoring Periodically check tool memory statistics and alert on: - Success rate degradation - Increasing time/token costs - Unusual failure patterns Implementation examples : See cookbook/simple_demo/use_tool_memory_demo.py and the ToolBench evaluation scripts. 9. Benchmark Results Tool Memory Performance Evaluation We evaluated Tool Memory effectiveness using a controlled benchmark with three mock search tools, each optimized for different query complexity levels (simple, moderate, complex). The benchmark compares agent performance with and without tool memory guidance across multiple epochs. Experimental Settings: - Model : Qwen3-30B-Instruct with default parameters - Task : Single-turn tool selection and invocation - Dataset : 60 training queries + 60 test queries per epoch - Tools : 3 mock search tools with varying performance profiles - Metrics : Average quality score (0.0-1.0) based on LLM evaluation - Baseline : Test set performance without tool memory - Replication : Results averaged across 3 independent experimental runs Results (averaged across 3 epochs): Scenario Avg Score Improvement Train (No Memory) 0.650 - Test (No Memory) 0.672 Baseline Test (With Memory) 0.772 +14.88% Key Findings: - Consistent improvement : Tool Memory boosted test performance by ~15% on average - Knowledge transfer : Training data successfully informed test-time tool selection - Stability : Improvement remained consistent across all 3 epochs (9.90% \u2192 17.39% \u2192 17.13%) The benchmark demonstrates that Tool Memory enables agents to make data-driven tool selection decisions, significantly improving task success rates compared to relying solely on static tool descriptions. Benchmark Resources: - Design Documentation : docs/tool_memory/tool_bench.md - Complete benchmark methodology and workflow - Implementation : cookbook/tool_memory/run_reme_tool_bench.py - Full benchmark script - Query Dataset : cookbook/tool_memory/query.json - 60 train + 60 test queries across 3 complexity levels 10. References Implementation : See reme_ai/summary/tool/ and reme_ai/retrieve/tool/ Demo : cookbook/simple_demo/use_tool_memory_demo.py Benchmark : cookbook/tool_memory/run_reme_tool_bench.py Schema : reme_ai/schema/memory.py Utilities : reme_ai/utils/tool_memory_utils.py","title":"Overview"},{"location":"tool_memory/tool_memory/#1-background-why-tool-memory","text":"","title":"1. Background: Why Tool Memory?"},{"location":"tool_memory/tool_memory/#the-mcp-tool-selection-challenge","text":"In modern AI agent systems, LLMs face a rapidly expanding ecosystem of MCP (Model Context Protocol) tools. With hundreds or thousands of available tools, a critical problem emerges: The Core Problem: Tool Description is Not Enough When an LLM faces numerous MCP tools, it relies heavily on tool descriptions to decide which tool to use and how to use it. However: Ambiguous Descriptions : Many tools have similar descriptions but different performance characteristics Hidden Complexity : Static descriptions can't capture runtime behaviors, edge cases, or failure patterns Parameter Confusion : Tools may accept similar parameters with different optimal values No Quality Signal : Descriptions don't tell you which tools are reliable, fast, or cost-effective Example: Web Search Tools Imagine an LLM choosing between three search tools: Tool A : \"Search the web for information\" Tool B : \"Perform web searches with customizable parameters\" Tool C : \"Query search engines and return results\" The descriptions are nearly identical, but in reality: - Tool A: 95% success rate, avg 2.3s, best for technical queries - Tool B: 70% success rate, avg 5.8s, often times out with >20 results - Tool C: 85% success rate, avg 3.1s, good for general queries Without historical data, the LLM can't make informed decisions.","title":"The MCP Tool Selection Challenge"},{"location":"tool_memory/tool_memory/#the-solution-tool-memory-as-context-enhancement","text":"Tool Memory solves this by providing learned context from historical usage , transforming static tool descriptions into dynamic, data-driven guidance: 1. Rule-Based Statistics (Objective Metrics) - Success Rate : \"This tool succeeds 92% of the time\" - Performance : \"Average execution time: 2.3s, token cost: 150\" - Usage Patterns : \"Most successful calls use max_results=10-20\" 2. LLM-as-Judge Evaluation (Qualitative Insights) - Quality Assessment : LLM evaluates each call's effectiveness - Pattern Recognition : Identifies why some calls succeed and others fail - Actionable Recommendations : Synthesizes guidelines from patterns 3. Enhanced Context for LLM Decision-Making Instead of just a tool description, the LLM now receives: Tool : web_search Static Description : \"Search the web for information\" + Tool Memory Context : \"Based on 150 historical calls: - Success rate: 92% (138 successful, 12 failed) - Avg time: 2.3s, Avg tokens: 150 - Best for: Technical documentation, tutorials (95% success) - Optimal params: max_results=5-20, language='en' - Common failures: Generic queries timeout, max_results>50 unreliable - Recommendation: Use specific multi-word queries with filter_type='technical_docs'\" This enriched context enables the LLM to: - Choose the right tool based on task requirements and reliability - Use optimal parameters learned from successful historical calls - Avoid known pitfalls that caused previous failures - Estimate costs (time and tokens) before execution","title":"The Solution: Tool Memory as Context Enhancement"},{"location":"tool_memory/tool_memory/#the-impact-from-static-descriptions-to-dynamic-intelligence","text":"Traditional Approach (Static Descriptions Only): LLM : \"I have 50 search tools, all with similar descriptions\" \u2192 Random choice or first match \u2192 Trial - and - error parameter selection \u2192 75 % success rate , repeated failures Tool Memory Approach (Description + Historical Context): LLM : \"I have 50 search tools, but Tool A has 95 % s uccess for technical queries\" \u2192 Informed choice based on data \u2192 Use proven parameter configurations \u2192 92 % success rate , optimized performance Real-World Impact: Before Tool Memory : - Success rate : 75 % - Average time cost : 5.2 s - Token cost : 200 + per call - Repeated parameter errors - Random tool selection After Tool Memory : - Success rate : 92 % ( + 17 % ) - Average time cost : 2.8 s ( - 46 % ) - Token cost : 150 per call ( - 25 % ) - Consistent best practices - Data - driven tool selection","title":"The Impact: From Static Descriptions to Dynamic Intelligence"},{"location":"tool_memory/tool_memory/#why-this-matters-for-mcp-ecosystem","text":"As the MCP ecosystem grows, Tool Memory becomes essential: Scalability : LLMs can navigate thousands of tools with confidence Quality Control : Tools with poor performance get flagged automatically Continuous Improvement : Every call improves the knowledge base Transfer Learning : Insights from one agent benefit all agents in the workspace Tool Memory transforms tool descriptions from static documentation into living, learned manuals that improve with every use.","title":"Why This Matters for MCP Ecosystem"},{"location":"tool_memory/tool_memory/#2-what-is-tool-memory","text":"Tool Memory is a structured knowledge base that captures insights from tool usage history. Each Tool Memory represents accumulated wisdom about a specific tool.","title":"2. What is Tool Memory?"},{"location":"tool_memory/tool_memory/#data-structure","text":"","title":"Data Structure"},{"location":"tool_memory/tool_memory/#toolmemory","text":"ToolMemory is the core data structure that stores comprehensive information about a tool's usage patterns: class ToolMemory ( BaseMemory ): memory_type : str = \"tool\" # Type identifier workspace_id : str # Workspace identifier memory_id : str # Unique memory ID when_to_use : str # Tool name (serves as unique identifier) content : str # Synthesized usage guidelines score : float # Overall quality score time_created : str # Creation timestamp time_modified : str # Last modification timestamp author : str # Creator (typically LLM model name) tool_call_results : List [ ToolCallResult ] # Historical invocation records metadata : dict # Additional metadata Key Fields: - when_to_use : The tool name, used as the unique identifier for retrieval - content : Human-readable usage guidelines synthesized from historical data - tool_call_results : Complete history of tool invocations with evaluations - score : Overall quality metric for the tool's performance","title":"ToolMemory"},{"location":"tool_memory/tool_memory/#toolcallresult","text":"Each tool invocation is captured as a ToolCallResult : class ToolCallResult ( BaseModel ): create_time : str # Invocation timestamp tool_name : str # Name of the tool input : dict | str # Input parameters output : str # Tool output token_cost : int # Token consumption success : bool # Whether invocation succeeded time_cost : float # Time consumed (seconds) summary : str # Brief summary of the result evaluation : str # Detailed evaluation (generated by LLM) score : float # Evaluation score (0.0 for failure, 1.0 for success) metadata : dict # Additional metadata Key Fields: - input / output : The complete I/O data for analysis - summary : LLM-generated brief summary of what happened - evaluation : LLM-generated detailed analysis of the call quality - score : Binary evaluation (0.0 = failure, 1.0 = success) - Performance metrics : time_cost , token_cost , success for statistical analysis","title":"ToolCallResult"},{"location":"tool_memory/tool_memory/#tool-memory-lifecycle","text":"graph LR A [ Tool Call ] --> B [ Evaluate ] B --> C [ Store Memory ] C --> D [( Vector Store )] D --> E [ Agent Retrieves ] E --> A C - . Periodic . -> F [ Summarize ] F --> C","title":"Tool Memory Lifecycle"},{"location":"tool_memory/tool_memory/#3-how-tool-memory-works-the-complete-flow","text":"Tool Memory operates through three complementary operations that work together to create a learning loop: graph LR A [ Agent ] -->| 1 retrieve_tool_memory | B [( Vector Store )] B -->| Guidelines | A A -->| 2 Execute Tool | C [ Tool ] C -->| Result | A A -->| 3 add_tool_call_result | D [ LLM Evaluate ] D -->| Store | B B -->| Periodic | E [ summary_tool_memory ] E -->| 4 Update Guidelines | B","title":"3. How Tool Memory Works: The Complete Flow"},{"location":"tool_memory/tool_memory/#operation-flow","text":"1. retrieve_tool_memory (Before Execution) - Agent queries: \"How should I use web_search tool?\" - Retrieves stored guidelines and historical patterns - Returns: Usage recommendations, parameter suggestions, common pitfalls 2. Tool Execution - Agent executes tool with informed parameters - Collects: input, output, time_cost, token_cost, success status 3. add_tool_call_result (After Execution) - Submits execution data for evaluation - LLM analyzes: Was it successful? What could be improved? - Generates: summary, evaluation, score (0.0 or 1.0) - Appends to tool's historical record in Vector Store 4. summary_tool_memory (Periodic) - Analyzes recent N tool calls (e.g., last 20-30) - Calculates statistics: success rate, avg costs, avg score - LLM synthesizes: Actionable usage guidelines - Updates the content field with comprehensive guidance","title":"Operation Flow"},{"location":"tool_memory/tool_memory/#example-flow-from-demo","text":"Based on use_tool_memory_demo.py , here's a typical workflow: # Step 1: Add tool call results (accumulate history) add_tool_call_results ([ { \"tool_name\" : \"web_search\" , \"input\" : { ... }, \"output\" : \"...\" , \"success\" : True }, { \"tool_name\" : \"web_search\" , \"input\" : { ... }, \"output\" : \"...\" , \"success\" : False }, # ... more results ]) # Step 2: Generate usage guidelines (periodic) summarize_tool_memory ( \"web_search\" ) # Step 3: Retrieve guidelines before next use memory = retrieve_tool_memory ( \"web_search\" ) # Returns: # \"For web_search tool: # - Use max_results=5-20 for optimal performance # - Avoid generic queries, be specific # - Language parameter 'en' has 95% success rate # Statistics: 83% success, avg 2.3s, avg 150 tokens\" # Step 4: Agent uses guidelines for better execution execute_with_recommended_parameters ()","title":"Example Flow from Demo"},{"location":"tool_memory/tool_memory/#4-operation-details-how-to-use-each-component","text":"","title":"4. Operation Details: How to Use Each Component"},{"location":"tool_memory/tool_memory/#41-add_tool_call_result","text":"Purpose : Evaluate and store tool call results into Tool Memory. Flow : add_tool_call_result : flow_content : parse_tool_call_result_op >> update_vector_store_op description : \"Evaluates and adds tool call results to the tool memory database\" Process : 1. Receives raw tool call results 2. Uses LLM to evaluate each call (generates summary, evaluation, score) 3. Groups results by tool name 4. Creates or updates ToolMemory objects 5. Stores in Vector Store Configuration ( default.yaml ): op : parse_tool_call_result_op : backend : parse_tool_call_result_op llm : default params : max_history_tool_call_cnt : 100 # Max calls to retain per tool evaluation_sleep_interval : 1.0 # Delay between evaluations (seconds)","title":"4.1 add_tool_call_result"},{"location":"tool_memory/tool_memory/#usage-with-curl","text":"curl -X POST http://0.0.0.0:8002/add_tool_call_result \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_call_results\": [ { \"create_time\": \"2025-10-21 10:30:00\", \"tool_name\": \"web_search\", \"input\": { \"query\": \"Python asyncio tutorial\", \"max_results\": 10, \"language\": \"en\" }, \"output\": \"Found 10 relevant results including official docs and tutorials\", \"token_cost\": 150, \"success\": true, \"time_cost\": 2.3 }, { \"create_time\": \"2025-10-21 10:32:00\", \"tool_name\": \"web_search\", \"input\": { \"query\": \"test\", \"max_results\": 100, \"language\": \"unknown\" }, \"output\": \"Error: Invalid language parameter\", \"token_cost\": 50, \"success\": false, \"time_cost\": 0.5 } ] }' Response : { \"success\" : true , \"answer\" : \"Successfully evaluated and stored 2 tool call results\" , \"metadata\" : { \"memory_list\" : [ { \"when_to_use\" : \"web_search\" , \"memory_id\" : \"abc123...\" , \"tool_call_results\" : [ { \"tool_name\" : \"web_search\" , \"summary\" : \"Successfully retrieved relevant Python asyncio documentation\" , \"evaluation\" : \"Good parameter choices with appropriate max_results and language settings\" , \"score\" : 1.0 , ... }, { \"tool_name\" : \"web_search\" , \"summary\" : \"Failed due to invalid language parameter\" , \"evaluation\" : \"Query too generic and language parameter not supported\" , \"score\" : 0.0 , ... } ] } ] } }","title":"Usage with curl"},{"location":"tool_memory/tool_memory/#usage-with-python","text":"import requests from datetime import datetime def add_tool_call_results ( tool_call_results : list ) -> dict : \"\"\"Add tool call results to Tool Memory\"\"\" response = requests . post ( url = f \" { BASE_URL } add_tool_call_result\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_call_results\" : tool_call_results } ) return response . json () # Example: Record a tool invocation result = add_tool_call_results ([{ \"create_time\" : datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ), \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio\" , \"max_results\" : 10 }, \"output\" : \"Found 10 relevant results...\" , \"token_cost\" : 150 , \"success\" : True , \"time_cost\" : 2.3 }]) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code.","title":"Usage with Python"},{"location":"tool_memory/tool_memory/#42-retrieve_tool_memory","text":"Purpose : Retrieve usage guidelines and historical data for specific tools. Flow : retrieve_tool_memory : flow_content : retrieve_tool_memory_op description : \"Retrieves tool memories from the vector database based on tool names\" Process : 1. Takes comma-separated tool names as input 2. Searches Vector Store for exact matches (by when_to_use field) 3. Returns complete ToolMemory objects with: - Usage guidelines ( content ) - Historical call records ( tool_call_results ) - Statistics and metadata","title":"4.2 retrieve_tool_memory"},{"location":"tool_memory/tool_memory/#usage-with-curl_1","text":"# Retrieve single tool curl -X POST http://0.0.0.0:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search\" }' # Retrieve multiple tools (comma-separated) curl -X POST http://0.0.0.0:8002/retrieve_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search,database_query,file_processor\" }' Response : { \"success\" : true , \"answer\" : \"Successfully retrieved 1 tool memories\" , \"metadata\" : { \"memory_list\" : [ { \"memory_type\" : \"tool\" , \"workspace_id\" : \"my_workspace\" , \"memory_id\" : \"abc123...\" , \"when_to_use\" : \"web_search\" , \"content\" : \"## Usage Guidelines\\n\\n**Best Practices:**\\n- Use max_results between 5-20 for optimal performance\\n- Always specify language parameter (en has 95% success rate)\\n- Avoid generic single-word queries\\n\\n**Common Pitfalls:**\\n- max_results > 50 often causes timeouts\\n- Unknown language values default to 'en' with warning\\n\\n## Statistics\\n- **Success Rate**: 83.33%\\n- **Average Score**: 0.833\\n- **Average Time Cost**: 2.345s\\n- **Average Token Cost**: 156.7\" , \"score\" : 0.85 , \"time_created\" : \"2025-10-20 10:00:00\" , \"time_modified\" : \"2025-10-21 10:35:00\" , \"author\" : \"gpt-4\" , \"tool_call_results\" : [ { \"create_time\" : \"2025-10-21 10:30:00\" , \"tool_name\" : \"web_search\" , \"input\" : { \"query\" : \"Python asyncio\" , \"max_results\" : 10 }, \"output\" : \"Found 10 results...\" , \"summary\" : \"Successfully retrieved relevant documentation\" , \"evaluation\" : \"Good parameter choices...\" , \"score\" : 1.0 , \"token_cost\" : 150 , \"success\" : true , \"time_cost\" : 2.3 } // ... more historical calls ] } ] } }","title":"Usage with curl"},{"location":"tool_memory/tool_memory/#usage-with-python_1","text":"import requests def retrieve_tool_memory ( tool_names : str ) -> dict : \"\"\"Retrieve tool memories by tool names\"\"\" response = requests . post ( url = f \" { BASE_URL } retrieve_tool_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_names\" : tool_names } ) return response . json () # Example: Retrieve and use guidelines result = retrieve_tool_memory ( \"web_search\" ) if result [ 'success' ]: memory = result [ 'metadata' ][ 'memory_list' ][ 0 ] print ( f \"Tool: { memory [ 'when_to_use' ] } \" ) print ( f \"Guidelines: \\n { memory [ 'content' ] } \" ) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code.","title":"Usage with Python"},{"location":"tool_memory/tool_memory/#43-summary_tool_memory","text":"Purpose : Analyze historical tool calls and generate comprehensive usage guidelines. Flow : summary_tool_memory : flow_content : summary_tool_memory_op >> update_vector_store_op description : \"Analyzes tool call history and generates comprehensive usage patterns\" Process : 1. Retrieves existing ToolMemory by tool name 2. Analyzes recent N tool calls (default: 30) 3. Calculates statistics: - Success rate - Average score - Average time cost - Average token cost 4. Uses LLM to synthesize actionable guidelines from call summaries 5. Appends statistics to guidelines 6. Updates ToolMemory content in Vector Store Configuration ( default.yaml ): op : summary_tool_memory_op : backend : summary_tool_memory_op llm : default params : recent_call_count : 30 # Number of recent calls to analyze summary_sleep_interval : 1.0 # Delay between summaries (seconds)","title":"4.3 summary_tool_memory"},{"location":"tool_memory/tool_memory/#usage-with-curl_2","text":"# Summarize single tool curl -X POST http://0.0.0.0:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search\" }' # Summarize multiple tools (comma-separated) curl -X POST http://0.0.0.0:8002/summary_tool_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"tool_names\": \"web_search,database_query,file_processor\" }' Response : { \"success\" : true , \"answer\" : \"Successfully summarized 1 tool memories\" , \"metadata\" : { \"memory_list\" : [ { \"memory_type\" : \"tool\" , \"when_to_use\" : \"web_search\" , \"content\" : \"## Usage Guidelines\\n\\n**Optimal Parameters:**\\n- Set max_results between 5-20 for best balance of coverage and speed\\n- Always specify language='en' for technical queries (95% success rate)\\n- Use filter_type='technical_docs' for development-related searches\\n\\n**Success Patterns:**\\n- Specific, multi-word queries perform significantly better than generic terms\\n- Queries with clear intent (e.g., 'Python asyncio tutorial') return high-quality results\\n- Technical terms and version numbers improve result relevance\\n\\n**Common Failures:**\\n- Generic single-word queries (e.g., 'test') return poor results\\n- max_results > 50 increases timeout risk (5 failures observed)\\n- Invalid language codes cause fallback to default with warnings\\n\\n**Performance Insights:**\\n- Typical response time: 1.5-3.5s for successful queries\\n- Timeout threshold: 10s (consider simplifying complex queries)\\n- Token cost scales with result count: ~150 tokens for 10 results\\n\\n**Recommendations:**\\n1. Always validate language parameter before calling\\n2. Start with max_results=10, adjust based on needs\\n3. For time-sensitive operations, set timeout < 5s\\n4. Monitor token costs for high-frequency usage\\n\\n## Statistics\\n- **Success Rate**: 83.33%\\n- **Average Score**: 0.833\\n- **Average Time Cost**: 2.345s\\n- **Average Token Cost**: 156.7\" , \"memory_id\" : \"abc123...\" , \"time_modified\" : \"2025-10-21 10:40:00\" , ... } ] } }","title":"Usage with curl"},{"location":"tool_memory/tool_memory/#usage-with-python_2","text":"import requests def summarize_tool_memory ( tool_names : str ) -> dict : \"\"\"Generate comprehensive usage guidelines for tools\"\"\" response = requests . post ( url = f \" { BASE_URL } summary_tool_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"tool_names\" : tool_names } ) return response . json () # Example: Generate guidelines result = summarize_tool_memory ( \"web_search\" ) if result [ 'success' ]: memory = result [ 'metadata' ][ 'memory_list' ][ 0 ] print ( f \"Tool: { memory [ 'when_to_use' ] } \" ) print ( f \"Guidelines: \\n { memory [ 'content' ] } \" ) Complete examples : See cookbook/simple_demo/use_tool_memory_demo.py for full working code.","title":"Usage with Python"},{"location":"tool_memory/tool_memory/#5-best-practices","text":"","title":"5. Best Practices"},{"location":"tool_memory/tool_memory/#when-to-record-tool-calls","text":"Always : Record every tool invocation, including failures Include : Complete input parameters, output, and performance metrics Timing : Record immediately after tool execution completes","title":"When to Record Tool Calls"},{"location":"tool_memory/tool_memory/#when-to-generate-summaries","text":"Initial : After accumulating 20-30 tool calls for meaningful patterns Periodic : Re-summarize every 50-100 new calls or weekly Trigger-based : When success rate drops or patterns change significantly","title":"When to Generate Summaries"},{"location":"tool_memory/tool_memory/#when-to-retrieve-guidelines","text":"Before first use : Always retrieve before using an unfamiliar tool Before critical operations : Check latest guidelines for important tasks After updates : Re-retrieve when tool memory has been updated","title":"When to Retrieve Guidelines"},{"location":"tool_memory/tool_memory/#performance-tuning","text":"For High-Volume Tools (>100 calls/day): op : parse_tool_call_result_op : params : max_history_tool_call_cnt : 200 # Keep more history evaluation_sleep_interval : 0.5 # Faster evaluation summary_tool_memory_op : params : recent_call_count : 50 # Analyze more calls For Low-Volume Tools (<20 calls/day): op : parse_tool_call_result_op : params : max_history_tool_call_cnt : 50 # Less history needed evaluation_sleep_interval : 1.0 # Standard rate summary_tool_memory_op : params : recent_call_count : 20 # Analyze fewer calls","title":"Performance Tuning"},{"location":"tool_memory/tool_memory/#quality-maintenance","text":"Monitor Metrics : memory = retrieve_tool_memory ( \"web_search\" )[ 'metadata' ][ 'memory_list' ][ 0 ] stats = ToolMemory ( ** memory ) . statistic ( recent_frequency = 30 ) print ( f \"Success Rate: { stats [ 'success_rate' ] : .2% } \" ) print ( f \"Avg Score: { stats [ 'avg_score' ] : .2f } \" ) if stats [ 'success_rate' ] < 0.7 : print ( \"\u26a0\ufe0f Low success rate - investigate tool issues\" ) ``` 2. ** Clean Old Memories ** : - Delete tool memories for deprecated tools - Reset memories when tool behavior changes significantly 3. ** Validate Guidelines ** : - Periodically review generated guidelines for accuracy - Test recommended parameters in production scenarios ## 6. Memory Management ### Delete Workspace ``` bash curl - X POST http : // 0.0.0.0 : 8002 / vector_store \\ - H \"Content-Type: application/json\" \\ - d '{ \"workspace_id\" : \"my_workspace\" , \"action\" : \"delete\" } ' def delete_workspace ( workspace_id : str ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"delete\" } ) return response . json ()","title":"Quality Maintenance"},{"location":"tool_memory/tool_memory/#dump-memories-to-disk","text":"curl -X POST http://0.0.0.0:8002/vector_store \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"action\": \"dump\", \"path\": \"./memory_backup/\" }' def dump_memory ( workspace_id : str , path : str = \"./\" ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"dump\" , \"path\" : path } ) return response . json ()","title":"Dump Memories to Disk"},{"location":"tool_memory/tool_memory/#load-memories-from-disk","text":"curl -X POST http://0.0.0.0:8002/vector_store \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"my_workspace\", \"action\": \"load\", \"path\": \"./memory_backup/\" }' def load_memory ( workspace_id : str , path : str = \"./\" ): response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : workspace_id , \"action\" : \"load\" , \"path\" : path } ) return response . json ()","title":"Load Memories from Disk"},{"location":"tool_memory/tool_memory/#7-complete-working-example","text":"For a complete, runnable example demonstrating the full Tool Memory lifecycle, see: cookbook/simple_demo/use_tool_memory_demo.py This demo includes: - Workspace management : Clean, delete, dump, and load operations - Tool call recording : Adding 30+ mock tool invocations with various scenarios - Summarization : Generating usage guidelines from historical data - Retrieval : Fetching and displaying tool memories - Statistics : Analyzing success rates, costs, and performance Run the demo: cd cookbook/simple_demo python use_tool_memory_demo.py Key Workflow Steps: 1. Clean workspace : Remove existing data 2. Add tool calls : Record 30+ invocations (success/failure scenarios) 3. Generate guidelines : LLM analyzes patterns and creates recommendations 4. Retrieve memory : Get usage guidelines for agent consumption 5. Persistence : Test dump/load operations","title":"7. Complete Working Example"},{"location":"tool_memory/tool_memory/#8-advanced-use-cases","text":"","title":"8. Advanced Use Cases"},{"location":"tool_memory/tool_memory/#use-case-1-adaptive-parameter-tuning","text":"Retrieve tool memory statistics and adapt parameters based on historical performance: - If avg_time_cost > 5s : Increase timeout - If success_rate < 80% : Enable retry logic - If avg_token_cost high: Reduce result limits","title":"Use Case 1: Adaptive Parameter Tuning"},{"location":"tool_memory/tool_memory/#use-case-2-multi-tool-workflow-optimization","text":"Retrieve memories for multiple tools at once and optimize workflow order based on: - Success rates: Execute reliable tools first - Time costs: Parallelize slow operations - Token costs: Budget-aware tool selection","title":"Use Case 2: Multi-Tool Workflow Optimization"},{"location":"tool_memory/tool_memory/#use-case-3-automated-quality-monitoring","text":"Periodically check tool memory statistics and alert on: - Success rate degradation - Increasing time/token costs - Unusual failure patterns Implementation examples : See cookbook/simple_demo/use_tool_memory_demo.py and the ToolBench evaluation scripts.","title":"Use Case 3: Automated Quality Monitoring"},{"location":"tool_memory/tool_memory/#9-benchmark-results","text":"","title":"9. Benchmark Results"},{"location":"tool_memory/tool_memory/#tool-memory-performance-evaluation","text":"We evaluated Tool Memory effectiveness using a controlled benchmark with three mock search tools, each optimized for different query complexity levels (simple, moderate, complex). The benchmark compares agent performance with and without tool memory guidance across multiple epochs. Experimental Settings: - Model : Qwen3-30B-Instruct with default parameters - Task : Single-turn tool selection and invocation - Dataset : 60 training queries + 60 test queries per epoch - Tools : 3 mock search tools with varying performance profiles - Metrics : Average quality score (0.0-1.0) based on LLM evaluation - Baseline : Test set performance without tool memory - Replication : Results averaged across 3 independent experimental runs Results (averaged across 3 epochs): Scenario Avg Score Improvement Train (No Memory) 0.650 - Test (No Memory) 0.672 Baseline Test (With Memory) 0.772 +14.88% Key Findings: - Consistent improvement : Tool Memory boosted test performance by ~15% on average - Knowledge transfer : Training data successfully informed test-time tool selection - Stability : Improvement remained consistent across all 3 epochs (9.90% \u2192 17.39% \u2192 17.13%) The benchmark demonstrates that Tool Memory enables agents to make data-driven tool selection decisions, significantly improving task success rates compared to relying solely on static tool descriptions. Benchmark Resources: - Design Documentation : docs/tool_memory/tool_bench.md - Complete benchmark methodology and workflow - Implementation : cookbook/tool_memory/run_reme_tool_bench.py - Full benchmark script - Query Dataset : cookbook/tool_memory/query.json - 60 train + 60 test queries across 3 complexity levels","title":"Tool Memory Performance Evaluation"},{"location":"tool_memory/tool_memory/#10-references","text":"Implementation : See reme_ai/summary/tool/ and reme_ai/retrieve/tool/ Demo : cookbook/simple_demo/use_tool_memory_demo.py Benchmark : cookbook/tool_memory/run_reme_tool_bench.py Schema : reme_ai/schema/memory.py Utilities : reme_ai/utils/tool_memory_utils.py","title":"10. References"},{"location":"tool_memory/tool_retrieve_ops/","text":"RetrieveToolMemoryOp Purpose Retrieves tool memories from the vector database based on tool names, providing usage patterns, best practices, and historical call data. Functionality Accepts comma-separated tool names as input Searches the vector store for exact tool name matches Validates that retrieved memories are of type \"tool\" Returns complete tool memories including usage guidelines and call history Parameters This operation has no configurable parameters. It uses the default vector store configuration.","title":"Retrieve Ops"},{"location":"tool_memory/tool_retrieve_ops/#retrievetoolmemoryop","text":"","title":"RetrieveToolMemoryOp"},{"location":"tool_memory/tool_retrieve_ops/#purpose","text":"Retrieves tool memories from the vector database based on tool names, providing usage patterns, best practices, and historical call data.","title":"Purpose"},{"location":"tool_memory/tool_retrieve_ops/#functionality","text":"Accepts comma-separated tool names as input Searches the vector store for exact tool name matches Validates that retrieved memories are of type \"tool\" Returns complete tool memories including usage guidelines and call history","title":"Functionality"},{"location":"tool_memory/tool_retrieve_ops/#parameters","text":"This operation has no configurable parameters. It uses the default vector store configuration.","title":"Parameters"},{"location":"tool_memory/tool_summary_ops/","text":"ParseToolCallResultOp Purpose Evaluates individual tool invocations and adds them to the tool memory database with comprehensive assessments. Functionality Receives tool call results with input parameters, output, and metadata Uses LLM to evaluate each tool call based on success and parameter alignment Generates summary, evaluation, and score (0.0 or 1.0) for each call Appends evaluated results to existing tool memory or creates new memory Maintains a sliding window of recent tool calls (configurable limit) Parameters op.parse_tool_call_result_op.params.max_history_tool_call_cnt (integer, default: 100 ): Maximum number of historical tool call results to retain per tool When exceeded, oldest results are removed (FIFO) op.parse_tool_call_result_op.params.evaluation_sleep_interval (float, default: 1.0 ): Delay in seconds between concurrent evaluations Prevents rate limiting when evaluating multiple calls SummaryToolMemoryOp Purpose Analyzes accumulated tool call history and generates comprehensive usage patterns, best practices, and recommendations. Functionality Retrieves existing tool memories from the vector store Intelligently skips tools where all recent calls have already been summarized (using is_summarized flag) Analyzes the most recent N tool calls (configurable) Calculates statistical metrics (success rate, average scores, costs) Uses LLM to synthesize actionable usage guidelines Updates tool memory content with generated insights Marks processed calls as summarized to avoid redundant processing in future runs Smart Skip Logic To optimize costs and performance, SummaryToolMemoryOp tracks which tool call results have been included in a summary: Skip Condition : If all recent N calls are already summarized ( is_summarized=True ), the tool is skipped entirely Trigger Condition : If at least 1 recent call is new ( is_summarized=False ), re-summarization is triggered Automatic Marking : After successful summarization, all processed calls are marked with is_summarized=True Example Behavior : Run 1: 30 new calls \u2192 Summarize all 30, mark as summarized Run 2: Same 30 calls \u2192 Skip (all already summarized) \u2713 Cost savings Run 3: 30 old + 1 new \u2192 Re-summarize all 31, mark new call as summarized This ensures summaries stay fresh while avoiding unnecessary LLM calls. Parameters op.summary_tool_memory_op.params.recent_call_count (integer, default: 30 ): Number of most recent tool calls to analyze Also determines the window for checking summarization status Focuses on recent usage patterns op.summary_tool_memory_op.params.summary_sleep_interval (float, default: 1.0 ): Delay in seconds between concurrent summarizations Prevents rate limiting when summarizing multiple tools Return Value The operation returns a response message indicating: - Number of tools summarized (had new unsummarized calls) - Number of tools skipped (all recent calls already summarized) Example: \"Successfully processed 5 tool memories: 2 summarized, 3 skipped (already up-to-date)\" See Also For detailed implementation and examples, see is_summarized Feature Documentation .","title":"Summary Ops"},{"location":"tool_memory/tool_summary_ops/#parsetoolcallresultop","text":"","title":"ParseToolCallResultOp"},{"location":"tool_memory/tool_summary_ops/#purpose","text":"Evaluates individual tool invocations and adds them to the tool memory database with comprehensive assessments.","title":"Purpose"},{"location":"tool_memory/tool_summary_ops/#functionality","text":"Receives tool call results with input parameters, output, and metadata Uses LLM to evaluate each tool call based on success and parameter alignment Generates summary, evaluation, and score (0.0 or 1.0) for each call Appends evaluated results to existing tool memory or creates new memory Maintains a sliding window of recent tool calls (configurable limit)","title":"Functionality"},{"location":"tool_memory/tool_summary_ops/#parameters","text":"op.parse_tool_call_result_op.params.max_history_tool_call_cnt (integer, default: 100 ): Maximum number of historical tool call results to retain per tool When exceeded, oldest results are removed (FIFO) op.parse_tool_call_result_op.params.evaluation_sleep_interval (float, default: 1.0 ): Delay in seconds between concurrent evaluations Prevents rate limiting when evaluating multiple calls","title":"Parameters"},{"location":"tool_memory/tool_summary_ops/#summarytoolmemoryop","text":"","title":"SummaryToolMemoryOp"},{"location":"tool_memory/tool_summary_ops/#purpose_1","text":"Analyzes accumulated tool call history and generates comprehensive usage patterns, best practices, and recommendations.","title":"Purpose"},{"location":"tool_memory/tool_summary_ops/#functionality_1","text":"Retrieves existing tool memories from the vector store Intelligently skips tools where all recent calls have already been summarized (using is_summarized flag) Analyzes the most recent N tool calls (configurable) Calculates statistical metrics (success rate, average scores, costs) Uses LLM to synthesize actionable usage guidelines Updates tool memory content with generated insights Marks processed calls as summarized to avoid redundant processing in future runs","title":"Functionality"},{"location":"tool_memory/tool_summary_ops/#smart-skip-logic","text":"To optimize costs and performance, SummaryToolMemoryOp tracks which tool call results have been included in a summary: Skip Condition : If all recent N calls are already summarized ( is_summarized=True ), the tool is skipped entirely Trigger Condition : If at least 1 recent call is new ( is_summarized=False ), re-summarization is triggered Automatic Marking : After successful summarization, all processed calls are marked with is_summarized=True Example Behavior : Run 1: 30 new calls \u2192 Summarize all 30, mark as summarized Run 2: Same 30 calls \u2192 Skip (all already summarized) \u2713 Cost savings Run 3: 30 old + 1 new \u2192 Re-summarize all 31, mark new call as summarized This ensures summaries stay fresh while avoiding unnecessary LLM calls.","title":"Smart Skip Logic"},{"location":"tool_memory/tool_summary_ops/#parameters_1","text":"op.summary_tool_memory_op.params.recent_call_count (integer, default: 30 ): Number of most recent tool calls to analyze Also determines the window for checking summarization status Focuses on recent usage patterns op.summary_tool_memory_op.params.summary_sleep_interval (float, default: 1.0 ): Delay in seconds between concurrent summarizations Prevents rate limiting when summarizing multiple tools","title":"Parameters"},{"location":"tool_memory/tool_summary_ops/#return-value","text":"The operation returns a response message indicating: - Number of tools summarized (had new unsummarized calls) - Number of tools skipped (all recent calls already summarized) Example: \"Successfully processed 5 tool memories: 2 summarized, 3 skipped (already up-to-date)\"","title":"Return Value"},{"location":"tool_memory/tool_summary_ops/#see-also","text":"For detailed implementation and examples, see is_summarized Feature Documentation .","title":"See Also"}]}