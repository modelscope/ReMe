{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ReMe (formerly MemoryScope): Memory Management Framework for Agents Remember Me, Refine Me. ReMe provides AI agents with a unified memory system\u2014enabling the ability to extract, reuse, and share memories across users, tasks, and agents. Personal Memory + Task Memory = Agent Memory Personal memory helps \" understand user preferences \", while task memory helps agents \" perform better \". \ud83d\udcf0 Latest Updates [2025-09] \ud83c\udf89 ReMe v0.1.8 has been officially released, adding support for asynchronous operations. It has also been integrated into the memory service of agentscope-runtime. [2025-09] \ud83c\udf89 ReMe v0.1 officially released, integrating task memory and personal memory. If you want to use the original memoryscope project, you can find it in MemoryScope . [2025-09] \ud83e\uddea We validated the effectiveness of task memory extraction and reuse in agents in appworld, bfcl(v3), and frozenlake environments. For more information, check appworld exp , bfcl exp , and frozenlake exp . [2025-08] \ud83d\ude80 MCP protocol support is now available -> MCP Quick Start . [2025-06] \ud83d\ude80 Multiple backend vector storage support (Elasticsearch & ChromaDB) -> Vector DB quick start . [2024-09] \ud83e\udde0 MemoryScope v0.1 released, personalized and time-aware memory storage and usage. \u2728 Architecture Design ReMe integrates two complementary memory capabilities: \ud83e\udde0 Task Memory/Experience Procedural knowledge reused across agents Success Pattern Recognition : Identify effective strategies and understand their underlying principles Failure Analysis Learning : Learn from mistakes and avoid repeating the same issues Comparative Patterns : Different sampling trajectories provide more valuable memories through comparison Validation Patterns : Confirm the effectiveness of extracted memories through validation modules Learn more about how to use task memory from task memory \ud83d\udc64 Personal Memory Contextualized memory for specific users Individual Preferences : User habits, preferences, and interaction styles Contextual Adaptation : Intelligent memory management based on time and context Progressive Learning : Gradually build deep understanding through long-term interaction Time Awareness : Time sensitivity in both retrieval and integration Learn more about how to use personal memory from personal memory \ud83d\udee0\ufe0f Installation Install from PyPI (Recommended) pip install reme-ai Install from Source git clone https://github.com/modelscope/ReMe.git cd ReMe pip install . Environment Configuration Copy example.env to .env and modify the corresponding parameters: # Required: LLM API Configuration FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 # Required: Embedding Model Configuration FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 \ud83d\ude80 Quick Start HTTP Service Startup reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local MCP Server Support reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Core API Usage Task Memory Management import requests # Experience Summarizer: Learn from execution trajectories response = requests . post ( \"http://localhost:8002/summary_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Help me create a project plan\" }], \"score\" : 1.0 } ] }) # Retriever: Get relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"How to efficiently manage project progress?\" , \"top_k\" : 1 }) curl version # Experience Summarizer: Learn from execution trajectories curl -X POST http://localhost:8002/summary_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [{\"role\": \"user\", \"content\": \"Help me create a project plan\"}], \"score\": 1.0} ] }' # Retriever: Get relevant memories curl -X POST http://localhost:8002/retrieve_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"How to efficiently manage project progress?\", \"top_k\": 1 }' Node.js version // Experience Summarizer: Learn from execution trajectories fetch ( \"http://localhost:8002/summary_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [{ role : \"user\" , content : \"Help me create a project plan\" }], score : 1.0 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retriever: Get relevant memories fetch ( \"http://localhost:8002/retrieve_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"How to efficiently manage project progress?\" , top_k : 1 }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); Personal Memory Management # Memory Integration: Learn from user interactions response = requests . post ( \"http://localhost:8002/summary_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"I like to drink coffee while working in the morning\" }, { \"role\" : \"assistant\" , \"content\" : \"I understand, you prefer to start your workday with coffee to stay energized\" } ] } ] }) # Memory Retrieval: Get personal memory fragments response = requests . post ( \"http://localhost:8002/retrieve_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"What are the user's work habits?\" , \"top_k\" : 5 }) curl version # Memory Integration: Learn from user interactions curl -X POST http://localhost:8002/summary_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [ {\"role\": \"user\", \"content\": \"I like to drink coffee while working in the morning\"}, {\"role\": \"assistant\", \"content\": \"I understand, you prefer to start your workday with coffee to stay energized\"} ]} ] }' # Memory Retrieval: Get personal memory fragments curl -X POST http://localhost:8002/retrieve_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"What are the user' s work habits? \", \" top_k \": 5 }' Node.js version // Memory Integration: Learn from user interactions fetch ( \"http://localhost:8002/summary_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [ { role : \"user\" , content : \"I like to drink coffee while working in the morning\" }, { role : \"assistant\" , content : \"I understand, you prefer to start your workday with coffee to stay energized\" } ]} ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Memory Retrieval: Get personal memory fragments fetch ( \"http://localhost:8002/retrieve_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"What are the user's work habits?\" , top_k : 5 }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); \ud83d\udce6 Ready-to-Use Libraries ReMe provides pre-built memory libraries that agents can immediately use with verified best practices: Available Libraries appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls Quick Usage # Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 }) \ud83e\uddea Experiments \ud83c\udf0d Appworld Experiment We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md . \ud83e\uddca Frozenlake Experiment without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md . \ud83d\udd27 BFCL-V3 Experiment We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%) \ud83d\udcda Resources Quick Start : Get started quickly with practical examples Vector Storage Setup : Configure local/vector databases and usage MCP Guide : Create MCP services personal memory & task memory : Operators used in personal memory and task memory, You can modify the config to customize the pipelines. Example Collection : Real use cases and best practices \ud83e\udd1d Contribution We believe the best memory systems come from collective wisdom. Contributions welcome \ud83d\udc49 Guide : Code Contributions New operation and tool development Backend implementation and optimization API enhancements and new endpoints Documentation Improvements Usage examples and tutorials Best practice guides \ud83d\udcc4 Citation @software { ReMe2025 , title = {ReMe: Memory Management Framework for Agents} , author = {Li Yu, Jiaji Deng, Zouying Cao} , url = {https://github.com/modelscope/ReMe} , year = {2025} } \u2696\ufe0f License This project is licensed under the Apache License 2.0 - see the LICENSE file for details. Star History","title":"Welcome"},{"location":"#latest-updates","text":"[2025-09] \ud83c\udf89 ReMe v0.1.8 has been officially released, adding support for asynchronous operations. It has also been integrated into the memory service of agentscope-runtime. [2025-09] \ud83c\udf89 ReMe v0.1 officially released, integrating task memory and personal memory. If you want to use the original memoryscope project, you can find it in MemoryScope . [2025-09] \ud83e\uddea We validated the effectiveness of task memory extraction and reuse in agents in appworld, bfcl(v3), and frozenlake environments. For more information, check appworld exp , bfcl exp , and frozenlake exp . [2025-08] \ud83d\ude80 MCP protocol support is now available -> MCP Quick Start . [2025-06] \ud83d\ude80 Multiple backend vector storage support (Elasticsearch & ChromaDB) -> Vector DB quick start . [2024-09] \ud83e\udde0 MemoryScope v0.1 released, personalized and time-aware memory storage and usage.","title":"\ud83d\udcf0 Latest Updates"},{"location":"#architecture-design","text":"ReMe integrates two complementary memory capabilities:","title":"\u2728 Architecture Design"},{"location":"#task-memoryexperience","text":"Procedural knowledge reused across agents Success Pattern Recognition : Identify effective strategies and understand their underlying principles Failure Analysis Learning : Learn from mistakes and avoid repeating the same issues Comparative Patterns : Different sampling trajectories provide more valuable memories through comparison Validation Patterns : Confirm the effectiveness of extracted memories through validation modules Learn more about how to use task memory from task memory","title":"\ud83e\udde0 Task Memory/Experience"},{"location":"#personal-memory","text":"Contextualized memory for specific users Individual Preferences : User habits, preferences, and interaction styles Contextual Adaptation : Intelligent memory management based on time and context Progressive Learning : Gradually build deep understanding through long-term interaction Time Awareness : Time sensitivity in both retrieval and integration Learn more about how to use personal memory from personal memory","title":"\ud83d\udc64 Personal Memory"},{"location":"#installation","text":"","title":"\ud83d\udee0\ufe0f Installation"},{"location":"#install-from-pypi-recommended","text":"pip install reme-ai","title":"Install from PyPI (Recommended)"},{"location":"#install-from-source","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe pip install .","title":"Install from Source"},{"location":"#environment-configuration","text":"Copy example.env to .env and modify the corresponding parameters: # Required: LLM API Configuration FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 # Required: Embedding Model Configuration FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1","title":"Environment Configuration"},{"location":"#quick-start","text":"","title":"\ud83d\ude80 Quick Start"},{"location":"#http-service-startup","text":"reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"HTTP Service Startup"},{"location":"#mcp-server-support","text":"reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"MCP Server Support"},{"location":"#core-api-usage","text":"","title":"Core API Usage"},{"location":"#task-memory-management","text":"import requests # Experience Summarizer: Learn from execution trajectories response = requests . post ( \"http://localhost:8002/summary_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [{ \"role\" : \"user\" , \"content\" : \"Help me create a project plan\" }], \"score\" : 1.0 } ] }) # Retriever: Get relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"How to efficiently manage project progress?\" , \"top_k\" : 1 }) curl version # Experience Summarizer: Learn from execution trajectories curl -X POST http://localhost:8002/summary_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [{\"role\": \"user\", \"content\": \"Help me create a project plan\"}], \"score\": 1.0} ] }' # Retriever: Get relevant memories curl -X POST http://localhost:8002/retrieve_task_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"How to efficiently manage project progress?\", \"top_k\": 1 }' Node.js version // Experience Summarizer: Learn from execution trajectories fetch ( \"http://localhost:8002/summary_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [{ role : \"user\" , content : \"Help me create a project plan\" }], score : 1.0 } ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Retriever: Get relevant memories fetch ( \"http://localhost:8002/retrieve_task_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"How to efficiently manage project progress?\" , top_k : 1 }) }) . then ( response => response . json ()) . then ( data => console . log ( data ));","title":"Task Memory Management"},{"location":"#personal-memory-management","text":"# Memory Integration: Learn from user interactions response = requests . post ( \"http://localhost:8002/summary_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"trajectories\" : [ { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"I like to drink coffee while working in the morning\" }, { \"role\" : \"assistant\" , \"content\" : \"I understand, you prefer to start your workday with coffee to stay energized\" } ] } ] }) # Memory Retrieval: Get personal memory fragments response = requests . post ( \"http://localhost:8002/retrieve_personal_memory\" , json = { \"workspace_id\" : \"task_workspace\" , \"query\" : \"What are the user's work habits?\" , \"top_k\" : 5 }) curl version # Memory Integration: Learn from user interactions curl -X POST http://localhost:8002/summary_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"trajectories\": [ {\"messages\": [ {\"role\": \"user\", \"content\": \"I like to drink coffee while working in the morning\"}, {\"role\": \"assistant\", \"content\": \"I understand, you prefer to start your workday with coffee to stay energized\"} ]} ] }' # Memory Retrieval: Get personal memory fragments curl -X POST http://localhost:8002/retrieve_personal_memory \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"task_workspace\", \"query\": \"What are the user' s work habits? \", \" top_k \": 5 }' Node.js version // Memory Integration: Learn from user interactions fetch ( \"http://localhost:8002/summary_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , trajectories : [ { messages : [ { role : \"user\" , content : \"I like to drink coffee while working in the morning\" }, { role : \"assistant\" , content : \"I understand, you prefer to start your workday with coffee to stay energized\" } ]} ] }) }) . then ( response => response . json ()) . then ( data => console . log ( data )); // Memory Retrieval: Get personal memory fragments fetch ( \"http://localhost:8002/retrieve_personal_memory\" , { method : \"POST\" , headers : { \"Content-Type\" : \"application/json\" , }, body : JSON . stringify ({ workspace_id : \"task_workspace\" , query : \"What are the user's work habits?\" , top_k : 5 }) }) . then ( response => response . json ()) . then ( data => console . log ( data ));","title":"Personal Memory Management"},{"location":"#ready-to-use-libraries","text":"ReMe provides pre-built memory libraries that agents can immediately use with verified best practices:","title":"\ud83d\udce6 Ready-to-Use Libraries"},{"location":"#available-libraries","text":"appworld.jsonl : Memory library for Appworld agent interactions, covering complex task planning and execution patterns bfcl_v3.jsonl : Working memory library for BFCL tool calls","title":"Available Libraries"},{"location":"#quick-usage","text":"# Load pre-built memories response = requests . post ( \"http://localhost:8002/vector_store\" , json = { \"workspace_id\" : \"appworld\" , \"action\" : \"load\" , \"path\" : \"./docs/library/\" }) # Query relevant memories response = requests . post ( \"http://localhost:8002/retrieve_task_memory\" , json = { \"workspace_id\" : \"appworld\" , \"query\" : \"How to navigate to settings and update user profile?\" , \"top_k\" : 1 })","title":"Quick Usage"},{"location":"#experiments","text":"","title":"\ud83e\uddea Experiments"},{"location":"#appworld-experiment","text":"We tested ReMe on Appworld using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.083 0.140 0.228 with ReMe 0.109 (+2.6%) 0.175 (+3.5%) 0.281 (+5.3%) Pass@K measures the probability that at least one of the K generated samples successfully completes the task ( score=1). The current experiment uses an internal AppWorld environment, which may have slight differences. You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83c\udf0d Appworld Experiment"},{"location":"#frozenlake-experiment","text":"without ReMe with ReMe We tested on 100 random frozenlake maps using qwen3-8b: Method pass rate without ReMe 0.66 with ReMe 0.72 (+6.0%) You can find more details on reproducing the experiment in quickstart.md .","title":"\ud83e\uddca Frozenlake Experiment"},{"location":"#bfcl-v3-experiment","text":"We tested ReMe on BFCL-V3 multi-turn-base (randomly split 50train/150val) using qwen3-8b: Method pass@1 pass@2 pass@4 without ReMe 0.2472 0.2733 0.2922 with ReMe 0.3061 (+5.89%) 0.3500 (+7.67%) 0.3888 (+9.66%)","title":"\ud83d\udd27 BFCL-V3 Experiment"},{"location":"#resources","text":"Quick Start : Get started quickly with practical examples Vector Storage Setup : Configure local/vector databases and usage MCP Guide : Create MCP services personal memory & task memory : Operators used in personal memory and task memory, You can modify the config to customize the pipelines. Example Collection : Real use cases and best practices","title":"\ud83d\udcda Resources"},{"location":"#contribution","text":"We believe the best memory systems come from collective wisdom. Contributions welcome \ud83d\udc49 Guide :","title":"\ud83e\udd1d Contribution"},{"location":"#code-contributions","text":"New operation and tool development Backend implementation and optimization API enhancements and new endpoints","title":"Code Contributions"},{"location":"#documentation-improvements","text":"Usage examples and tutorials Best practice guides","title":"Documentation Improvements"},{"location":"#citation","text":"@software { ReMe2025 , title = {ReMe: Memory Management Framework for Agents} , author = {Li Yu, Jiaji Deng, Zouying Cao} , url = {https://github.com/modelscope/ReMe} , year = {2025} }","title":"\ud83d\udcc4 Citation"},{"location":"#license","text":"This project is licensed under the Apache License 2.0 - see the LICENSE file for details.","title":"\u2696\ufe0f License"},{"location":"#star-history","text":"","title":"Star History"},{"location":"contribution/","text":"Our community thrives on the diverse ideas and contributions of its members. Whether you're fixing a bug, adding a new feature, improving the documentation, or adding examples, your help is welcome. Here's how you can contribute: Report Bugs and Ask For New Features? Did you find a bug or have a feature request? Please first check the issue tracker to see if it has already been reported. If not, feel free to open a new issue. Include as much detail as possible: - A descriptive title - Clear description of the issue - Steps to reproduce the problem - Version of the ReMe you are using - Any relevant code snippets or error messages Contribute to Codebase Fork and Clone the Repository To work on an issue or a new feature, start by forking the ReMe repository and then cloning your fork locally. git clone https://github.com/your-username/ReMe.git cd ReMe Create a New Branch Create a new branch for your work. This helps keep proposed changes organized and separate from the main branch. git checkout -b your-feature-branch-name Making Changes With your new branch checked out, you can now make your changes to the code. Remember to keep your changes as focused as possible. If you're addressing multiple issues or features, it's better to create separate branches and pull requests for each. Commit Your Changes Once you've made your changes, it's time to commit them. Write clear and concise commit messages that explain your changes. git add -A git commit -m \"A brief description of the changes\" Submit a Pull Request When you're ready for feedback, submit a pull request to the ReMe main branch. In your pull request description, explain the changes you've made and any other relevant context. We will review your pull request. This process might involve some discussion, additional changes on your part, or both. Code Review Wait for us to review your pull request. We may suggest some changes or improvements. Keep an eye on your GitHub notifications and be responsive to any feedback.","title":"Contributions"},{"location":"contribution/#report-bugs-and-ask-for-new-features","text":"Did you find a bug or have a feature request? Please first check the issue tracker to see if it has already been reported. If not, feel free to open a new issue. Include as much detail as possible: - A descriptive title - Clear description of the issue - Steps to reproduce the problem - Version of the ReMe you are using - Any relevant code snippets or error messages","title":"Report Bugs and Ask For New Features?"},{"location":"contribution/#contribute-to-codebase","text":"","title":"Contribute to Codebase"},{"location":"contribution/#fork-and-clone-the-repository","text":"To work on an issue or a new feature, start by forking the ReMe repository and then cloning your fork locally. git clone https://github.com/your-username/ReMe.git cd ReMe","title":"Fork and Clone the Repository"},{"location":"contribution/#create-a-new-branch","text":"Create a new branch for your work. This helps keep proposed changes organized and separate from the main branch. git checkout -b your-feature-branch-name","title":"Create a New Branch"},{"location":"contribution/#making-changes","text":"With your new branch checked out, you can now make your changes to the code. Remember to keep your changes as focused as possible. If you're addressing multiple issues or features, it's better to create separate branches and pull requests for each.","title":"Making Changes"},{"location":"contribution/#commit-your-changes","text":"Once you've made your changes, it's time to commit them. Write clear and concise commit messages that explain your changes. git add -A git commit -m \"A brief description of the changes\"","title":"Commit Your Changes"},{"location":"contribution/#submit-a-pull-request","text":"When you're ready for feedback, submit a pull request to the ReMe main branch. In your pull request description, explain the changes you've made and any other relevant context. We will review your pull request. This process might involve some discussion, additional changes on your part, or both.","title":"Submit a Pull Request"},{"location":"contribution/#code-review","text":"Wait for us to review your pull request. We may suggest some changes or improvements. Keep an eye on your GitHub notifications and be responsive to any feedback.","title":"Code Review"},{"location":"mcp_quick_start/","text":"This guide will help you get started with ReMe using the Model Context Protocol (MCP) interface for seamless integration with MCP-compatible clients. \ud83d\ude80 What You'll Learn How to set up and configure ReMe MCP server How to connect to the server using Python MCP clients How to use task memory operations through MCP How to build memory-enhanced agents with MCP integration \ud83d\udccb Prerequisites Python 3.12+ LLM API access (OpenAI or compatible) Embedding model API access MCP-compatible client (Claude Desktop, or custom MCP client) \ud83d\udee0\ufe0f Installation Option 1: Install from PyPI (Recommended) pip install reme-ai Option 2: Install from Source git clone https://github.com/modelscope/ReMe.git cd ReMe pip install . \u2699\ufe0f Environment Setup Create a .env file in your project directory: FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1 \ud83d\ude80 Building an MCP Server with ReMe ReMe provides a flexible framework for building MCP servers that can communicate using either STDIO or SSE (Server-Sent Events) transport protocols. Starting the MCP Server Option 1: STDIO Transport (Recommended for MCP clients) reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Option 2: SSE Transport (Server-Sent Events) reme \\ backend = mcp \\ mcp.transport = sse \\ http_service.port = 8001 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local The SSE server will start on http://localhost:8002/sse Configuring MCP Server for Claude Desktop To integrate with Claude Desktop, add the following configuration to your claude_desktop_config.json : { \"mcpServers\" : { \"reme\" : { \"command\" : \"reme\" , \"args\" : [ \"backend=mcp\" , \"mcp.transport=stdio\" , \"llm.default.model_name=qwen3-30b-a3b-thinking-2507\" , \"embedding_model.default.model_name=text-embedding-v4\" , \"vector_store.default.backend=local_file\" ] } } } This configuration: Registers a new MCP server named \"reme\" Specifies the command to launch the server ( reme ) Configures the server to use STDIO transport Sets the LLM and embedding models to use Configures the vector store backend Advanced Server Configuration Options For more advanced use cases, you can configure the server with additional parameters: # Full configuration example reme \\ backend = mcp \\ mcp.transport = stdio \\ http_service.host = 0 .0.0.0 \\ http_service.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = elasticsearch \\ \ud83d\udd0c Using Python Client to Call MCP Services The ReMe framework provides a Python client for interacting with MCP services. This section focuses specifically on using the summary_task_memory and retrieve_task_memory tools. Setting Up the Python MCP Client First, install the required packages: pip install fastmcp dotenv Then, create a basic client connection: import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # MCP server URL (for SSE transport) MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"my_workspace\" async def main (): async with Client ( MCP_URL ) as client : # Your MCP operations will go here pass if __name__ == \"__main__\" : asyncio . run ( main ()) Using the Task Memory Summarizer The summary_task_memory tool transforms conversation trajectories into valuable task memories: async def run_summary ( client , messages ): \"\"\" Generate a summary of conversation messages and create task memories Args: client: MCP client instance messages: List of message objects from a conversation Returns: None \"\"\" try : result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract memory list from response memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) print ( f \"Created memories: { memory_list } \" ) # Optionally save memories to file with open ( \"task_memory.jsonl\" , \"w\" ) as f : f . write ( json . dumps ( memory_list , indent = 2 , ensure_ascii = False )) except Exception as e : print ( f \"Error running summary: { e } \" ) Using the Task Memory Retriever The retrieve_task_memory tool allows you to retrieve relevant memories based on a query: async def run_retrieve ( client , query ): \"\"\" Retrieve relevant task memories based on a query Args: client: MCP client instance query: The query to retrieve relevant memories Returns: String containing the retrieved memory answer \"\"\" try : result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"query\" : query , } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract and return the answer answer = response_data . get ( \"answer\" , \"\" ) print ( f \"Retrieved memory: { answer } \" ) return answer except Exception as e : print ( f \"Error retrieving memory: { e } \" ) return \"\" Complete Memory-Augmented Agent Example Here's a complete example showing how to build a memory-augmented agent using the MCP client: import json import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # API configuration MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"test_workspace\" async def run_agent ( client , query ): \"\"\"Run the agent with a specific query\"\"\" result = await client . call_tool ( \"react\" , arguments = { \"query\" : query } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) messages = response_data . get ( \"messages\" , []) return messages async def run_summary ( client , messages ): \"\"\"Generate task memories from conversation\"\"\" result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) response_data = json . loads ( result . content ) memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) return memory_list async def run_retrieve ( client , query ): \"\"\"Retrieve relevant task memories\"\"\" result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query , } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) return answer async def memory_augmented_workflow (): \"\"\"Complete memory-augmented agent workflow\"\"\" query1 = \"Analyze Xiaomi Corporation\" query2 = \"Analyze the company Tesla.\" async with Client ( MCP_URL ) as client : # Step 1: Build initial memories with query2 print ( f \"Building memories with: ' { query2 } '\" ) messages = await run_agent ( client , query = query2 ) # Step 2: Summarize conversation to create memories print ( \"Creating memories from conversation\" ) memory_list = await run_summary ( client , messages ) print ( f \"Created { len ( memory_list ) } memories\" ) # Step 3: Retrieve relevant memories for query1 print ( f \"Retrieving memories for: ' { query1 } '\" ) retrieved_memory = await run_retrieve ( client , query1 ) # Step 4: Run agent with memory-augmented query print ( \"Running memory-augmented agent\" ) augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query1 } \" final_messages = await run_agent ( client , query = augmented_query ) # Extract the agent's final answer final_answer = \"\" for msg in final_messages : if msg . get ( \"role\" ) == \"assistant\" and msg . get ( \"content\" ): final_answer = msg . get ( \"content\" ) break print ( f \"Memory-augmented response: { final_answer } \" ) # Run the workflow if __name__ == \"__main__\" : asyncio . run ( memory_augmented_workflow ()) Managing Vector Store with MCP You can also manage your vector store through MCP: async def manage_vector_store ( client ): # Delete a workspace await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" , } ) # Dump memories to disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./backups/\" , } ) # Load memories from disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./backups/\" , } ) \ud83d\udc1b Common Issues and Troubleshooting MCP Server Won't Start Check if the required ports are available (for SSE transport) Verify your API keys in .env file Ensure Python version is 3.12+ Check MCP transport configuration MCP Client Connection Issues For STDIO: Ensure the command path is correct in your MCP client config For SSE: Verify the server URL and port accessibility Check firewall settings for SSE connections No Memories Retrieved Make sure you've run the summarizer tool first to create memories Check if workspace_id matches between operations Verify vector store backend is properly configured API Connection Errors Confirm LLM_BASE_URL and API keys are correct Test API access independently Check network connectivity","title":"MCP"},{"location":"mcp_quick_start/#what-youll-learn","text":"How to set up and configure ReMe MCP server How to connect to the server using Python MCP clients How to use task memory operations through MCP How to build memory-enhanced agents with MCP integration","title":"\ud83d\ude80 What You'll Learn"},{"location":"mcp_quick_start/#prerequisites","text":"Python 3.12+ LLM API access (OpenAI or compatible) Embedding model API access MCP-compatible client (Claude Desktop, or custom MCP client)","title":"\ud83d\udccb Prerequisites"},{"location":"mcp_quick_start/#installation","text":"","title":"\ud83d\udee0\ufe0f Installation"},{"location":"mcp_quick_start/#option-1-install-from-pypi-recommended","text":"pip install reme-ai","title":"Option 1: Install from PyPI (Recommended)"},{"location":"mcp_quick_start/#option-2-install-from-source","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe pip install .","title":"Option 2: Install from Source"},{"location":"mcp_quick_start/#environment-setup","text":"Create a .env file in your project directory: FLOW_EMBEDDING_API_KEY = sk-xxxx FLOW_EMBEDDING_BASE_URL = https://xxxx/v1 FLOW_LLM_API_KEY = sk-xxxx FLOW_LLM_BASE_URL = https://xxxx/v1","title":"\u2699\ufe0f Environment Setup"},{"location":"mcp_quick_start/#building-an-mcp-server-with-reme","text":"ReMe provides a flexible framework for building MCP servers that can communicate using either STDIO or SSE (Server-Sent Events) transport protocols.","title":"\ud83d\ude80 Building an MCP Server with ReMe"},{"location":"mcp_quick_start/#starting-the-mcp-server","text":"","title":"Starting the MCP Server"},{"location":"mcp_quick_start/#option-1-stdio-transport-recommended-for-mcp-clients","text":"reme \\ backend = mcp \\ mcp.transport = stdio \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local","title":"Option 1: STDIO Transport (Recommended for MCP clients)"},{"location":"mcp_quick_start/#option-2-sse-transport-server-sent-events","text":"reme \\ backend = mcp \\ mcp.transport = sse \\ http_service.port = 8001 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local The SSE server will start on http://localhost:8002/sse","title":"Option 2: SSE Transport (Server-Sent Events)"},{"location":"mcp_quick_start/#configuring-mcp-server-for-claude-desktop","text":"To integrate with Claude Desktop, add the following configuration to your claude_desktop_config.json : { \"mcpServers\" : { \"reme\" : { \"command\" : \"reme\" , \"args\" : [ \"backend=mcp\" , \"mcp.transport=stdio\" , \"llm.default.model_name=qwen3-30b-a3b-thinking-2507\" , \"embedding_model.default.model_name=text-embedding-v4\" , \"vector_store.default.backend=local_file\" ] } } } This configuration: Registers a new MCP server named \"reme\" Specifies the command to launch the server ( reme ) Configures the server to use STDIO transport Sets the LLM and embedding models to use Configures the vector store backend","title":"Configuring MCP Server for Claude Desktop"},{"location":"mcp_quick_start/#advanced-server-configuration-options","text":"For more advanced use cases, you can configure the server with additional parameters: # Full configuration example reme \\ backend = mcp \\ mcp.transport = stdio \\ http_service.host = 0 .0.0.0 \\ http_service.port = 8002 \\ llm.default.model_name = qwen3-30b-a3b-thinking-2507 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = elasticsearch \\","title":"Advanced Server Configuration Options"},{"location":"mcp_quick_start/#using-python-client-to-call-mcp-services","text":"The ReMe framework provides a Python client for interacting with MCP services. This section focuses specifically on using the summary_task_memory and retrieve_task_memory tools.","title":"\ud83d\udd0c Using Python Client to Call MCP Services"},{"location":"mcp_quick_start/#setting-up-the-python-mcp-client","text":"First, install the required packages: pip install fastmcp dotenv Then, create a basic client connection: import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # MCP server URL (for SSE transport) MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"my_workspace\" async def main (): async with Client ( MCP_URL ) as client : # Your MCP operations will go here pass if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Setting Up the Python MCP Client"},{"location":"mcp_quick_start/#using-the-task-memory-summarizer","text":"The summary_task_memory tool transforms conversation trajectories into valuable task memories: async def run_summary ( client , messages ): \"\"\" Generate a summary of conversation messages and create task memories Args: client: MCP client instance messages: List of message objects from a conversation Returns: None \"\"\" try : result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract memory list from response memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) print ( f \"Created memories: { memory_list } \" ) # Optionally save memories to file with open ( \"task_memory.jsonl\" , \"w\" ) as f : f . write ( json . dumps ( memory_list , indent = 2 , ensure_ascii = False )) except Exception as e : print ( f \"Error running summary: { e } \" )","title":"Using the Task Memory Summarizer"},{"location":"mcp_quick_start/#using-the-task-memory-retriever","text":"The retrieve_task_memory tool allows you to retrieve relevant memories based on a query: async def run_retrieve ( client , query ): \"\"\" Retrieve relevant task memories based on a query Args: client: MCP client instance query: The query to retrieve relevant memories Returns: String containing the retrieved memory answer \"\"\" try : result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : \"my_workspace\" , \"query\" : query , } ) # Parse the response import json response_data = json . loads ( result . content ) # Extract and return the answer answer = response_data . get ( \"answer\" , \"\" ) print ( f \"Retrieved memory: { answer } \" ) return answer except Exception as e : print ( f \"Error retrieving memory: { e } \" ) return \"\"","title":"Using the Task Memory Retriever"},{"location":"mcp_quick_start/#complete-memory-augmented-agent-example","text":"Here's a complete example showing how to build a memory-augmented agent using the MCP client: import json import asyncio from fastmcp import Client from dotenv import load_dotenv # Load environment variables load_dotenv () # API configuration MCP_URL = \"http://0.0.0.0:8002/sse/\" WORKSPACE_ID = \"test_workspace\" async def run_agent ( client , query ): \"\"\"Run the agent with a specific query\"\"\" result = await client . call_tool ( \"react\" , arguments = { \"query\" : query } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) messages = response_data . get ( \"messages\" , []) return messages async def run_summary ( client , messages ): \"\"\"Generate task memories from conversation\"\"\" result = await client . call_tool ( \"summary_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) response_data = json . loads ( result . content ) memory_list = response_data . get ( \"metadata\" , {}) . get ( \"memory_list\" , []) return memory_list async def run_retrieve ( client , query ): \"\"\"Retrieve relevant task memories\"\"\" result = await client . call_tool ( \"retrieve_task_memory\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query , } ) response_data = json . loads ( result . content ) answer = response_data . get ( \"answer\" , \"\" ) return answer async def memory_augmented_workflow (): \"\"\"Complete memory-augmented agent workflow\"\"\" query1 = \"Analyze Xiaomi Corporation\" query2 = \"Analyze the company Tesla.\" async with Client ( MCP_URL ) as client : # Step 1: Build initial memories with query2 print ( f \"Building memories with: ' { query2 } '\" ) messages = await run_agent ( client , query = query2 ) # Step 2: Summarize conversation to create memories print ( \"Creating memories from conversation\" ) memory_list = await run_summary ( client , messages ) print ( f \"Created { len ( memory_list ) } memories\" ) # Step 3: Retrieve relevant memories for query1 print ( f \"Retrieving memories for: ' { query1 } '\" ) retrieved_memory = await run_retrieve ( client , query1 ) # Step 4: Run agent with memory-augmented query print ( \"Running memory-augmented agent\" ) augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query1 } \" final_messages = await run_agent ( client , query = augmented_query ) # Extract the agent's final answer final_answer = \"\" for msg in final_messages : if msg . get ( \"role\" ) == \"assistant\" and msg . get ( \"content\" ): final_answer = msg . get ( \"content\" ) break print ( f \"Memory-augmented response: { final_answer } \" ) # Run the workflow if __name__ == \"__main__\" : asyncio . run ( memory_augmented_workflow ())","title":"Complete Memory-Augmented Agent Example"},{"location":"mcp_quick_start/#managing-vector-store-with-mcp","text":"You can also manage your vector store through MCP: async def manage_vector_store ( client ): # Delete a workspace await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" , } ) # Dump memories to disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./backups/\" , } ) # Load memories from disk await client . call_tool ( \"vector_store\" , arguments = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./backups/\" , } )","title":"Managing Vector Store with MCP"},{"location":"mcp_quick_start/#common-issues-and-troubleshooting","text":"","title":"\ud83d\udc1b Common Issues and Troubleshooting"},{"location":"mcp_quick_start/#mcp-server-wont-start","text":"Check if the required ports are available (for SSE transport) Verify your API keys in .env file Ensure Python version is 3.12+ Check MCP transport configuration","title":"MCP Server Won't Start"},{"location":"mcp_quick_start/#mcp-client-connection-issues","text":"For STDIO: Ensure the command path is correct in your MCP client config For SSE: Verify the server URL and port accessibility Check firewall settings for SSE connections","title":"MCP Client Connection Issues"},{"location":"mcp_quick_start/#no-memories-retrieved","text":"Make sure you've run the summarizer tool first to create memories Check if workspace_id matches between operations Verify vector store backend is properly configured","title":"No Memories Retrieved"},{"location":"mcp_quick_start/#api-connection-errors","text":"Confirm LLM_BASE_URL and API keys are correct Test API access independently Check network connectivity","title":"API Connection Errors"},{"location":"vector_store_api_guide/","text":"This guide covers the vector store implementations available in flowllm, their APIs, and how to use them effectively. \ud83d\udccb Overview flowllm provides multiple vector store backends for different use cases: LocalVectorStore ( backend=local ) - \ud83d\udcc1 Simple file-based storage for development and small datasets ChromaVectorStore ( backend=chroma ) - \ud83d\udd2e Embedded vector database for moderate scale EsVectorStore ( backend=elasticsearch ) - \ud83d\udd0d Elasticsearch-based storage for production and large scale MemoryVectorStore ( backend=memory ) - \u26a1 In-memory storage for ultra-fast access and testing All vector stores implement the BaseVectorStore interface, providing a consistent API across implementations. \ud83d\udcca Comparison Table Feature LocalVectorStore ChromaVectorStore EsVectorStore MemoryVectorStore Storage File (JSONL) Embedded DB Elasticsearch In-Memory Performance Medium Good Excellent Ultra-Fast Scalability < 10K vectors < 1M vectors > 1M vectors < 1M vectors Persistence \u2705 Auto \u2705 Auto \u2705 Auto \u26a0\ufe0f Manual Setup Complexity \ud83d\udfe2 Simple \ud83d\udfe1 Medium \ud83d\udd34 Complex \ud83d\udfe2 Simple Dependencies None ChromaDB Elasticsearch None Filtering \u274c Basic \u2705 Metadata \u2705 Advanced \u274c Basic Concurrency \u274c Limited \u2705 Good \u2705 Excellent \u274c Single Process Best For Development Local Apps Production Testing \ud83d\udd04 Common API Methods All vector store implementations share these core methods: \ud83d\udd04 Async Support All vector stores provide both synchronous and asynchronous versions of every method: # Synchronous methods store . search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) store . insert ( nodes , workspace_id = \"workspace\" ) # Asynchronous methods (with async_ prefix) await store . async_search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) await store . async_insert ( nodes , workspace_id = \"workspace\" ) Workspace Management # Check if workspace exists store . exist_workspace ( workspace_id : str ) -> bool # Create a new workspace store . create_workspace ( workspace_id : str , ** kwargs ) # Delete a workspace store . delete_workspace ( workspace_id : str , ** kwargs ) # Copy a workspace store . copy_workspace ( src_workspace_id : str , dest_workspace_id : str , ** kwargs ) Data Operations # Insert nodes (single or list) store . insert ( nodes : VectorNode | List [ VectorNode ], workspace_id : str , ** kwargs ) # Delete nodes by ID store . delete ( node_ids : str | List [ str ], workspace_id : str , ** kwargs ) # Search for similar nodes store . search ( query : str , workspace_id : str , top_k : int = 1 , ** kwargs ) -> List [ VectorNode ] # Iterate through workspace nodes for node in store . iter_workspace_nodes ( workspace_id : str , ** kwargs ): # Process each node Import/Export # Export workspace to file store . dump_workspace ( workspace_id : str , path : str | Path = \"\" , callback_fn = None , ** kwargs ) # Import workspace from file store . load_workspace ( workspace_id : str , path : str | Path = \"\" , nodes : List [ VectorNode ] = None , callback_fn = None , ** kwargs ) \u26a1 Vector Store Implementations 1. \ud83d\udcc1 LocalVectorStore ( backend=local ) A simple file-based vector store that saves data to local JSONL files. \ud83d\udca1 When to Use Development and testing - No external dependencies required \ud83d\udee0\ufe0f Small datasets - Suitable for datasets with < 10,000 vectors \ud83d\udcca Single-user applications - Limited concurrent access support \ud83d\udc64 \u2699\ufe0f Configuration from flowllm.storage.vector_store import LocalVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables (for API keys) load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./file_vector_store\" , # Directory to store JSONL files batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode # Create workspace workspace_id = \"my_workspace\" vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Artificial intelligence is revolutionizing technology\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article1\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Machine learning enables data-driven insights\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article2\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"What is AI?\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) 2. \ud83d\udd2e ChromaVectorStore ( backend=chroma ) An embedded vector database that provides persistent storage with advanced features. \ud83d\udca1 When to Use Local development with persistence requirements \ud83c\udfe0 Medium-scale applications (10K - 1M vectors) \ud83d\udcc8 Applications requiring metadata filtering \ud83d\udd0d \u2699\ufe0f Configuration from flowllm.storage.vector_store import ChromaVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = ChromaVectorStore ( embedding_model = embedding_model , store_dir = \"./chroma_vector_store\" , # Directory for Chroma database batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode workspace_id = \"chroma_workspace\" # Check if workspace exists and create if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with metadata nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Deep learning models require large datasets\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"advanced\" , \"topic\" : \"deep_learning\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"intermediate\" , \"topic\" : \"transformers\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"deep learning\" , workspace_id , top_k = 5 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) 3. \ud83d\udd0d EsVectorStore ( backend=elasticsearch ) Production-grade vector search using Elasticsearch with advanced filtering and scaling capabilities. \ud83d\udca1 When to Use Production environments requiring high availability \ud83c\udfed Large-scale applications (1M+ vectors) \ud83d\ude80 Complex filtering requirements on metadata \ud83c\udfaf \ud83d\udee0\ufe0f Setup Elasticsearch Before using EsVectorStore, set up Elasticsearch: Option 1: Docker Run # Pull the latest Elasticsearch image docker pull docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 # Run Elasticsearch container docker run -p 9200 :9200 \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"xpack.license.self_generated.type=trial\" \\ -e \"http.host=0.0.0.0\" \\ docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 Environment Configuration export FLOW_ES_HOSTS = http://localhost:9200 \u2699\ufe0f Configuration from flowllm.storage.vector_store import EsVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env import os # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = EsVectorStore ( embedding_model = embedding_model , hosts = os . getenv ( \"FLOW_ES_HOSTS\" , \"http://localhost:9200\" ), # Elasticsearch hosts basic_auth = None , # (\"username\", \"password\") for auth batch_size = 1024 # Batch size for bulk operations ) \ud83c\udfaf Advanced Filtering EsVectorStore supports advanced filtering capabilities through the filter_dict parameter: # Term filters (exact match) term_filter = { \"category\" : \"technology\" , \"author\" : \"research_team\" } # Range filters (numeric and date ranges) range_filter = { \"score\" : { \"gte\" : 0.8 }, # Score >= 0.8 \"confidence\" : { \"gte\" : 0.5 , \"lte\" : 0.9 }, # Between 0.5 and 0.9 \"timestamp\" : { \"gte\" : \"2024-01-01\" , \"lte\" : \"2024-12-31\" } } # Combined filters (filters are combined with AND logic) combined_filter = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } # Search with filters applied results = vector_store . search ( \"machine learning\" , workspace_id , top_k = 10 , filter_dict = combined_filter ) \u26a1 Performance Optimization # Refresh index for immediate availability (useful after bulk inserts) vector_store . insert ( nodes , workspace_id , refresh = True ) # Auto-refresh vector_store . refresh ( workspace_id ) # Manual refresh # Bulk operations with custom batch size vector_store . insert ( large_node_list , workspace_id , refresh = False ) # Skip refresh for speed vector_store . refresh ( workspace_id ) # Refresh once after all inserts \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode # Define workspace workspace_id = \"production_workspace\" # Create workspace if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with rich metadata nodes = [ VectorNode ( unique_id = \"doc1\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"subcategory\" : \"NLP\" , \"author\" : \"research_team\" , \"timestamp\" : \"2024-01-15\" , \"confidence\" : 0.95 , \"tags\" : [ \"transformer\" , \"nlp\" , \"attention\" ] } ) ] # Insert with refresh for immediate availability vector_store . insert ( nodes , workspace_id , refresh = True ) # Advanced search with filters filter_dict = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } results = vector_store . search ( \"transformer models\" , workspace_id , top_k = 5 , filter_dict = filter_dict ) for result in results : print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) 4. \u26a1 MemoryVectorStore ( backend=memory ) An ultra-fast in-memory vector store that keeps all data in RAM for maximum performance. \ud83d\udca1 When to Use Testing and development - Fastest possible operations for unit tests \ud83e\uddea Small to medium datasets that fit in memory (< 1M vectors) \ud83d\udcbe Applications requiring ultra-low latency search operations \u26a1 Temporary workspaces that don't need persistence \ud83d\ude80 \u2699\ufe0f Configuration from flowllm.storage.vector_store import MemoryVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = MemoryVectorStore ( embedding_model = embedding_model , store_dir = \"./memory_vector_store\" , # Directory for backup/restore operations batch_size = 1024 # Batch size for operations ) \ud83d\udcbb Example Usage from flowllm.schema.vector_node import VectorNode workspace_id = \"memory_workspace\" # Create workspace in memory vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"mem_node1\" , workspace_id = workspace_id , content = \"Memory stores provide ultra-fast access to data\" , metadata = { \"category\" : \"performance\" , \"type\" : \"memory\" , \"speed\" : \"ultra_fast\" } ), VectorNode ( unique_id = \"mem_node2\" , workspace_id = workspace_id , content = \"In-memory databases excel at low-latency operations\" , metadata = { \"category\" : \"performance\" , \"type\" : \"database\" , \"latency\" : \"low\" } ) ] # Insert nodes (stored in memory) vector_store . insert ( nodes , workspace_id ) # Ultra-fast search results = vector_store . search ( \"fast memory access\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) # Optional: Save to disk for backup vector_store . dump_workspace ( workspace_id , path = \"./backup\" ) # Optional: Load from disk to memory vector_store . load_workspace ( workspace_id , path = \"./backup\" ) \u26a1 Performance Benefits Zero I/O latency - All operations happen in RAM Instant search results - No disk or network overhead Perfect for testing - Fast setup and teardown Memory efficient - Only stores what you need \ud83d\udea8 Important Notes Data is volatile - Lost when process ends unless explicitly saved Memory usage - Entire dataset must fit in available RAM No persistence - Use dump_workspace() to save to disk Single process - Not suitable for distributed applications \ud83d\udcdd Working with VectorNode The VectorNode class is the fundamental data unit for all vector stores: from flowllm.schema.vector_node import VectorNode # Create a node node = VectorNode ( unique_id = \"unique_identifier\" , # Unique ID for the node (required) workspace_id = \"my_workspace\" , # Workspace ID (required) content = \"Text content to embed\" , # Content to be embedded (required) metadata = { # Optional metadata \"source\" : \"document1\" , \"category\" : \"technology\" , \"timestamp\" : \"2024-08-29\" }, vector = None # Vector will be generated automatically if None ) \ud83d\udd04 Import/Export Example Export and import workspaces for backup or transfer: # Export workspace to file vector_store . dump_workspace ( workspace_id = \"my_workspace\" , path = \"./backup_data\" # Directory to store the exported data ) # Import workspace from file vector_store . load_workspace ( workspace_id = \"new_workspace\" , path = \"./backup_data\" # Directory containing the exported data ) # Copy workspace within the same store vector_store . copy_workspace ( src_workspace_id = \"original_workspace\" , dest_workspace_id = \"copied_workspace\" ) \ud83e\udde9 Integration with Embedding Models All vector stores require an embedding model to function: from flowllm.embedding_model import OpenAICompatibleEmbeddingModel # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , # Embedding dimensions model_name = \"text-embedding-v4\" , # Model name batch_size = 32 # Batch size for embedding generation ) # Pass to vector store (example with LocalVectorStore) # You can also use: ChromaVectorStore, EsVectorStore, or MemoryVectorStore vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./vector_store\" ) \ud83c\udf89 This guide provides everything you need to work with vector stores in flowllm. Choose the implementation that best fits your use case and scale up as needed! \u2728","title":"Vector Store"},{"location":"vector_store_api_guide/#overview","text":"flowllm provides multiple vector store backends for different use cases: LocalVectorStore ( backend=local ) - \ud83d\udcc1 Simple file-based storage for development and small datasets ChromaVectorStore ( backend=chroma ) - \ud83d\udd2e Embedded vector database for moderate scale EsVectorStore ( backend=elasticsearch ) - \ud83d\udd0d Elasticsearch-based storage for production and large scale MemoryVectorStore ( backend=memory ) - \u26a1 In-memory storage for ultra-fast access and testing All vector stores implement the BaseVectorStore interface, providing a consistent API across implementations.","title":"\ud83d\udccb Overview"},{"location":"vector_store_api_guide/#comparison-table","text":"Feature LocalVectorStore ChromaVectorStore EsVectorStore MemoryVectorStore Storage File (JSONL) Embedded DB Elasticsearch In-Memory Performance Medium Good Excellent Ultra-Fast Scalability < 10K vectors < 1M vectors > 1M vectors < 1M vectors Persistence \u2705 Auto \u2705 Auto \u2705 Auto \u26a0\ufe0f Manual Setup Complexity \ud83d\udfe2 Simple \ud83d\udfe1 Medium \ud83d\udd34 Complex \ud83d\udfe2 Simple Dependencies None ChromaDB Elasticsearch None Filtering \u274c Basic \u2705 Metadata \u2705 Advanced \u274c Basic Concurrency \u274c Limited \u2705 Good \u2705 Excellent \u274c Single Process Best For Development Local Apps Production Testing","title":"\ud83d\udcca Comparison Table"},{"location":"vector_store_api_guide/#common-api-methods","text":"All vector store implementations share these core methods:","title":"\ud83d\udd04 Common API Methods"},{"location":"vector_store_api_guide/#async-support","text":"All vector stores provide both synchronous and asynchronous versions of every method: # Synchronous methods store . search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) store . insert ( nodes , workspace_id = \"workspace\" ) # Asynchronous methods (with async_ prefix) await store . async_search ( query = \"example\" , workspace_id = \"workspace\" , top_k = 5 ) await store . async_insert ( nodes , workspace_id = \"workspace\" )","title":"\ud83d\udd04 Async Support"},{"location":"vector_store_api_guide/#workspace-management","text":"# Check if workspace exists store . exist_workspace ( workspace_id : str ) -> bool # Create a new workspace store . create_workspace ( workspace_id : str , ** kwargs ) # Delete a workspace store . delete_workspace ( workspace_id : str , ** kwargs ) # Copy a workspace store . copy_workspace ( src_workspace_id : str , dest_workspace_id : str , ** kwargs )","title":"Workspace Management"},{"location":"vector_store_api_guide/#data-operations","text":"# Insert nodes (single or list) store . insert ( nodes : VectorNode | List [ VectorNode ], workspace_id : str , ** kwargs ) # Delete nodes by ID store . delete ( node_ids : str | List [ str ], workspace_id : str , ** kwargs ) # Search for similar nodes store . search ( query : str , workspace_id : str , top_k : int = 1 , ** kwargs ) -> List [ VectorNode ] # Iterate through workspace nodes for node in store . iter_workspace_nodes ( workspace_id : str , ** kwargs ): # Process each node","title":"Data Operations"},{"location":"vector_store_api_guide/#importexport","text":"# Export workspace to file store . dump_workspace ( workspace_id : str , path : str | Path = \"\" , callback_fn = None , ** kwargs ) # Import workspace from file store . load_workspace ( workspace_id : str , path : str | Path = \"\" , nodes : List [ VectorNode ] = None , callback_fn = None , ** kwargs )","title":"Import/Export"},{"location":"vector_store_api_guide/#vector-store-implementations","text":"","title":"\u26a1 Vector Store Implementations"},{"location":"vector_store_api_guide/#1-localvectorstore-backendlocal","text":"A simple file-based vector store that saves data to local JSONL files.","title":"1. \ud83d\udcc1 LocalVectorStore (backend=local)"},{"location":"vector_store_api_guide/#when-to-use","text":"Development and testing - No external dependencies required \ud83d\udee0\ufe0f Small datasets - Suitable for datasets with < 10,000 vectors \ud83d\udcca Single-user applications - Limited concurrent access support \ud83d\udc64","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration","text":"from flowllm.storage.vector_store import LocalVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables (for API keys) load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./file_vector_store\" , # Directory to store JSONL files batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage","text":"from flowllm.schema.vector_node import VectorNode # Create workspace workspace_id = \"my_workspace\" vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Artificial intelligence is revolutionizing technology\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article1\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Machine learning enables data-driven insights\" , metadata = { \"category\" : \"tech\" , \"source\" : \"article2\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"What is AI?\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#2-chromavectorstore-backendchroma","text":"An embedded vector database that provides persistent storage with advanced features.","title":"2. \ud83d\udd2e ChromaVectorStore (backend=chroma)"},{"location":"vector_store_api_guide/#when-to-use_1","text":"Local development with persistence requirements \ud83c\udfe0 Medium-scale applications (10K - 1M vectors) \ud83d\udcc8 Applications requiring metadata filtering \ud83d\udd0d","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration_1","text":"from flowllm.storage.vector_store import ChromaVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = ChromaVectorStore ( embedding_model = embedding_model , store_dir = \"./chroma_vector_store\" , # Directory for Chroma database batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage_1","text":"from flowllm.schema.vector_node import VectorNode workspace_id = \"chroma_workspace\" # Check if workspace exists and create if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with metadata nodes = [ VectorNode ( unique_id = \"node1\" , workspace_id = workspace_id , content = \"Deep learning models require large datasets\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"advanced\" , \"topic\" : \"deep_learning\" } ), VectorNode ( unique_id = \"node2\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"difficulty\" : \"intermediate\" , \"topic\" : \"transformers\" } ) ] # Insert nodes vector_store . insert ( nodes , workspace_id ) # Search results = vector_store . search ( \"deep learning\" , workspace_id , top_k = 5 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#3-esvectorstore-backendelasticsearch","text":"Production-grade vector search using Elasticsearch with advanced filtering and scaling capabilities.","title":"3. \ud83d\udd0d EsVectorStore (backend=elasticsearch)"},{"location":"vector_store_api_guide/#when-to-use_2","text":"Production environments requiring high availability \ud83c\udfed Large-scale applications (1M+ vectors) \ud83d\ude80 Complex filtering requirements on metadata \ud83c\udfaf","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#setup-elasticsearch","text":"Before using EsVectorStore, set up Elasticsearch:","title":"\ud83d\udee0\ufe0f Setup Elasticsearch"},{"location":"vector_store_api_guide/#option-1-docker-run","text":"# Pull the latest Elasticsearch image docker pull docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0 # Run Elasticsearch container docker run -p 9200 :9200 \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"xpack.license.self_generated.type=trial\" \\ -e \"http.host=0.0.0.0\" \\ docker.elastic.co/elasticsearch/elasticsearch-wolfi:9.0.0","title":"Option 1: Docker Run"},{"location":"vector_store_api_guide/#environment-configuration","text":"export FLOW_ES_HOSTS = http://localhost:9200","title":"Environment Configuration"},{"location":"vector_store_api_guide/#configuration_2","text":"from flowllm.storage.vector_store import EsVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env import os # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = EsVectorStore ( embedding_model = embedding_model , hosts = os . getenv ( \"FLOW_ES_HOSTS\" , \"http://localhost:9200\" ), # Elasticsearch hosts basic_auth = None , # (\"username\", \"password\") for auth batch_size = 1024 # Batch size for bulk operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#advanced-filtering","text":"EsVectorStore supports advanced filtering capabilities through the filter_dict parameter: # Term filters (exact match) term_filter = { \"category\" : \"technology\" , \"author\" : \"research_team\" } # Range filters (numeric and date ranges) range_filter = { \"score\" : { \"gte\" : 0.8 }, # Score >= 0.8 \"confidence\" : { \"gte\" : 0.5 , \"lte\" : 0.9 }, # Between 0.5 and 0.9 \"timestamp\" : { \"gte\" : \"2024-01-01\" , \"lte\" : \"2024-12-31\" } } # Combined filters (filters are combined with AND logic) combined_filter = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } # Search with filters applied results = vector_store . search ( \"machine learning\" , workspace_id , top_k = 10 , filter_dict = combined_filter )","title":"\ud83c\udfaf Advanced Filtering"},{"location":"vector_store_api_guide/#performance-optimization","text":"# Refresh index for immediate availability (useful after bulk inserts) vector_store . insert ( nodes , workspace_id , refresh = True ) # Auto-refresh vector_store . refresh ( workspace_id ) # Manual refresh # Bulk operations with custom batch size vector_store . insert ( large_node_list , workspace_id , refresh = False ) # Skip refresh for speed vector_store . refresh ( workspace_id ) # Refresh once after all inserts","title":"\u26a1 Performance Optimization"},{"location":"vector_store_api_guide/#example-usage_2","text":"from flowllm.schema.vector_node import VectorNode # Define workspace workspace_id = \"production_workspace\" # Create workspace if needed if not vector_store . exist_workspace ( workspace_id ): vector_store . create_workspace ( workspace_id ) # Create nodes with rich metadata nodes = [ VectorNode ( unique_id = \"doc1\" , workspace_id = workspace_id , content = \"Transformer architecture revolutionized NLP\" , metadata = { \"category\" : \"AI\" , \"subcategory\" : \"NLP\" , \"author\" : \"research_team\" , \"timestamp\" : \"2024-01-15\" , \"confidence\" : 0.95 , \"tags\" : [ \"transformer\" , \"nlp\" , \"attention\" ] } ) ] # Insert with refresh for immediate availability vector_store . insert ( nodes , workspace_id , refresh = True ) # Advanced search with filters filter_dict = { \"category\" : \"AI\" , \"confidence\" : { \"gte\" : 0.9 } } results = vector_store . search ( \"transformer models\" , workspace_id , top_k = 5 , filter_dict = filter_dict ) for result in results : print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) print ( f \"Content: { result . content } \" ) print ( f \"Metadata: { result . metadata } \" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#4-memoryvectorstore-backendmemory","text":"An ultra-fast in-memory vector store that keeps all data in RAM for maximum performance.","title":"4. \u26a1 MemoryVectorStore (backend=memory)"},{"location":"vector_store_api_guide/#when-to-use_3","text":"Testing and development - Fastest possible operations for unit tests \ud83e\uddea Small to medium datasets that fit in memory (< 1M vectors) \ud83d\udcbe Applications requiring ultra-low latency search operations \u26a1 Temporary workspaces that don't need persistence \ud83d\ude80","title":"\ud83d\udca1 When to Use"},{"location":"vector_store_api_guide/#configuration_3","text":"from flowllm.storage.vector_store import MemoryVectorStore from flowllm.embedding_model import OpenAICompatibleEmbeddingModel from flowllm.utils.common_utils import load_env # Load environment variables load_env () # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , model_name = \"text-embedding-v4\" ) # Initialize vector store vector_store = MemoryVectorStore ( embedding_model = embedding_model , store_dir = \"./memory_vector_store\" , # Directory for backup/restore operations batch_size = 1024 # Batch size for operations )","title":"\u2699\ufe0f Configuration"},{"location":"vector_store_api_guide/#example-usage_3","text":"from flowllm.schema.vector_node import VectorNode workspace_id = \"memory_workspace\" # Create workspace in memory vector_store . create_workspace ( workspace_id ) # Create nodes nodes = [ VectorNode ( unique_id = \"mem_node1\" , workspace_id = workspace_id , content = \"Memory stores provide ultra-fast access to data\" , metadata = { \"category\" : \"performance\" , \"type\" : \"memory\" , \"speed\" : \"ultra_fast\" } ), VectorNode ( unique_id = \"mem_node2\" , workspace_id = workspace_id , content = \"In-memory databases excel at low-latency operations\" , metadata = { \"category\" : \"performance\" , \"type\" : \"database\" , \"latency\" : \"low\" } ) ] # Insert nodes (stored in memory) vector_store . insert ( nodes , workspace_id ) # Ultra-fast search results = vector_store . search ( \"fast memory access\" , workspace_id , top_k = 2 ) for result in results : print ( f \"Content: { result . content } \" ) print ( f \"Score: { result . metadata . get ( 'score' , 'N/A' ) } \" ) # Optional: Save to disk for backup vector_store . dump_workspace ( workspace_id , path = \"./backup\" ) # Optional: Load from disk to memory vector_store . load_workspace ( workspace_id , path = \"./backup\" )","title":"\ud83d\udcbb Example Usage"},{"location":"vector_store_api_guide/#performance-benefits","text":"Zero I/O latency - All operations happen in RAM Instant search results - No disk or network overhead Perfect for testing - Fast setup and teardown Memory efficient - Only stores what you need","title":"\u26a1 Performance Benefits"},{"location":"vector_store_api_guide/#important-notes","text":"Data is volatile - Lost when process ends unless explicitly saved Memory usage - Entire dataset must fit in available RAM No persistence - Use dump_workspace() to save to disk Single process - Not suitable for distributed applications","title":"\ud83d\udea8 Important Notes"},{"location":"vector_store_api_guide/#working-with-vectornode","text":"The VectorNode class is the fundamental data unit for all vector stores: from flowllm.schema.vector_node import VectorNode # Create a node node = VectorNode ( unique_id = \"unique_identifier\" , # Unique ID for the node (required) workspace_id = \"my_workspace\" , # Workspace ID (required) content = \"Text content to embed\" , # Content to be embedded (required) metadata = { # Optional metadata \"source\" : \"document1\" , \"category\" : \"technology\" , \"timestamp\" : \"2024-08-29\" }, vector = None # Vector will be generated automatically if None )","title":"\ud83d\udcdd Working with VectorNode"},{"location":"vector_store_api_guide/#importexport-example","text":"Export and import workspaces for backup or transfer: # Export workspace to file vector_store . dump_workspace ( workspace_id = \"my_workspace\" , path = \"./backup_data\" # Directory to store the exported data ) # Import workspace from file vector_store . load_workspace ( workspace_id = \"new_workspace\" , path = \"./backup_data\" # Directory containing the exported data ) # Copy workspace within the same store vector_store . copy_workspace ( src_workspace_id = \"original_workspace\" , dest_workspace_id = \"copied_workspace\" )","title":"\ud83d\udd04 Import/Export Example"},{"location":"vector_store_api_guide/#integration-with-embedding-models","text":"All vector stores require an embedding model to function: from flowllm.embedding_model import OpenAICompatibleEmbeddingModel # Initialize embedding model embedding_model = OpenAICompatibleEmbeddingModel ( dimensions = 64 , # Embedding dimensions model_name = \"text-embedding-v4\" , # Model name batch_size = 32 # Batch size for embedding generation ) # Pass to vector store (example with LocalVectorStore) # You can also use: ChromaVectorStore, EsVectorStore, or MemoryVectorStore vector_store = LocalVectorStore ( embedding_model = embedding_model , store_dir = \"./vector_store\" ) \ud83c\udf89 This guide provides everything you need to work with vector stores in flowllm. Choose the implementation that best fits your use case and scale up as needed! \u2728","title":"\ud83e\udde9 Integration with Embedding Models"},{"location":"cookbook/appworld/quickstart/","text":"This guide helps you quickly set up and run AppWorld experiments with ReMe integration. Env Setup 1. Clone the Repository git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/appworld 2. Appworld Environment Setup Create a new conda environment with Python 3.12: conda create -p ./appworld-env python == 3 .12 conda activate ./appworld-env Install required Python packages: pip install -r requirements.txt Install AppWorld and download the dataset: pip install appworld appworld install appworld download data Note : The AppWorld data will be saved in the current directory. 3. Start ReMe Service Install ReMe (if not already installed) If you haven't installed the ReMe environment yet, follow these steps: # Go back to the project root cd ../.. # Create ReMe environment conda create -p ./reme-env python == 3 .12 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-latest \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local add memories for appworld: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"appworld\", \"action\": \"load\", \"path\": \"./docs/library\" }' Now you have loaded the ReMe memory library to enable memory-based agent! 4. Common Issues AppWorld data not found : Ensure appworld download data completed successfully pydantic version issue : AppWorld depends on an older version of pydantic, which is why a separate environment is needed. If you encounter issues running the experiments, try pip install appworld to override the dependencies. Run Experiments 1. Test: With Memory vs Without Memory Run the main experiment script to compare performance with and without memory: python run_appworld.py What this does: - Runs AppWorld tasks on the development dataset - Compares agent performance with ReMe memory ( use_memory=True ) vs without memory - Uses multiple workers for parallel processing - Runs each task multiple times for statistical significance - Results are automatically saved to ./exp_result/ directory Configuration options in run_appworld.py : - max_workers : Number of parallel workers (default: 6) - num_runs : Number of times each task is repeated (default: 4) - use_memory : Whether to use ReMe memory library 2. View Experiment Results After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv Metrics explained: - best@k : Takes groups of k runs per task, finds the maximum score in each group, then averages these maximums - Higher k values show potential performance, lower k values show consistency Output Files ./exp_result/*.jsonl : Raw experiment results for each configuration ./exp_result/experiment_summary.csv : Statistical summary table Console output: Real-time progress and summary statistics Understanding Results The experiment compares: 1. Baseline : Agent without memory library 2. With Memory : Agent enhanced with ReMe memory library Key metrics to look for: - best@1 : Average performance across all single runs - best@k : Performance when taking the best of k attempts - Improvement percentage when using memory vs baseline","title":"AppWorld"},{"location":"cookbook/appworld/quickstart/#env-setup","text":"","title":"Env Setup"},{"location":"cookbook/appworld/quickstart/#1-clone-the-repository","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/appworld","title":"1. Clone the Repository"},{"location":"cookbook/appworld/quickstart/#2-appworld-environment-setup","text":"Create a new conda environment with Python 3.12: conda create -p ./appworld-env python == 3 .12 conda activate ./appworld-env Install required Python packages: pip install -r requirements.txt Install AppWorld and download the dataset: pip install appworld appworld install appworld download data Note : The AppWorld data will be saved in the current directory.","title":"2. Appworld Environment Setup"},{"location":"cookbook/appworld/quickstart/#3-start-reme-service","text":"Install ReMe (if not already installed) If you haven't installed the ReMe environment yet, follow these steps: # Go back to the project root cd ../.. # Create ReMe environment conda create -p ./reme-env python == 3 .12 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-latest \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local add memories for appworld: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"appworld\", \"action\": \"load\", \"path\": \"./docs/library\" }' Now you have loaded the ReMe memory library to enable memory-based agent!","title":"3. Start ReMe Service"},{"location":"cookbook/appworld/quickstart/#4-common-issues","text":"AppWorld data not found : Ensure appworld download data completed successfully pydantic version issue : AppWorld depends on an older version of pydantic, which is why a separate environment is needed. If you encounter issues running the experiments, try pip install appworld to override the dependencies.","title":"4. Common Issues"},{"location":"cookbook/appworld/quickstart/#run-experiments","text":"","title":"Run Experiments"},{"location":"cookbook/appworld/quickstart/#1-test-with-memory-vs-without-memory","text":"Run the main experiment script to compare performance with and without memory: python run_appworld.py What this does: - Runs AppWorld tasks on the development dataset - Compares agent performance with ReMe memory ( use_memory=True ) vs without memory - Uses multiple workers for parallel processing - Runs each task multiple times for statistical significance - Results are automatically saved to ./exp_result/ directory Configuration options in run_appworld.py : - max_workers : Number of parallel workers (default: 6) - num_runs : Number of times each task is repeated (default: 4) - use_memory : Whether to use ReMe memory library","title":"1. Test: With Memory vs Without Memory"},{"location":"cookbook/appworld/quickstart/#2-view-experiment-results","text":"After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv Metrics explained: - best@k : Takes groups of k runs per task, finds the maximum score in each group, then averages these maximums - Higher k values show potential performance, lower k values show consistency Output Files ./exp_result/*.jsonl : Raw experiment results for each configuration ./exp_result/experiment_summary.csv : Statistical summary table Console output: Real-time progress and summary statistics","title":"2. View Experiment Results"},{"location":"cookbook/appworld/quickstart/#understanding-results","text":"The experiment compares: 1. Baseline : Agent without memory library 2. With Memory : Agent enhanced with ReMe memory library Key metrics to look for: - best@1 : Average performance across all single runs - best@k : Performance when taking the best of k attempts - Improvement percentage when using memory vs baseline","title":"Understanding Results"},{"location":"cookbook/bfcl/quickstart/","text":"This guide helps you quickly set up and run BFCL experiments with ReMe integration. Env Setup 1. BFCL installation clone the repository git clone https://github.com/ShishirPatil/gorilla.git Change directory to the berkeley-function-call-leaderboard cd gorilla/berkeley-function-call-leaderboard Install the package in editable mode conda create -n bfcl-env python == 3 .12 conda activate bfcl-env pip install -e . pip install -r requirements.txt Move the dataset to the data folder under bfcl cp -r bfcl_eval/data { /path/to/bfcl/data } Note : The original BFCL data is designed as a benchmark dataset and does not have a train/validation split, you can use split_into_trainval.py to split JSONL file into train and validation sets. 2. Collect agent trajectories on training data set Run the main experiment script to collect agent trajectories on training data set without task memory( use_memory=False ): python run_bfcl.py Note : - max_workers : Number of parallel workers (default: 4 ) - num_runs : Number of times each task is repeated (default: 1 ) - model_name : LLM model name (default: qwen3-8b ) - enable_thinking : Control the model's thinking mode (default: False ) - data_path : Path to the training dataset (default: ./data/multiturn_data_base_train.jsonl ) - answer_path : Path to the possible answer, which are used to evaluate the model's output function (default: ./data/possible_answer ) - Results are automatically saved to ./exp_result/{model_name}/{no_think/with_think} directory 3. Start ReMe Service and Init the task memory pool After collecting trajectories, Launch the ReMe service (make sure you have installed ReMe environment, if not please follow the steps in the ReMe Installation Guide to install): reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local and then init the task memory pool: python init_exp_pool.py Configuration options in init_exp_pool.py : - jsonl_file : Path to the collloaded trajectories - service_url : ReMe service URL (default: http://localhost:8002 ) - workspace_id : Workspace ID for the task memory pool (default: bfcl_test ) - n_threads : Number of threads for processing (default: 4 ) - output_file : Output file to save results (optional) Now you have inited the task memory pool using local backend (start on http://localhost:8002 ). Then, use local_file_to_library.py script to convert the local file to the memory library or run the following curl command: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"dump\", \"path\": \"./library\" }' to dump the memory library (default in ./library/bfcl_test.jsonl ). Next time, you can import this previously exported task memory data to populate the new started workspace with existing knowledge: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"load\", \"path\": \"./library\" }' 4. Run Experiments on Validation Set Run you can compare agent performance on the validation set with task memory ( use_memory=True ) and without task memory: # remember to change the configuration options, e.g., `data_path=./data/multiturn_data_base_val.jsonl` python run_bfcl.py After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv","title":"BFCL"},{"location":"cookbook/bfcl/quickstart/#env-setup","text":"","title":"Env Setup"},{"location":"cookbook/bfcl/quickstart/#1-bfcl-installation","text":"","title":"1. BFCL installation"},{"location":"cookbook/bfcl/quickstart/#clone-the-repository","text":"git clone https://github.com/ShishirPatil/gorilla.git","title":"clone the repository"},{"location":"cookbook/bfcl/quickstart/#change-directory-to-the-berkeley-function-call-leaderboard","text":"cd gorilla/berkeley-function-call-leaderboard","title":"Change directory to the berkeley-function-call-leaderboard"},{"location":"cookbook/bfcl/quickstart/#install-the-package-in-editable-mode","text":"conda create -n bfcl-env python == 3 .12 conda activate bfcl-env pip install -e . pip install -r requirements.txt","title":"Install the package in editable mode"},{"location":"cookbook/bfcl/quickstart/#move-the-dataset-to-the-data-folder-under-bfcl","text":"cp -r bfcl_eval/data { /path/to/bfcl/data } Note : The original BFCL data is designed as a benchmark dataset and does not have a train/validation split, you can use split_into_trainval.py to split JSONL file into train and validation sets.","title":"Move the dataset to the data folder under bfcl"},{"location":"cookbook/bfcl/quickstart/#2-collect-agent-trajectories-on-training-data-set","text":"Run the main experiment script to collect agent trajectories on training data set without task memory( use_memory=False ): python run_bfcl.py Note : - max_workers : Number of parallel workers (default: 4 ) - num_runs : Number of times each task is repeated (default: 1 ) - model_name : LLM model name (default: qwen3-8b ) - enable_thinking : Control the model's thinking mode (default: False ) - data_path : Path to the training dataset (default: ./data/multiturn_data_base_train.jsonl ) - answer_path : Path to the possible answer, which are used to evaluate the model's output function (default: ./data/possible_answer ) - Results are automatically saved to ./exp_result/{model_name}/{no_think/with_think} directory","title":"2. Collect agent trajectories on training data set"},{"location":"cookbook/bfcl/quickstart/#3-start-reme-service-and-init-the-task-memory-pool","text":"After collecting trajectories, Launch the ReMe service (make sure you have installed ReMe environment, if not please follow the steps in the ReMe Installation Guide to install): reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local and then init the task memory pool: python init_exp_pool.py Configuration options in init_exp_pool.py : - jsonl_file : Path to the collloaded trajectories - service_url : ReMe service URL (default: http://localhost:8002 ) - workspace_id : Workspace ID for the task memory pool (default: bfcl_test ) - n_threads : Number of threads for processing (default: 4 ) - output_file : Output file to save results (optional) Now you have inited the task memory pool using local backend (start on http://localhost:8002 ). Then, use local_file_to_library.py script to convert the local file to the memory library or run the following curl command: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"dump\", \"path\": \"./library\" }' to dump the memory library (default in ./library/bfcl_test.jsonl ). Next time, you can import this previously exported task memory data to populate the new started workspace with existing knowledge: curl -X POST \"http://0.0.0.0:8002/vector_store\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"workspace_id\": \"bfcl_test\", \"action\": \"load\", \"path\": \"./library\" }'","title":"3. Start ReMe Service and Init the task memory pool"},{"location":"cookbook/bfcl/quickstart/#4-run-experiments-on-validation-set","text":"Run you can compare agent performance on the validation set with task memory ( use_memory=True ) and without task memory: # remember to change the configuration options, e.g., `data_path=./data/multiturn_data_base_val.jsonl` python run_bfcl.py After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates best@k metrics for different k values - Generates a summary table showing performance comparisons - Saves results to experiment_summary.csv","title":"4. Run Experiments on Validation Set"},{"location":"cookbook/frozenlake/quickstart/","text":"This guide helps you quickly set up and run FrozenLake experiments with ReMe integration. The FrozenLake experiment demonstrates how task memory can improve an agent's performance in a navigation task. Environment Setup 1. Clone the Repository git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/frozenlake 2. FrozenLake Environment Setup Install Gymnasium for FrozenLake environment: pip install gymnasium This will install: - gymnasium - for the FrozenLake environment - ray - for parallel execution - openai - for LLM API access - other dependencies 3. Start ReMe Service If you haven't installed ReMe yet, follow these steps: # Go back to the project root cd ../.. # Create a virtual environment (optional) conda create -p ./reme-env python == 3 .10 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Add your api key for agent: export OPENAI_API_KEY = \"xxx\" export OPENAI_BASE_URL = \"xxx\" Run Experiments 1. Quick Test: Performance Evaluation Only (Default) Run the main experiment script to test agent performance using existing memory: cd cookbook/frozenlake python run_frozenlake.py What this does: - Tests the agent on randomly generated FrozenLake maps - Uses the default memory library ( frozenlake_no_slippery ) - Evaluates performance with multiple runs for statistical significance - Results are automatically saved to ./exp_result/ directory 2. Advanced: Training + Testing (Memory Generation) To create new memories through training and then test performance: You can modify the experiment parameters directly in the run_frozenlake.py file. The main parameters are in the main() function: def main (): experiment_name = \"frozenlake_no_slippery\" # Name of the experiment max_workers = 4 # Number of parallel workers training_runs = 4 # Runs per training map num_training_maps = 50 # Number of maps for training test_runs = 1 # Runs per test configuration num_test_maps = 100 # Number of test maps is_slippery = False # Enable slippery mode Key parameters to consider: - experiment_name : Used as the workspace ID for task memory - is_slippery : When True, agent movement becomes stochastic (harder) - max_workers : Increase for faster execution on multi-core systems 3. View Experiment Results After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates success rates and performance metrics - Generates a summary table showing performance comparisons - Analyzes the effect of task memory on performance - Saves results to frozenlake_summary.csv Understanding the Implementation Key Components FrozenLakeReactAgent ( frozenlake_react_agent.py ) Implements a ReAct agent that interacts with the FrozenLake environment Handles task memory retrieval and storage Uses LLM (via OpenAI API) for decision making Experiment Runner ( run_frozenlake.py ) Manages the overall experiment flow Handles training and testing phases Uses Ray for parallel execution Map Manager ( map_manager.py ) Generates and manages test maps Ensures consistent evaluation across experiments Statistics Analyzer ( run_exp_statistic.py ) Processes experiment results Calculates performance metrics Generates comparative analysis Output Files ./exp_result/*_training.jsonl : Results from training phase ./exp_result/*_test_no_memory.jsonl : Test results without task memory ./exp_result/*_test_with_memory.jsonl : Test results with task memory ./exp_result/frozenlake_summary.csv : Statistical summary Task Memory Mechanism The task memory system works as follows: Memory Creation : During training, successful trajectories are sent to the ReMe service Memory Retrieval : During testing, the agent queries relevant memories based on the current map Memory Application : The agent uses retrieved memories to guide its decision-making The experiment demonstrates how task memory can significantly improve performance, especially in challenging environments like the slippery FrozenLake.","title":"FrozenLake"},{"location":"cookbook/frozenlake/quickstart/#environment-setup","text":"","title":"Environment Setup"},{"location":"cookbook/frozenlake/quickstart/#1-clone-the-repository","text":"git clone https://github.com/modelscope/ReMe.git cd ReMe/cookbook/frozenlake","title":"1. Clone the Repository"},{"location":"cookbook/frozenlake/quickstart/#2-frozenlake-environment-setup","text":"Install Gymnasium for FrozenLake environment: pip install gymnasium This will install: - gymnasium - for the FrozenLake environment - ray - for parallel execution - openai - for LLM API access - other dependencies","title":"2. FrozenLake Environment Setup"},{"location":"cookbook/frozenlake/quickstart/#3-start-reme-service","text":"If you haven't installed ReMe yet, follow these steps: # Go back to the project root cd ../.. # Create a virtual environment (optional) conda create -p ./reme-env python == 3 .10 conda activate ./reme-env # Install ReMe pip install . Launch the ReMe service to enable memory library functionality: reme \\ backend = http \\ http.port = 8002 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local Add your api key for agent: export OPENAI_API_KEY = \"xxx\" export OPENAI_BASE_URL = \"xxx\"","title":"3. Start ReMe Service"},{"location":"cookbook/frozenlake/quickstart/#run-experiments","text":"","title":"Run Experiments"},{"location":"cookbook/frozenlake/quickstart/#1-quick-test-performance-evaluation-only-default","text":"Run the main experiment script to test agent performance using existing memory: cd cookbook/frozenlake python run_frozenlake.py What this does: - Tests the agent on randomly generated FrozenLake maps - Uses the default memory library ( frozenlake_no_slippery ) - Evaluates performance with multiple runs for statistical significance - Results are automatically saved to ./exp_result/ directory","title":"1. Quick Test: Performance Evaluation Only (Default)"},{"location":"cookbook/frozenlake/quickstart/#2-advanced-training-testing-memory-generation","text":"To create new memories through training and then test performance: You can modify the experiment parameters directly in the run_frozenlake.py file. The main parameters are in the main() function: def main (): experiment_name = \"frozenlake_no_slippery\" # Name of the experiment max_workers = 4 # Number of parallel workers training_runs = 4 # Runs per training map num_training_maps = 50 # Number of maps for training test_runs = 1 # Runs per test configuration num_test_maps = 100 # Number of test maps is_slippery = False # Enable slippery mode Key parameters to consider: - experiment_name : Used as the workspace ID for task memory - is_slippery : When True, agent movement becomes stochastic (harder) - max_workers : Increase for faster execution on multi-core systems","title":"2. Advanced: Training + Testing (Memory Generation)"},{"location":"cookbook/frozenlake/quickstart/#3-view-experiment-results","text":"After running experiments, analyze the statistical results: python run_exp_statistic.py What this script does: - Processes all result files in ./exp_result/ - Calculates success rates and performance metrics - Generates a summary table showing performance comparisons - Analyzes the effect of task memory on performance - Saves results to frozenlake_summary.csv","title":"3. View Experiment Results"},{"location":"cookbook/frozenlake/quickstart/#understanding-the-implementation","text":"","title":"Understanding the Implementation"},{"location":"cookbook/frozenlake/quickstart/#key-components","text":"FrozenLakeReactAgent ( frozenlake_react_agent.py ) Implements a ReAct agent that interacts with the FrozenLake environment Handles task memory retrieval and storage Uses LLM (via OpenAI API) for decision making Experiment Runner ( run_frozenlake.py ) Manages the overall experiment flow Handles training and testing phases Uses Ray for parallel execution Map Manager ( map_manager.py ) Generates and manages test maps Ensures consistent evaluation across experiments Statistics Analyzer ( run_exp_statistic.py ) Processes experiment results Calculates performance metrics Generates comparative analysis","title":"Key Components"},{"location":"cookbook/frozenlake/quickstart/#output-files","text":"./exp_result/*_training.jsonl : Results from training phase ./exp_result/*_test_no_memory.jsonl : Test results without task memory ./exp_result/*_test_with_memory.jsonl : Test results with task memory ./exp_result/frozenlake_summary.csv : Statistical summary","title":"Output Files"},{"location":"cookbook/frozenlake/quickstart/#task-memory-mechanism","text":"The task memory system works as follows: Memory Creation : During training, successful trajectories are sent to the ReMe service Memory Retrieval : During testing, the agent queries relevant memories based on the current map Memory Application : The agent uses retrieved memories to guide its decision-making The experiment demonstrates how task memory can significantly improve performance, especially in challenging environments like the slippery FrozenLake.","title":"Task Memory Mechanism"},{"location":"library/library/","text":"Clear Showing 0 of 0 items Loading memory library\u2026 \u26a0\ufe0f Failed to load memory library. Try again \u2190 Back to Libraries Libraries \ud83d\udd0e No results found. Try changing your search. \u2715 When to use Memory Metadata Author Created Memory ID Workspace Close /* \u2014\u2014 \u57fa\u4e8e shadcn/mkdocs \u4e3b\u9898\u53d8\u91cf\uff0c\u5c3d\u91cf\u5c11\u5199\u786c\u7f16\u7801\u989c\u8272 \u2014\u2014 */ :root { --ml-radius: .75rem; --ml-gap: 1rem; --ml-shadow: 0 6px 24px rgba(0,0,0,.08); } @media (prefers-color-scheme: dark) { /* \u4e3b\u9898\u4f1a\u5904\u7406\u8272\u677f\uff0c\u8fd9\u91cc\u4e0d\u989d\u5916\u8986\u76d6 */ } /* \u5bb9\u5668\u4e0e\u5361\u7247 */ .ml-prose-container { display: grid; gap: var(--ml-gap); } .ml-card { background: var(--background, #fff); color: var(--foreground, #0a0a0a); border: 1px solid var(--border, rgba(0,0,0,.08)); border-radius: var(--ml-radius); padding: 1rem; box-shadow: var(--shadow, 0 1px 0 rgba(0,0,0,.02)); } .ml-grid { display: grid; gap: var(--ml-gap); grid-template-columns: repeat(1, minmax(0,1fr)); } @media (min-width: 640px){ .ml-grid{ grid-template-columns: repeat(2, minmax(0,1fr)); } } @media (min-width: 1024px){ .ml-grid{ grid-template-columns: repeat(3, minmax(0,1fr)); } } .ml-card-item{ background: var(--card, var(--background, #fff)); border: 1px solid var(--border, rgba(0,0,0,.08)); border-radius: var(--ml-radius); padding: 1rem; transition: transform .18s ease, box-shadow .18s ease, border-color .18s ease; cursor: pointer; } .ml-card-item:hover{ transform: translateY(-2px); box-shadow: var(--ml-shadow); border-color: var(--primary, #3b82f6); } .ml-card-head{ display:flex; align-items:flex-start; justify-content:space-between; gap:.75rem; margin-bottom:.5rem; } .ml-card-title{ font-weight: 650; font-size: 1rem; } .ml-card-sub{ font-size: .85rem; opacity: .7; } .ml-card-sample{ margin-top:.5rem; font-size:.92rem; line-height:1.5; opacity:.9; display:-webkit-box; -webkit-line-clamp:3; -webkit-box-orient:vertical; overflow:hidden; } .ml-card-foot{ display:flex; justify-content:space-between; align-items:center; border-top:1px solid var(--border, rgba(0,0,0,.08)); padding-top:.5rem; margin-top:.75rem; font-size:.85rem; opacity:.8; } /* \u5de5\u5177\u6761\u4e0e\u8f93\u5165 */ .ml-toolbar{ display:flex; gap:.75rem; align-items:center; justify-content:space-between; flex-wrap:wrap; } .ml-input-wrap{ position:relative; flex:1; min-width: 260px; } .ml-input-wrap input{ width:100%; padding:.6rem .9rem .6rem 2.2rem; border-radius:.6rem; border:1px solid var(--border, rgba(0,0,0,.12)); background: var(--muted, rgba(0,0,0,.02)); color: var(--foreground, #0a0a0a); outline:none; } .ml-input-wrap input:focus{ border-color: var(--primary, #3b82f6); box-shadow: 0 0 0 3px color-mix(in srgb, var(--primary, #3b82f6) 22%, transparent); background: var(--background, #fff); } .ml-icon{ position:absolute; left:.6rem; top:50%; transform:translateY(-50%); width:1.1rem; height:1.1rem; opacity:.6; } .ml-btn{ border:1px solid var(--border, rgba(0,0,0,.12)); background: var(--accent, var(--background, #fff)); color: var(--foreground, #0a0a0a); padding:.55rem .9rem; border-radius:.55rem; cursor:pointer; } .ml-btn.secondary{ background: var(--muted, rgba(0,0,0,.03)); } .ml-btn:hover{ border-color: var(--primary, #3b82f6); } /* \u7edf\u8ba1/\u9762\u5305\u5c51 */ .ml-stats{ margin-top:.5rem; font-size:.9rem; opacity:.8; } .ml-crumb{ display:flex; align-items:center; gap:.75rem; } .ml-link{ background:none; border:none; color: var(--primary, #3b82f6); cursor:pointer; padding:.25rem .5rem; border-radius:.4rem; } .ml-link:hover{ text-decoration: underline; } .ml-crumb-title{ font-weight:600; opacity:.8; } /* \u72b6\u6001\u533a */ .ml-loading, .ml-error, .ml-empty{ display:grid; justify-items:center; gap:.5rem; padding:3rem 1rem; } .ml-spinner{ width:38px; height:38px; border-radius:999px; border:3px solid color-mix(in srgb, var(--foreground,#000) 12%, transparent); border-top-color: var(--primary,#3b82f6); animation: ml-spin 1s linear infinite; } @keyframes ml-spin{ to{ transform: rotate(360deg); } } .ml-muted{ opacity:.7; } .ml-error-icon{ font-size:1.4rem; } /* \u6807\u7b7e/\u5757 */ .ml-chip{ display:inline-block; padding:.25rem .55rem; border-radius:999px; font-size:.78rem; background: color-mix(in srgb, var(--primary,#3b82f6) 12%, transparent); color: var(--primary,#3b82f6); } .ml-chip.success{ background: color-mix(in srgb, #16a34a 14%, transparent); color: #16a34a; } .ml-code{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", monospace; background: var(--muted, rgba(0,0,0,.04)); border:1px solid var(--border, rgba(0,0,0,.08)); padding:.75rem; border-radius:.6rem; white-space:pre-wrap; } .ml-note{ background: color-mix(in srgb, #f59e0b 9%, transparent); border:1px solid color-mix(in srgb, #f59e0b 28%, transparent); padding:.75rem; border-radius:.6rem; } /* \u5143\u6570\u636e */ .ml-meta{ display:grid; grid-template-columns: repeat(1, minmax(0,1fr)); gap:.5rem; } @media (min-width: 640px){ .ml-meta{ grid-template-columns: repeat(2, minmax(0,1fr)); } } .ml-meta > div{ display:flex; justify-content:space-between; align-items:center; padding:.5rem .75rem; border:1px dashed var(--border, rgba(0,0,0,.12)); border-radius:.5rem; background: var(--background, #fff); } .ml-meta span{ opacity:.7; } .mono{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace; } /* \u5f39\u7a97 */ .ml-modal{ padding:0; border:none; background: transparent; } .ml-modal[open]{ display:grid; align-items:center; justify-items:center; } .ml-modal::backdrop{ background: rgba(0,0,0,.45); } .ml-modal-card{ width:min(100%, 960px); max-height: 85vh; overflow:auto; background: var(--background, #fff); color: var(--foreground,#0a0a0a); border:1px solid var(--border, rgba(0,0,0,.1)); border-radius: var(--ml-radius); padding: 1rem; box-shadow: var(--ml-shadow); } .ml-modal-header{ display:flex; justify-content:space-between; align-items:center; gap:.75rem; margin-bottom:.5rem; } .ml-close{ border:none; background:none; font-size:1.1rem; cursor:pointer; opacity:.6; } .ml-close:hover{ opacity:1; } .ml-modal-section{ display:grid; gap:.35rem; margin-top:.75rem; } .ml-section-title{ font-weight:650; opacity:.85; } .ml-modal-footer{ display:flex; justify-content:flex-end; margin-top:1rem; } (() => { // \u2014\u2014 State let ALL = []; let GROUPED = {}; let VIEW = \"libraries\"; // \"libraries\" | \"memories\" let CURR = null; // \u2014\u2014 DOM const $ = (id) => document.getElementById(id); const elLoading = $(\"ml-loading\"); const elError = $(\"ml-error\"); const elRetry = $(\"ml-retry\"); const elLibraries = $(\"ml-libraries\"); const elMemories = $(\"ml-memories\"); const elEmpty = $(\"ml-empty\"); const elSearch = $(\"ml-search\"); const elClear = $(\"ml-clear\"); const elStats = $(\"ml-stats\"); const elCount = $(\"ml-count\"); const elTotal = $(\"ml-total\"); const elType = $(\"ml-type\"); const elCrumb = $(\"ml-crumb\"); const elBack = $(\"ml-back\"); const elCrumbTitle = $(\"ml-crumb-title\"); const dlg = $(\"ml-modal\"); const mLib = $(\"ml-modal-lib\"); const mScore = $(\"ml-modal-score\"); const mWhen = $(\"ml-modal-when\"); const mCont = $(\"ml-modal-content\"); const mAuth = $(\"ml-modal-author\"); const mCreated = $(\"ml-modal-created\"); const mId = $(\"ml-modal-id\"); const mWs = $(\"ml-modal-ws\"); // \u2014\u2014 Config\uff1aJSONL \u6587\u4ef6\u4f4d\u4e8e\u672c\u9875\u540c\u7ea7\u76ee\u5f55\uff08docs/library/\uff09 const BASE = \"..\"; const FILES = [ \"appworld.jsonl\", \"bfcl_v3.jsonl\", // \u9700\u8981\u7684\u8bdd\u5728\u8fd9\u91cc\u7ee7\u7eed\u6dfb\u52a0\u6587\u4ef6\u540d ]; // \u2014\u2014 Utils function show(el){ el.hidden = false; } function hide(el){ el.hidden = true; } function setLoading(on){ on ? (show(elLoading), [elError, elLibraries, elMemories, elEmpty, elStats, elCrumb].forEach(hide)) : hide(elLoading); } function setError(on){ on ? (show(elError), [elLoading].forEach(hide)) : hide(elError); } function clampTxt(s, n){ if(!s) return \"\"; return s.length<=n? s : s.slice(0,n)+\"\u2026\"; } const fmtDate = (t)=> t ? new Date(t).toLocaleDateString() : \"Unknown\"; function debounce(fn, ms=250){ let t; return (...a)=>{ clearTimeout(t); t=setTimeout(()=>fn(...a), ms); }; } // \u2014\u2014 Data Loading async function loadAll(){ setLoading(true); setError(false); try{ const arr = await Promise.all(FILES.map(async f=>{ try{ const res = await fetch(`${BASE}/${f}`); if(!res.ok) return []; const txt = await res.text(); return txt.split(\"\\n\").filter(l=>l.trim()).map(line=>{ try{ const obj = JSON.parse(line); obj._library = f.replace(/\\.jsonl$/,\"\"); return obj; }catch{ return null; } }).filter(Boolean); }catch{ return []; } })); ALL = arr.flat(); if(!ALL.length) throw new Error(\"no data\"); GROUPED = ALL.reduce((acc,m)=>{ (acc[m._library] ||= []).push(m); return acc; }, {}); renderLibraries(); }catch(e){ setError(true); }finally{ setLoading(false); } } // \u2014\u2014 Render function renderLibraries(list){ VIEW = \"libraries\"; CURR = null; hide(elMemories); hide(elEmpty); show(elLibraries); hide(elCrumb); elCrumbTitle.textContent = \"Libraries\"; elType.textContent = \"libraries\"; const libs = list ?? Object.keys(GROUPED); if(!libs.length){ hide(elLibraries); show(elEmpty); hide(elStats); return; } elLibraries.innerHTML = libs.map(name=>{ const arr = GROUPED[name]; const sample = arr[0] || {}; const sampleText = sample.when_to_use || sample.content || \"No description available\"; const author = sample.author || \"Unknown\"; return ` <div class=\"ml-card-item\" data-lib=\"${name}\"> <div class=\"ml-card-head\"> <div> <div class=\"ml-card-title\">${name}</div> <div class=\"ml-card-sub\">${arr.length} memories</div> </div> <div class=\"ml-chip\">DB</div> </div> <div class=\"ml-card-sample\">${clampTxt(sampleText, 180)}</div> <div class=\"ml-card-foot\"> <span>\ud83d\udc64 ${author}</span> <span>View \u2192</span> </div> </div> `; }).join(\"\"); bindLibraryClicks(); show(elStats); $(\"ml-count\").textContent = libs.length; $(\"ml-total\").textContent = Object.keys(GROUPED).length; } function renderMemories(memList){ VIEW = \"memories\"; hide(elLibraries); hide(elEmpty); show(elMemories); show(elCrumb); elType.textContent = \"memories\"; elCrumbTitle.textContent = `Exploring ${CURR}`; if(!memList?.length){ hide(elMemories); show(elEmpty); hide(elStats); return; } elMemories.innerHTML = memList.map((m,idx)=>` <div class=\"ml-card-item\" data-idx=\"${idx}\"> <div class=\"ml-card-head\"> <div class=\"ml-chip\">${m._library}</div> ${m.score ? `<div class=\"ml-chip success\">Score: ${m.score}</div>` : \"\"} </div> <div class=\"ml-card-sample\"><b>When to use:</b> ${clampTxt(m.when_to_use || \"No specific guidance provided\", 140)}</div> <div class=\"ml-card-foot\"> <span>\ud83d\udc64 ${m.author || \"Unknown\"}</span> <span>Details \u2192</span> </div> </div> `).join(\"\"); // \u7ed1\u5b9a\u5361\u7247 \u2192 \u6253\u5f00\u5f39\u7a97 [...elMemories.querySelectorAll(\".ml-card-item\")].forEach(card=>{ card.addEventListener(\"click\", ()=>{ const idx = Number(card.getAttribute(\"data-idx\")); const m = GROUPED[CURR][idx]; mLib.textContent = m._library; const hasScore = \"score\" in m && m.score !== null && m.score !== undefined; if(hasScore){ mScore.textContent = `Score: ${m.score}`; mScore.hidden = false; } else { mScore.hidden = true; } mWhen.textContent = m.when_to_use || \"No specific guidance provided\"; mCont.textContent = m.content || \"No content available\"; mAuth.textContent = m.author || \"Unknown\"; mCreated.textContent = fmtDate(m.time_created); mId.textContent = m.memory_id || \"N/A\"; mWs.textContent = m.workspace_id || \"N/A\"; dlg.showModal(); }); }); show(elStats); $(\"ml-count\").textContent = memList.length; $(\"ml-total\").textContent = memList.length; } function bindLibraryClicks(){ [...elLibraries.querySelectorAll(\".ml-card-item\")].forEach(card=>{ card.addEventListener(\"click\", ()=>{ CURR = card.getAttribute(\"data-lib\"); renderMemories(GROUPED[CURR]); }); }); } // \u2014\u2014 Search function handleSearch(){ const q = elSearch.value.trim().toLowerCase(); if(!q){ if(VIEW===\"libraries\") renderLibraries(); else renderMemories(GROUPED[CURR]); return; } if(VIEW===\"libraries\"){ const libs = Object.keys(GROUPED).filter(name=>{ const arr = GROUPED[name]; return name.toLowerCase().includes(q) || arr.some(m => (m.when_to_use||\"\").toLowerCase().includes(q) || (m.content||\"\").toLowerCase().includes(q) || (m.author||\"\").toLowerCase().includes(q)); }); renderLibraries(libs); }else{ const arr = GROUPED[CURR] || []; const filtered = arr.filter(m => (m.when_to_use||\"\").toLowerCase().includes(q) || (m.content||\"\").toLowerCase().includes(q) || (m.author||\"\").toLowerCase().includes(q) ); renderMemories(filtered); } } // \u2014\u2014 Events elRetry?.addEventListener(\"click\", loadAll); elBack?.addEventListener(\"click\", ()=> renderLibraries()); elSearch?.addEventListener(\"input\", debounce(handleSearch, 250)); elClear?.addEventListener(\"click\", ()=>{ elSearch.value = \"\"; handleSearch(); }); // \u2014\u2014 Init document.addEventListener(\"DOMContentLoaded\", loadAll); })();","title":"Library Home"},{"location":"personal_memory/personal_memory/","text":"Configuration Logic ReMe's personal memory system consists of two main components: retrieval and summarization. The configuration for these components is defined in the default.yaml file. Retrieval Configuration ( retrieve_personal_memory ) retrieve_personal_memory : flow_content : set_query_op >> (extract_time_op | (retrieve_memory_op >> semantic_rank_op)) >> fuse_rerank_op This flow performs the following operations: 1. set_query_op : Prepares the query for memory retrieval 2. Parallel paths: - extract_time_op : Extracts time-related information from the query - retrieve_memory_op >> semantic_rank_op : Retrieves memories and ranks them semantically 3. fuse_rerank_op : Combines and reranks the results for final output Summarization Configuration ( summary_personal_memory ) summary_personal_memory : flow_content : info_filter_op >> (get_observation_op | get_observation_with_time_op | load_today_memory_op) >> contra_repeat_op >> update_vector_store_op This flow performs the following operations: 1. info_filter_op : Filters incoming information to extract relevant personal details 2. Parallel paths for observation extraction: - get_observation_op : Extracts general observations - get_observation_with_time_op : Extracts observations with time context - load_today_memory_op : Loads memories from the current day 3. contra_repeat_op : Removes contradictions and repetitions 4. update_vector_store_op : Stores the processed memories in the vector database Basic Usage The following example demonstrates how to use personal memory in MemoryScope: 1. Setup import asyncio import json import aiohttp # API base URL (default is http://0.0.0.0:8002) base_url = \"http://0.0.0.0:8002\" workspace_id = \"personal_memory_demo\" 2. Clear Existing Memories async with aiohttp . ClientSession () as session : # Delete existing workspace memories async with session . post ( f \" { base_url } /vector_store\" , json = { \"action\" : \"delete\" , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () 3. Create Conversation with Personal Information # Example conversation with personal details messages = [ { \"role\" : \"user\" , \"content\" : \"My name is John Smith, I'm 28 years old\" }, { \"role\" : \"assistant\" , \"content\" : \"Nice to meet you, John!\" }, { \"role\" : \"user\" , \"content\" : \"I'm a software engineer working with Python\" }, { \"role\" : \"assistant\" , \"content\" : \"I see, you're a Python engineer.\" }, # Additional conversation messages... ] 4. Summarize Personal Memories async with session . post ( f \" { base_url } /summary_personal_memory\" , json = { \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ], \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () 5. Retrieve Personal Memories # Example queries to retrieve personal information queries = [ \"What's my name and age?\" , \"What do I do for work?\" , \"What are my hobbies?\" ] for query in queries : async with session . post ( f \" { base_url } /retrieve_personal_memory\" , json = { \"query\" : query , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () print ( f \"Query: { query } \" ) print ( f \"Answer: { result . get ( 'answer' , '' ) } \" ) Complete Example For a complete working example, refer to /cookbook/simple_demo/use_personal_memory_demo.py in the ReMe repository.","title":"Overview"},{"location":"personal_memory/personal_memory/#configuration-logic","text":"ReMe's personal memory system consists of two main components: retrieval and summarization. The configuration for these components is defined in the default.yaml file.","title":"Configuration Logic"},{"location":"personal_memory/personal_memory/#retrieval-configuration-retrieve_personal_memory","text":"retrieve_personal_memory : flow_content : set_query_op >> (extract_time_op | (retrieve_memory_op >> semantic_rank_op)) >> fuse_rerank_op This flow performs the following operations: 1. set_query_op : Prepares the query for memory retrieval 2. Parallel paths: - extract_time_op : Extracts time-related information from the query - retrieve_memory_op >> semantic_rank_op : Retrieves memories and ranks them semantically 3. fuse_rerank_op : Combines and reranks the results for final output","title":"Retrieval Configuration (retrieve_personal_memory)"},{"location":"personal_memory/personal_memory/#summarization-configuration-summary_personal_memory","text":"summary_personal_memory : flow_content : info_filter_op >> (get_observation_op | get_observation_with_time_op | load_today_memory_op) >> contra_repeat_op >> update_vector_store_op This flow performs the following operations: 1. info_filter_op : Filters incoming information to extract relevant personal details 2. Parallel paths for observation extraction: - get_observation_op : Extracts general observations - get_observation_with_time_op : Extracts observations with time context - load_today_memory_op : Loads memories from the current day 3. contra_repeat_op : Removes contradictions and repetitions 4. update_vector_store_op : Stores the processed memories in the vector database","title":"Summarization Configuration (summary_personal_memory)"},{"location":"personal_memory/personal_memory/#basic-usage","text":"The following example demonstrates how to use personal memory in MemoryScope:","title":"Basic Usage"},{"location":"personal_memory/personal_memory/#1-setup","text":"import asyncio import json import aiohttp # API base URL (default is http://0.0.0.0:8002) base_url = \"http://0.0.0.0:8002\" workspace_id = \"personal_memory_demo\"","title":"1. Setup"},{"location":"personal_memory/personal_memory/#2-clear-existing-memories","text":"async with aiohttp . ClientSession () as session : # Delete existing workspace memories async with session . post ( f \" { base_url } /vector_store\" , json = { \"action\" : \"delete\" , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json ()","title":"2. Clear Existing Memories"},{"location":"personal_memory/personal_memory/#3-create-conversation-with-personal-information","text":"# Example conversation with personal details messages = [ { \"role\" : \"user\" , \"content\" : \"My name is John Smith, I'm 28 years old\" }, { \"role\" : \"assistant\" , \"content\" : \"Nice to meet you, John!\" }, { \"role\" : \"user\" , \"content\" : \"I'm a software engineer working with Python\" }, { \"role\" : \"assistant\" , \"content\" : \"I see, you're a Python engineer.\" }, # Additional conversation messages... ]","title":"3. Create Conversation with Personal Information"},{"location":"personal_memory/personal_memory/#4-summarize-personal-memories","text":"async with session . post ( f \" { base_url } /summary_personal_memory\" , json = { \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ], \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json ()","title":"4. Summarize Personal Memories"},{"location":"personal_memory/personal_memory/#5-retrieve-personal-memories","text":"# Example queries to retrieve personal information queries = [ \"What's my name and age?\" , \"What do I do for work?\" , \"What are my hobbies?\" ] for query in queries : async with session . post ( f \" { base_url } /retrieve_personal_memory\" , json = { \"query\" : query , \"workspace_id\" : workspace_id , }, headers = { \"Content-Type\" : \"application/json\" } ) as response : result = await response . json () print ( f \"Query: { query } \" ) print ( f \"Answer: { result . get ( 'answer' , '' ) } \" )","title":"5. Retrieve Personal Memories"},{"location":"personal_memory/personal_memory/#complete-example","text":"For a complete working example, refer to /cookbook/simple_demo/use_personal_memory_demo.py in the ReMe repository.","title":"Complete Example"},{"location":"personal_memory/personal_retrieve_ops/","text":"SetQueryOp Functionality SetQueryOp prepares the query for memory retrieval by setting the query and its associated timestamp into the context. It's the first operation in the personal memory retrieval flow. Parameters op.set_query_op.params.timestamp : (Optional) Integer timestamp to use instead of the current time. If not provided, the current timestamp will be used. Implementation Details The operation: 1. Takes the query from the context (which is guaranteed to exist as a flow input requirement) 2. Sets a timestamp (either current time or from parameters) 3. Stores the query and timestamp as a tuple in the context for downstream operations ExtractTimeOp Functionality ExtractTimeOp identifies and extracts time-related information from the query. It uses an LLM to analyze the query text and determine any temporal references or constraints. Parameters op.extract_time_op.params.language : Language for time extraction (defaults to \"en\") Implementation Details The operation: 1. Checks if the query contains datetime keywords 2. If time-related words are found, it prepares a prompt for the LLM with: - System instructions - Few-shot examples - The user's query and current time 3. Parses the LLM response to extract time information (year, month, day, etc.) 4. Stores the extracted time dictionary in the context for downstream operations RetrieveMemoryOp Functionality RetrieveMemoryOp retrieves memories from the vector store based on the query. It extends the RecallVectorStoreOp class to provide memory retrieval functionality. Parameters op.retrieve_memory_op.params.recall_key : Key in the context to use as the query (default: \"query\") op.retrieve_memory_op.params.top_k : Maximum number of memories to retrieve (default: 3) op.retrieve_memory_op.params.threshold_score : (Optional) Minimum similarity score for memories (filters out memories below this threshold) Implementation Details The operation: 1. Retrieves the query from the context 2. Searches the vector store for relevant memories based on the query 3. Removes duplicate memories 4. Filters memories by threshold score if specified 5. Stores the retrieved memories in the context for downstream operations SemanticRankOp Functionality SemanticRankOp ranks memories based on their semantic relevance to the query using an LLM. This improves the quality of retrieved memories by considering deeper semantic relationships beyond vector similarity. Parameters op.semantic_rank_op.params.enable_ranker : Whether to enable semantic ranking (default: true) op.semantic_rank_op.params.output_memory_max_count : Maximum number of memories to output (default: 10) Implementation Details The operation: 1. Retrieves the memory list from the context 2. If ranking is enabled and there are more memories than the output limit: - Removes duplicates based on content - Formats memories for LLM ranking - Asks the LLM to rank memories by relevance on a scale of 0.0 to 1.0 - Parses the ranking results and applies scores to memories 3. Sorts memories by score 4. Stores the ranked memories in the context for downstream operations FuseRerankOp Functionality FuseRerankOp performs the final reranking of memories by combining multiple factors: semantic scores, memory types, and temporal relevance. It also formats the final output. Parameters op.fuse_rerank_op.params.fuse_score_threshold : Minimum score threshold for memories (default: 0.1) op.fuse_rerank_op.params.fuse_ratio_dict : Dictionary of memory type to score multiplier ratios (default: {\"conversation\": 0.5, \"observation\": 1, \"obs_customized\": 1.2, \"insight\": 2.0}) op.fuse_rerank_op.params.fuse_time_ratio : Score multiplier for time-relevant memories (default: 2.0) op.fuse_rerank_op.params.output_memory_max_count : Maximum number of memories to output (default: 5) Implementation Details The operation: 1. Retrieves extracted time information and memory list from the context 2. For each memory: - Checks if the memory score is above the threshold - Applies a type-based adjustment factor based on the memory type - Determines time relevance by matching memory time metadata with extracted time - Calculates the final score by multiplying the original score by type and time factors 3. Sorts memories by the reranked scores 4. Selects the top-K memories based on the output limit 5. Formats memories for output with timestamps if available 6. Stores both the formatted output and the memory list in the context PrintMemoryOp Functionality PrintMemoryOp formats the retrieved memories for display to the user. It provides a clean, structured representation of the memory content. Parameters No specific parameters for this operation. Implementation Details The operation: 1. Retrieves the memory list from the context 2. Formats each memory with: - Memory index - When to use information - Content - Additional metadata (if available) 3. Joins the formatted memories into a single string 4. Stores the formatted string in the context as the response answer ReadMessageOp Functionality ReadMessageOp fetches unmemorized chat messages from the context. This is useful for retrieving recent conversations that haven't been processed into memories yet. Parameters op.read_message_op.params.contextual_msg_max_count : Maximum number of contextual messages to retrieve (default: 10) Implementation Details The operation: 1. Retrieves chat messages from the context 2. Filters for messages that: - Are not marked as memorized - Contain the target name 3. Flattens the messages into a single list 4. Sorts messages by creation time if available 5. Stores the filtered messages back in the context","title":"Retrieve Ops"},{"location":"personal_memory/personal_retrieve_ops/#setqueryop","text":"","title":"SetQueryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality","text":"SetQueryOp prepares the query for memory retrieval by setting the query and its associated timestamp into the context. It's the first operation in the personal memory retrieval flow.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters","text":"op.set_query_op.params.timestamp : (Optional) Integer timestamp to use instead of the current time. If not provided, the current timestamp will be used.","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details","text":"The operation: 1. Takes the query from the context (which is guaranteed to exist as a flow input requirement) 2. Sets a timestamp (either current time or from parameters) 3. Stores the query and timestamp as a tuple in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#extracttimeop","text":"","title":"ExtractTimeOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_1","text":"ExtractTimeOp identifies and extracts time-related information from the query. It uses an LLM to analyze the query text and determine any temporal references or constraints.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_1","text":"op.extract_time_op.params.language : Language for time extraction (defaults to \"en\")","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_1","text":"The operation: 1. Checks if the query contains datetime keywords 2. If time-related words are found, it prepares a prompt for the LLM with: - System instructions - Few-shot examples - The user's query and current time 3. Parses the LLM response to extract time information (year, month, day, etc.) 4. Stores the extracted time dictionary in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#retrievememoryop","text":"","title":"RetrieveMemoryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_2","text":"RetrieveMemoryOp retrieves memories from the vector store based on the query. It extends the RecallVectorStoreOp class to provide memory retrieval functionality.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_2","text":"op.retrieve_memory_op.params.recall_key : Key in the context to use as the query (default: \"query\") op.retrieve_memory_op.params.top_k : Maximum number of memories to retrieve (default: 3) op.retrieve_memory_op.params.threshold_score : (Optional) Minimum similarity score for memories (filters out memories below this threshold)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_2","text":"The operation: 1. Retrieves the query from the context 2. Searches the vector store for relevant memories based on the query 3. Removes duplicate memories 4. Filters memories by threshold score if specified 5. Stores the retrieved memories in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#semanticrankop","text":"","title":"SemanticRankOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_3","text":"SemanticRankOp ranks memories based on their semantic relevance to the query using an LLM. This improves the quality of retrieved memories by considering deeper semantic relationships beyond vector similarity.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_3","text":"op.semantic_rank_op.params.enable_ranker : Whether to enable semantic ranking (default: true) op.semantic_rank_op.params.output_memory_max_count : Maximum number of memories to output (default: 10)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_3","text":"The operation: 1. Retrieves the memory list from the context 2. If ranking is enabled and there are more memories than the output limit: - Removes duplicates based on content - Formats memories for LLM ranking - Asks the LLM to rank memories by relevance on a scale of 0.0 to 1.0 - Parses the ranking results and applies scores to memories 3. Sorts memories by score 4. Stores the ranked memories in the context for downstream operations","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#fusererankop","text":"","title":"FuseRerankOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_4","text":"FuseRerankOp performs the final reranking of memories by combining multiple factors: semantic scores, memory types, and temporal relevance. It also formats the final output.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_4","text":"op.fuse_rerank_op.params.fuse_score_threshold : Minimum score threshold for memories (default: 0.1) op.fuse_rerank_op.params.fuse_ratio_dict : Dictionary of memory type to score multiplier ratios (default: {\"conversation\": 0.5, \"observation\": 1, \"obs_customized\": 1.2, \"insight\": 2.0}) op.fuse_rerank_op.params.fuse_time_ratio : Score multiplier for time-relevant memories (default: 2.0) op.fuse_rerank_op.params.output_memory_max_count : Maximum number of memories to output (default: 5)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_4","text":"The operation: 1. Retrieves extracted time information and memory list from the context 2. For each memory: - Checks if the memory score is above the threshold - Applies a type-based adjustment factor based on the memory type - Determines time relevance by matching memory time metadata with extracted time - Calculates the final score by multiplying the original score by type and time factors 3. Sorts memories by the reranked scores 4. Selects the top-K memories based on the output limit 5. Formats memories for output with timestamps if available 6. Stores both the formatted output and the memory list in the context","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#printmemoryop","text":"","title":"PrintMemoryOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_5","text":"PrintMemoryOp formats the retrieved memories for display to the user. It provides a clean, structured representation of the memory content.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_5","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_5","text":"The operation: 1. Retrieves the memory list from the context 2. Formats each memory with: - Memory index - When to use information - Content - Additional metadata (if available) 3. Joins the formatted memories into a single string 4. Stores the formatted string in the context as the response answer","title":"Implementation Details"},{"location":"personal_memory/personal_retrieve_ops/#readmessageop","text":"","title":"ReadMessageOp"},{"location":"personal_memory/personal_retrieve_ops/#functionality_6","text":"ReadMessageOp fetches unmemorized chat messages from the context. This is useful for retrieving recent conversations that haven't been processed into memories yet.","title":"Functionality"},{"location":"personal_memory/personal_retrieve_ops/#parameters_6","text":"op.read_message_op.params.contextual_msg_max_count : Maximum number of contextual messages to retrieve (default: 10)","title":"Parameters"},{"location":"personal_memory/personal_retrieve_ops/#implementation-details_6","text":"The operation: 1. Retrieves chat messages from the context 2. Filters for messages that: - Are not marked as memorized - Contain the target name 3. Flattens the messages into a single list 4. Sorts messages by creation time if available 5. Stores the filtered messages back in the context","title":"Implementation Details"},{"location":"personal_memory/personal_summary_ops/","text":"InfoFilterOp Purpose Filters messages based on information content scores, retaining only those that include significant information about the user. Parameters op.info_filter_op.params.preserved_scores : Comma-separated string of scores to preserve (default: \"2,3\") op.info_filter_op.params.info_filter_msg_max_size : Maximum size of messages to process (default: 200) Description This operation analyzes messages to determine which ones contain valuable personal information. It uses an LLM to score each message on a scale of 0-3: - 0: No user information - 1: Hypothetical or fictional content - 2: General or time-sensitive information - 3: Clear, important information or explicitly requested records Only messages with scores specified in preserved_scores are retained. Messages are also filtered to exclude those already memorized and to only include messages from the user. GetObservationOp Purpose Extracts general observations about the user from messages that don't contain time-related information. Parameters No specific parameters for this operation. Description This operation processes messages that don't contain time-related keywords. It uses an LLM to extract meaningful observations about the user from these messages. Each observation includes: - Content: The actual observation text - Keywords: Tags that indicate when this observation might be relevant - Source message: The original message that led to this observation The operation creates PersonalMemory objects with observation type \"personal_info\" for each extracted observation. GetObservationWithTimeOp Purpose Extracts observations with time context from messages that contain time-related information. Parameters No specific parameters for this operation. Description This operation is the counterpart to GetObservationOp but focuses specifically on messages containing time-related keywords. It extracts observations while preserving the time context, which is important for memories related to schedules, appointments, or time-specific preferences. The operation creates PersonalMemory objects with observation type \"personal_info_with_time\" for each extracted observation, including the time information in the metadata. LoadTodayMemoryOp Purpose Loads memories created today from the vector store to prevent duplication and enable updating of recent memories. Parameters op.load_today_memory_op.params.top_k : Maximum number of memories to retrieve (default: 50) Description This operation retrieves memories created on the current day using vector store search with date filtering. It converts vector nodes to memory objects and makes them available for deduplication in subsequent operations. This helps ensure that new observations don't create redundant memories for information already captured earlier in the day. ContraRepeatOp Purpose Identifies and removes contradictory or repetitive information from the collected memories. Parameters op.contra_repeat_op.params.contra_repeat_max_count : Maximum number of memories to process (default: 50) op.contra_repeat_op.params.enable_contra_repeat : Whether to enable contradiction/repetition checking (default: true) Description This operation analyzes the combined memories from previous operations (observation_memories, observation_memories_with_time, today_memories) to identify contradictions or redundancies. It uses an LLM to evaluate each memory and mark it as: - \"Contradiction\": Contradicts other memories - \"Contained\": Redundant as the information is already contained in other memories - \"None\": Unique and should be kept Memories marked as contradictory or contained are filtered out, and their IDs are tracked for deletion from the vector store. LongContraRepeatOp Purpose Performs more sophisticated contradiction and redundancy analysis for longer-term memory management. Parameters op.long_contra_repeat_op.params.long_contra_repeat_max_count : Maximum number of memories to process (default: 50) op.long_contra_repeat_op.params.enable_long_contra_repeat : Whether to enable this operation (default: true) Description This operation extends the basic contradiction analysis of ContraRepeatOp with the ability to resolve conflicts by modifying contradictory memories rather than simply removing them. It's particularly useful for managing long-term personal memories where information might evolve over time. For contradictory memories, it can either: - Modify the content to resolve the contradiction - Remove the memory if it's completely invalidated - Keep the most accurate/recent information UpdateInsightOp Purpose Updates existing insight values based on new observations. Parameters op.update_insight_op.params.update_insight_threshold : Minimum relevance score threshold (default: 0.3) op.update_insight_op.params.update_insight_max_count : Maximum number of insights to update (default: 5) Description This operation integrates new observations into existing insights about the user. It: 1. Scores insight memories based on relevance to new observations 2. Selects the top insights that meet the relevance threshold 3. Updates each selected insight using an LLM to incorporate the new information 4. Creates updated insight memories with the original ID but new content This helps maintain accurate and up-to-date insights as new information about the user becomes available. GetReflectionSubjectOp Purpose Generates reflection subjects (topics) from personal memories for insight extraction. Parameters op.get_reflection_subject_op.params.reflect_obs_cnt_threshold : Minimum number of memories required for reflection (default: 10) op.get_reflection_subject_op.params.reflect_num_questions : Maximum number of new subjects to generate (default: 3) Description This operation analyzes a collection of personal memories to identify potential topics for reflection and insight generation. It: 1. Checks if there are sufficient memories for meaningful reflection 2. Extracts existing insight subjects to avoid duplication 3. Uses an LLM to generate new reflection subjects based on memory content 4. Creates insight memory objects for these new subjects The generated subjects serve as focal points for organizing and synthesizing personal information about the user. UpdateVectorStoreOp Purpose Stores the processed memories in the vector database and removes deleted memories. Parameters No specific parameters for this operation. Description This operation is the final step in the personal memory summarization flow. It: 1. Deletes memories that were marked for removal (contradictory or redundant) 2. Inserts new or updated memories into the vector store 3. Records the number of deleted and inserted memories This ensures that the vector store remains up-to-date with the latest processed memories.","title":"Summary Ops"},{"location":"personal_memory/personal_summary_ops/#infofilterop","text":"","title":"InfoFilterOp"},{"location":"personal_memory/personal_summary_ops/#purpose","text":"Filters messages based on information content scores, retaining only those that include significant information about the user.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters","text":"op.info_filter_op.params.preserved_scores : Comma-separated string of scores to preserve (default: \"2,3\") op.info_filter_op.params.info_filter_msg_max_size : Maximum size of messages to process (default: 200)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description","text":"This operation analyzes messages to determine which ones contain valuable personal information. It uses an LLM to score each message on a scale of 0-3: - 0: No user information - 1: Hypothetical or fictional content - 2: General or time-sensitive information - 3: Clear, important information or explicitly requested records Only messages with scores specified in preserved_scores are retained. Messages are also filtered to exclude those already memorized and to only include messages from the user.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getobservationop","text":"","title":"GetObservationOp"},{"location":"personal_memory/personal_summary_ops/#purpose_1","text":"Extracts general observations about the user from messages that don't contain time-related information.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_1","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_1","text":"This operation processes messages that don't contain time-related keywords. It uses an LLM to extract meaningful observations about the user from these messages. Each observation includes: - Content: The actual observation text - Keywords: Tags that indicate when this observation might be relevant - Source message: The original message that led to this observation The operation creates PersonalMemory objects with observation type \"personal_info\" for each extracted observation.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getobservationwithtimeop","text":"","title":"GetObservationWithTimeOp"},{"location":"personal_memory/personal_summary_ops/#purpose_2","text":"Extracts observations with time context from messages that contain time-related information.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_2","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_2","text":"This operation is the counterpart to GetObservationOp but focuses specifically on messages containing time-related keywords. It extracts observations while preserving the time context, which is important for memories related to schedules, appointments, or time-specific preferences. The operation creates PersonalMemory objects with observation type \"personal_info_with_time\" for each extracted observation, including the time information in the metadata.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#loadtodaymemoryop","text":"","title":"LoadTodayMemoryOp"},{"location":"personal_memory/personal_summary_ops/#purpose_3","text":"Loads memories created today from the vector store to prevent duplication and enable updating of recent memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_3","text":"op.load_today_memory_op.params.top_k : Maximum number of memories to retrieve (default: 50)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_3","text":"This operation retrieves memories created on the current day using vector store search with date filtering. It converts vector nodes to memory objects and makes them available for deduplication in subsequent operations. This helps ensure that new observations don't create redundant memories for information already captured earlier in the day.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#contrarepeatop","text":"","title":"ContraRepeatOp"},{"location":"personal_memory/personal_summary_ops/#purpose_4","text":"Identifies and removes contradictory or repetitive information from the collected memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_4","text":"op.contra_repeat_op.params.contra_repeat_max_count : Maximum number of memories to process (default: 50) op.contra_repeat_op.params.enable_contra_repeat : Whether to enable contradiction/repetition checking (default: true)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_4","text":"This operation analyzes the combined memories from previous operations (observation_memories, observation_memories_with_time, today_memories) to identify contradictions or redundancies. It uses an LLM to evaluate each memory and mark it as: - \"Contradiction\": Contradicts other memories - \"Contained\": Redundant as the information is already contained in other memories - \"None\": Unique and should be kept Memories marked as contradictory or contained are filtered out, and their IDs are tracked for deletion from the vector store.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#longcontrarepeatop","text":"","title":"LongContraRepeatOp"},{"location":"personal_memory/personal_summary_ops/#purpose_5","text":"Performs more sophisticated contradiction and redundancy analysis for longer-term memory management.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_5","text":"op.long_contra_repeat_op.params.long_contra_repeat_max_count : Maximum number of memories to process (default: 50) op.long_contra_repeat_op.params.enable_long_contra_repeat : Whether to enable this operation (default: true)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_5","text":"This operation extends the basic contradiction analysis of ContraRepeatOp with the ability to resolve conflicts by modifying contradictory memories rather than simply removing them. It's particularly useful for managing long-term personal memories where information might evolve over time. For contradictory memories, it can either: - Modify the content to resolve the contradiction - Remove the memory if it's completely invalidated - Keep the most accurate/recent information","title":"Description"},{"location":"personal_memory/personal_summary_ops/#updateinsightop","text":"","title":"UpdateInsightOp"},{"location":"personal_memory/personal_summary_ops/#purpose_6","text":"Updates existing insight values based on new observations.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_6","text":"op.update_insight_op.params.update_insight_threshold : Minimum relevance score threshold (default: 0.3) op.update_insight_op.params.update_insight_max_count : Maximum number of insights to update (default: 5)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_6","text":"This operation integrates new observations into existing insights about the user. It: 1. Scores insight memories based on relevance to new observations 2. Selects the top insights that meet the relevance threshold 3. Updates each selected insight using an LLM to incorporate the new information 4. Creates updated insight memories with the original ID but new content This helps maintain accurate and up-to-date insights as new information about the user becomes available.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#getreflectionsubjectop","text":"","title":"GetReflectionSubjectOp"},{"location":"personal_memory/personal_summary_ops/#purpose_7","text":"Generates reflection subjects (topics) from personal memories for insight extraction.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_7","text":"op.get_reflection_subject_op.params.reflect_obs_cnt_threshold : Minimum number of memories required for reflection (default: 10) op.get_reflection_subject_op.params.reflect_num_questions : Maximum number of new subjects to generate (default: 3)","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_7","text":"This operation analyzes a collection of personal memories to identify potential topics for reflection and insight generation. It: 1. Checks if there are sufficient memories for meaningful reflection 2. Extracts existing insight subjects to avoid duplication 3. Uses an LLM to generate new reflection subjects based on memory content 4. Creates insight memory objects for these new subjects The generated subjects serve as focal points for organizing and synthesizing personal information about the user.","title":"Description"},{"location":"personal_memory/personal_summary_ops/#updatevectorstoreop","text":"","title":"UpdateVectorStoreOp"},{"location":"personal_memory/personal_summary_ops/#purpose_8","text":"Stores the processed memories in the vector database and removes deleted memories.","title":"Purpose"},{"location":"personal_memory/personal_summary_ops/#parameters_8","text":"No specific parameters for this operation.","title":"Parameters"},{"location":"personal_memory/personal_summary_ops/#description_8","text":"This operation is the final step in the personal memory summarization flow. It: 1. Deletes memories that were marked for removal (contradictory or redundant) 2. Inserts new or updated memories into the vector store 3. Records the number of deleted and inserted memories This ensures that the vector store remains up-to-date with the latest processed memories.","title":"Description"},{"location":"sop_memory/making_sop_memories/","text":"1. Background In LLM application development, we often need to combine multiple basic operations (atomic operations) into more complex workflows. These workflows can handle complex tasks such as data retrieval, code generation, multi-turn dialogues, and more. By combining these atomic operations into Standard Operating Procedures (SOPs), we can: Improve code reusability Simplify implementation of complex tasks Standardize common workflows Reduce development and maintenance costs This document introduces how to combine atomic operations (Ops) to form new composite operation tools using the FlowLLM framework. 2. Technical Solution 2.1 Atomic Operation Definition Each operation (Op) needs to define the following core attributes: class BaseOp : description : str # Description of the operation input_schema : Dict [ str , ParamAttr ] # Input parameter schema definition output_schema : Dict [ str , ParamAttr ] # Output parameter schema definition Where ParamAttr defines parameter type, whether it's required, and other attributes: class ParamAttr : type : Type # Parameter type, such as str, int, Dict, etc. required : bool = True # Whether it must be provided default : Any = None # Default value description : str = \"\" # Parameter description 2.2 SOP Composition Process Step 1: Create Atomic Operation Instances First, instantiate the required atomic operations: from flowllm.op.gallery.mock_op import MockOp from flowllm.op.search.tavily_search_op import TavilySearchOp from flowllm.op.agent.react_v2_op import ReactV2Op # Create atomic operation instances search_op = TavilySearchOp () react_op = ReactV2Op () summary_op = MockOp ( description = \"Summarize search results\" , input_schema = { \"search_results\" : ParamAttr ( type = str , description = \"Search results to summarize\" )}, output_schema = { \"summary\" : ParamAttr ( type = str , description = \"Summarized content\" )} ) Step 2: Define Data Flow Between Operations Set up input-output relationships between operations, defining how data flows between them: # Set input parameter sources react_op . set_input ( \"context\" , \"search_summary\" ) # react_op's context parameter is retrieved from search_summary in memory # Set output parameter destinations search_op . set_output ( \"results\" , \"search_results\" ) # search_op's results output to search_results in memory summary_op . set_output ( \"summary\" , \"search_summary\" ) # summary_op's summary output to search_summary in memory Step 3: Build Operation Flow Graph Use operators to build the operation flow graph, defining execution order and parallel relationships: # Build operation flow graph flow = search_op >> summary_op >> react_op # Or more complex flows # Parallel operations use the | operator, sequential operations use the >> operator complex_flow = ( search_op >> summary_op ) | ( another_search_op >> another_summary_op ) >> react_op Operator explanation: >> : Sequential execution, execute the next operation after the previous one completes | : Parallel execution, execute multiple operations simultaneously Step 4: Create Composite Operation Class Encapsulate the built operation flow into a new composite operation class: class SearchAndReactOp ( BaseToolOp ): description = \"Search for information and generate a response based on search results\" input_schema = ... output_schema = ... def build_flow ( self ): search_op = TavilySearchOp () summary_op = MockOp () react_op = ReactV2Op () # Set data flow search_op . set_output ( \"results\" , \"search_results\" ) summary_op . set_input ( \"search_results\" , \"search_results\" ) summary_op . set_output ( \"summary\" , \"search_summary\" ) react_op . set_input ( \"context\" , \"search_summary\" ) react_op . set_output ( \"response\" , \"response\" ) # Build operation flow graph return search_op >> summary_op >> react_op async def execute ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # Execute operation flow return await self . flow . execute ( inputs )","title":"Making SOP Memories"},{"location":"sop_memory/making_sop_memories/#1-background","text":"In LLM application development, we often need to combine multiple basic operations (atomic operations) into more complex workflows. These workflows can handle complex tasks such as data retrieval, code generation, multi-turn dialogues, and more. By combining these atomic operations into Standard Operating Procedures (SOPs), we can: Improve code reusability Simplify implementation of complex tasks Standardize common workflows Reduce development and maintenance costs This document introduces how to combine atomic operations (Ops) to form new composite operation tools using the FlowLLM framework.","title":"1. Background"},{"location":"sop_memory/making_sop_memories/#2-technical-solution","text":"","title":"2. Technical Solution"},{"location":"sop_memory/making_sop_memories/#21-atomic-operation-definition","text":"Each operation (Op) needs to define the following core attributes: class BaseOp : description : str # Description of the operation input_schema : Dict [ str , ParamAttr ] # Input parameter schema definition output_schema : Dict [ str , ParamAttr ] # Output parameter schema definition Where ParamAttr defines parameter type, whether it's required, and other attributes: class ParamAttr : type : Type # Parameter type, such as str, int, Dict, etc. required : bool = True # Whether it must be provided default : Any = None # Default value description : str = \"\" # Parameter description","title":"2.1 Atomic Operation Definition"},{"location":"sop_memory/making_sop_memories/#22-sop-composition-process","text":"","title":"2.2 SOP Composition Process"},{"location":"sop_memory/making_sop_memories/#step-1-create-atomic-operation-instances","text":"First, instantiate the required atomic operations: from flowllm.op.gallery.mock_op import MockOp from flowllm.op.search.tavily_search_op import TavilySearchOp from flowllm.op.agent.react_v2_op import ReactV2Op # Create atomic operation instances search_op = TavilySearchOp () react_op = ReactV2Op () summary_op = MockOp ( description = \"Summarize search results\" , input_schema = { \"search_results\" : ParamAttr ( type = str , description = \"Search results to summarize\" )}, output_schema = { \"summary\" : ParamAttr ( type = str , description = \"Summarized content\" )} )","title":"Step 1: Create Atomic Operation Instances"},{"location":"sop_memory/making_sop_memories/#step-2-define-data-flow-between-operations","text":"Set up input-output relationships between operations, defining how data flows between them: # Set input parameter sources react_op . set_input ( \"context\" , \"search_summary\" ) # react_op's context parameter is retrieved from search_summary in memory # Set output parameter destinations search_op . set_output ( \"results\" , \"search_results\" ) # search_op's results output to search_results in memory summary_op . set_output ( \"summary\" , \"search_summary\" ) # summary_op's summary output to search_summary in memory","title":"Step 2: Define Data Flow Between Operations"},{"location":"sop_memory/making_sop_memories/#step-3-build-operation-flow-graph","text":"Use operators to build the operation flow graph, defining execution order and parallel relationships: # Build operation flow graph flow = search_op >> summary_op >> react_op # Or more complex flows # Parallel operations use the | operator, sequential operations use the >> operator complex_flow = ( search_op >> summary_op ) | ( another_search_op >> another_summary_op ) >> react_op Operator explanation: >> : Sequential execution, execute the next operation after the previous one completes | : Parallel execution, execute multiple operations simultaneously","title":"Step 3: Build Operation Flow Graph"},{"location":"sop_memory/making_sop_memories/#step-4-create-composite-operation-class","text":"Encapsulate the built operation flow into a new composite operation class: class SearchAndReactOp ( BaseToolOp ): description = \"Search for information and generate a response based on search results\" input_schema = ... output_schema = ... def build_flow ( self ): search_op = TavilySearchOp () summary_op = MockOp () react_op = ReactV2Op () # Set data flow search_op . set_output ( \"results\" , \"search_results\" ) summary_op . set_input ( \"search_results\" , \"search_results\" ) summary_op . set_output ( \"summary\" , \"search_summary\" ) react_op . set_input ( \"context\" , \"search_summary\" ) react_op . set_output ( \"response\" , \"response\" ) # Build operation flow graph return search_op >> summary_op >> react_op async def execute ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # Execute operation flow return await self . flow . execute ( inputs )","title":"Step 4: Create Composite Operation Class"},{"location":"task_memory/task_memory/","text":"Task Memory is a key component of ReMe that allows AI agents to learn from memories and improve their performance on similar tasks in the future. This document explains how task memory works and how to use it in your applications. What is Task Memory? Task Memory represents knowledge extracted from previous task executions, including: - Successful approaches to solving problems - Common pitfalls and failures to avoid - Comparative insights between different approaches Each task memory contains: - when_to_use : Conditions that indicate when this memory is relevant - content : The actual knowledge or memory to be applied - Metadata about the memory's source and utility Configuration Logic Task Memory in ReMe is configured through two main flows: 1. Summary Task Memory The summary_task_memory flow processes conversation trajectories to extract meaningful memories: summary_task_memory : flow_content : trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> update_vector_store_op description : \"Summarizes conversation trajectories or messages into structured memory representations for long-term storage\" This flow: 1. Preprocesses trajectories ( trajectory_preprocess_op ) 2. Extracts memories based on success/failure/comparative analysis 3. Validates memories ( memory_validation_op ) 4. Updates the vector store ( update_vector_store_op ) A simplified version ( summary_task_memory_simple ) is also available for less complex use cases. 2. Retrieve Task Memory The retrieve_task_memory flow fetches relevant memories based on a query: retrieve_task_memory : flow_content : build_query_op >> recall_vector_store_op >> rerank_memory_op >> rewrite_memory_op description : \"Retrieves the most relevant top-k memory from historical data based on the current query to enhance task-solving capabilities\" This flow: 1. Builds a query from the input ( build_query_op ) 2. Recalls relevant memories from the vector store ( recall_vector_store_op ) 3. Reranks memories by relevance ( rerank_memory_op ) 4. Rewrites memories for better context integration ( rewrite_memory_op ) A simplified version ( retrieve_task_memory_simple ) is also available. Basic Usage Here's how to use Task Memory in your application: Step 1: Set Up Your Environment import requests # API configuration BASE_URL = \"http://0.0.0.0:8002/\" WORKSPACE_ID = \"your_workspace_id\" Step 2: Run an Agent and Generate Memories # Run the agent with a query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : \"Your query here\" } ) messages = response . json () . get ( \"messages\" , []) # Summarize the conversation to create task memories response = requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) Step 3: Retrieve Relevant Memories for a New Task # Retrieve memories relevant to a new query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : \"Your new query here\" } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) Step 4: Use Retrieved Memories to Enhance Agent Performance # Augment a new query with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { your_query } \" # Run agent with the augmented query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : augmented_query } ) Complete Example Here's a complete example workflow that demonstrates how to use task memory: def run_agent_with_memory ( query_first , query_second ): # Run agent with second query to build initial memories messages = run_agent ( query = query_second ) # Summarize conversation to create memories requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Retrieve relevant memories for the first query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query_first } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) # Run agent with first query augmented with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query_first } \" return run_agent ( query = augmented_query ) Managing Task Memories Delete a Workspace response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" } ) Dump Memories to Disk response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./\" } ) Load Memories from Disk response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./\" } ) Advanced Features ReMe also provides additional task memory operations: record_task_memory : Update frequency and utility attributes of retrieved memories delete_task_memory : Delete memories based on utility/frequency thresholds For more detailed examples, see the use_task_memory_demo.py file in the cookbook directory of the ReMe project.","title":"Overview"},{"location":"task_memory/task_memory/#what-is-task-memory","text":"Task Memory represents knowledge extracted from previous task executions, including: - Successful approaches to solving problems - Common pitfalls and failures to avoid - Comparative insights between different approaches Each task memory contains: - when_to_use : Conditions that indicate when this memory is relevant - content : The actual knowledge or memory to be applied - Metadata about the memory's source and utility","title":"What is Task Memory?"},{"location":"task_memory/task_memory/#configuration-logic","text":"Task Memory in ReMe is configured through two main flows:","title":"Configuration Logic"},{"location":"task_memory/task_memory/#1-summary-task-memory","text":"The summary_task_memory flow processes conversation trajectories to extract meaningful memories: summary_task_memory : flow_content : trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> update_vector_store_op description : \"Summarizes conversation trajectories or messages into structured memory representations for long-term storage\" This flow: 1. Preprocesses trajectories ( trajectory_preprocess_op ) 2. Extracts memories based on success/failure/comparative analysis 3. Validates memories ( memory_validation_op ) 4. Updates the vector store ( update_vector_store_op ) A simplified version ( summary_task_memory_simple ) is also available for less complex use cases.","title":"1. Summary Task Memory"},{"location":"task_memory/task_memory/#2-retrieve-task-memory","text":"The retrieve_task_memory flow fetches relevant memories based on a query: retrieve_task_memory : flow_content : build_query_op >> recall_vector_store_op >> rerank_memory_op >> rewrite_memory_op description : \"Retrieves the most relevant top-k memory from historical data based on the current query to enhance task-solving capabilities\" This flow: 1. Builds a query from the input ( build_query_op ) 2. Recalls relevant memories from the vector store ( recall_vector_store_op ) 3. Reranks memories by relevance ( rerank_memory_op ) 4. Rewrites memories for better context integration ( rewrite_memory_op ) A simplified version ( retrieve_task_memory_simple ) is also available.","title":"2. Retrieve Task Memory"},{"location":"task_memory/task_memory/#basic-usage","text":"Here's how to use Task Memory in your application:","title":"Basic Usage"},{"location":"task_memory/task_memory/#step-1-set-up-your-environment","text":"import requests # API configuration BASE_URL = \"http://0.0.0.0:8002/\" WORKSPACE_ID = \"your_workspace_id\"","title":"Step 1: Set Up Your Environment"},{"location":"task_memory/task_memory/#step-2-run-an-agent-and-generate-memories","text":"# Run the agent with a query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : \"Your query here\" } ) messages = response . json () . get ( \"messages\" , []) # Summarize the conversation to create task memories response = requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } )","title":"Step 2: Run an Agent and Generate Memories"},{"location":"task_memory/task_memory/#step-3-retrieve-relevant-memories-for-a-new-task","text":"# Retrieve memories relevant to a new query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : \"Your new query here\" } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" )","title":"Step 3: Retrieve Relevant Memories for a New Task"},{"location":"task_memory/task_memory/#step-4-use-retrieved-memories-to-enhance-agent-performance","text":"# Augment a new query with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { your_query } \" # Run agent with the augmented query response = requests . post ( url = f \" { BASE_URL } react\" , json = { \"query\" : augmented_query } )","title":"Step 4: Use Retrieved Memories to Enhance Agent Performance"},{"location":"task_memory/task_memory/#complete-example","text":"Here's a complete example workflow that demonstrates how to use task memory: def run_agent_with_memory ( query_first , query_second ): # Run agent with second query to build initial memories messages = run_agent ( query = query_second ) # Summarize conversation to create memories requests . post ( url = f \" { BASE_URL } summary_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"trajectories\" : [ { \"messages\" : messages , \"score\" : 1.0 } ] } ) # Retrieve relevant memories for the first query response = requests . post ( url = f \" { BASE_URL } retrieve_task_memory\" , json = { \"workspace_id\" : WORKSPACE_ID , \"query\" : query_first } ) retrieved_memory = response . json () . get ( \"answer\" , \"\" ) # Run agent with first query augmented with retrieved memories augmented_query = f \" { retrieved_memory } \\n\\n User Question: \\n { query_first } \" return run_agent ( query = augmented_query )","title":"Complete Example"},{"location":"task_memory/task_memory/#managing-task-memories","text":"","title":"Managing Task Memories"},{"location":"task_memory/task_memory/#delete-a-workspace","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"delete\" } )","title":"Delete a Workspace"},{"location":"task_memory/task_memory/#dump-memories-to-disk","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"dump\" , \"path\" : \"./\" } )","title":"Dump Memories to Disk"},{"location":"task_memory/task_memory/#load-memories-from-disk","text":"response = requests . post ( url = f \" { BASE_URL } vector_store\" , json = { \"workspace_id\" : WORKSPACE_ID , \"action\" : \"load\" , \"path\" : \"./\" } )","title":"Load Memories from Disk"},{"location":"task_memory/task_memory/#advanced-features","text":"ReMe also provides additional task memory operations: record_task_memory : Update frequency and utility attributes of retrieved memories delete_task_memory : Delete memories based on utility/frequency thresholds For more detailed examples, see the use_task_memory_demo.py file in the cookbook directory of the ReMe project.","title":"Advanced Features"},{"location":"task_memory/task_retrieve_ops/","text":"BuildQueryOp Purpose Constructs a query for memory retrieval either from a direct query input or by analyzing conversation messages. Functionality If a direct query is provided in the context, it uses that query If messages are provided in the context, it can: Use an LLM to generate a query based on the conversation context Or create a simple query from recent messages without using an LLM Parameters op.build_query_op.params.enable_llm_build (boolean, default: true ): When true , uses an LLM to generate a query from conversation messages When false , creates a simple query by concatenating recent messages RerankMemoryOp Purpose Reranks and filters recalled memories to ensure the most relevant memories are prioritized. Functionality Reranks memories using LLM-based analysis (optional) Filters memories based on quality scores (optional) Returns the top-k most relevant memories Parameters op.rerank_memory_op.params.enable_llm_rerank (boolean, default: true ): When true , uses an LLM to rerank memories based on their relevance to the query op.rerank_memory_op.params.enable_score_filter (boolean, default: false ): When true , filters memories based on their quality scores op.rerank_memory_op.params.min_score_threshold (float, default: 0.3 ): Minimum score threshold for filtering memories when enable_score_filter is true op.rerank_memory_op.params.top_k (integer, default: 5 ): Number of top memories to retain after reranking RewriteMemoryOp Purpose Rewrites and formats the retrieved memories to make them more relevant and actionable for the current context. Functionality Formats retrieved memories into a structured format Can use an LLM to rewrite memories to better fit the current context (optional) Generates a cohesive context message from multiple memories Parameters op.rewrite_memory_op.params.enable_llm_rewrite (boolean, default: true ): When true , uses an LLM to rewrite the memories to make them more relevant and actionable When false , simply formats the memories without LLM-based rewriting MergeMemoryOp Purpose An alternative to RewriteMemoryOp that merges multiple memories into a single response without using an LLM. Functionality Collects the content from all memories in the memory list Formats them into a single response with a standard structure Adds a prompt to consider the helpful parts when answering the question","title":"Retrieve Ops"},{"location":"task_memory/task_retrieve_ops/#buildqueryop","text":"","title":"BuildQueryOp"},{"location":"task_memory/task_retrieve_ops/#purpose","text":"Constructs a query for memory retrieval either from a direct query input or by analyzing conversation messages.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality","text":"If a direct query is provided in the context, it uses that query If messages are provided in the context, it can: Use an LLM to generate a query based on the conversation context Or create a simple query from recent messages without using an LLM","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters","text":"op.build_query_op.params.enable_llm_build (boolean, default: true ): When true , uses an LLM to generate a query from conversation messages When false , creates a simple query by concatenating recent messages","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#rerankmemoryop","text":"","title":"RerankMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_1","text":"Reranks and filters recalled memories to ensure the most relevant memories are prioritized.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_1","text":"Reranks memories using LLM-based analysis (optional) Filters memories based on quality scores (optional) Returns the top-k most relevant memories","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters_1","text":"op.rerank_memory_op.params.enable_llm_rerank (boolean, default: true ): When true , uses an LLM to rerank memories based on their relevance to the query op.rerank_memory_op.params.enable_score_filter (boolean, default: false ): When true , filters memories based on their quality scores op.rerank_memory_op.params.min_score_threshold (float, default: 0.3 ): Minimum score threshold for filtering memories when enable_score_filter is true op.rerank_memory_op.params.top_k (integer, default: 5 ): Number of top memories to retain after reranking","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#rewritememoryop","text":"","title":"RewriteMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_2","text":"Rewrites and formats the retrieved memories to make them more relevant and actionable for the current context.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_2","text":"Formats retrieved memories into a structured format Can use an LLM to rewrite memories to better fit the current context (optional) Generates a cohesive context message from multiple memories","title":"Functionality"},{"location":"task_memory/task_retrieve_ops/#parameters_2","text":"op.rewrite_memory_op.params.enable_llm_rewrite (boolean, default: true ): When true , uses an LLM to rewrite the memories to make them more relevant and actionable When false , simply formats the memories without LLM-based rewriting","title":"Parameters"},{"location":"task_memory/task_retrieve_ops/#mergememoryop","text":"","title":"MergeMemoryOp"},{"location":"task_memory/task_retrieve_ops/#purpose_3","text":"An alternative to RewriteMemoryOp that merges multiple memories into a single response without using an LLM.","title":"Purpose"},{"location":"task_memory/task_retrieve_ops/#functionality_3","text":"Collects the content from all memories in the memory list Formats them into a single response with a standard structure Adds a prompt to consider the helpful parts when answering the question","title":"Functionality"},{"location":"task_memory/task_summary_ops/","text":"TrajectoryPreprocessOp Purpose Preprocesses trajectories by validating and classifying them based on their score. Functionality Validates and classifies trajectories as success or failure based on a threshold Modifies tool calls in messages to ensure consistent format Sets context for downstream operators with classified trajectories Parameters op.trajectory_preprocess_op.params.success_threshold (float, default: 1.0 ): The threshold score that determines if a trajectory is considered successful Trajectories with scores greater than or equal to this value are classified as successful TrajectorySegmentationOp Purpose Segments trajectories into meaningful step sequences to enable more granular memory extraction. Functionality Uses LLM to identify logical break points in trajectories Adds segmentation information to trajectory metadata Enables more focused memory extraction from specific parts of conversations Parameters op.trajectory_segmentation_op.params.segment_target (string, default: \"all\" ): Determines which trajectories to segment Options: \"all\" , \"success\" , \"failure\" SuccessExtractionOp Purpose Extracts task memories from successful trajectories. Functionality Processes successful trajectories to identify valuable memories Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions Parameters No specific parameters beyond the LLM configuration. FailureExtractionOp Purpose Extracts task memories from failed trajectories to capture lessons learned from unsuccessful attempts. Functionality Processes failed trajectories to identify pitfalls and mistakes Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions Parameters No specific parameters beyond the LLM configuration. ComparativeExtractionOp Purpose Extracts comparative task memories by comparing different scoring trajectories. Functionality Performs \"soft comparison\" between highest and lowest scoring trajectories Can perform \"hard comparison\" between success and failure trajectories using similarity search Identifies key differences that contributed to success or failure Parameters op.comparative_extraction_op.params.enable_soft_comparison (boolean, default: true ): When true , enables comparison between highest and lowest scoring trajectories op.comparative_extraction_op.params.enable_similarity_comparison (boolean, default: false ): When true , enables similarity-based comparison between success and failure trajectories op.comparative_extraction_op.params.similarity_threshold (float, default: 0.3 ): The threshold for considering two trajectories similar op.comparative_extraction_op.params.max_similarity_sequences (integer, default: 5 ): Maximum number of sequences to compare to avoid computational overload op.comparative_extraction_op.params.max_similarity_pairs (integer, default: 3 ): Maximum number of similar pairs to process MemoryValidationOp Purpose Validates the quality of extracted task memories to ensure they are useful and relevant. Functionality Uses LLM to validate each extracted memory Scores memories based on quality and relevance Filters out low-quality memories based on validation threshold Parameters op.memory_validation_op.params.validation_threshold (float, default: 0.5 ): The minimum score for a memory to be considered valid MemoryDeduplicationOp Purpose Removes duplicate task memories to avoid redundancy in the vector store. Functionality Compares new memories with existing memories in the vector store Uses embedding similarity to identify duplicates Ensures only unique memories are stored Parameters op.memory_deduplication_op.params.similarity_threshold (float, default: 0.5 ): The threshold for considering two memories similar op.memory_deduplication_op.params.max_existing_task_memories (integer, default: 1000 ): Maximum number of existing memories to check against SimpleSummaryOp Purpose A simplified version of memory extraction that processes entire trajectories in one step. Functionality Classifies trajectories as success or failure based on score threshold Extracts memories directly from complete trajectories Useful for simpler use cases where detailed segmentation is not required Parameters op.simple_summary_op.params.success_score_threshold (float, default: 0.9 ): The threshold score that determines if a trajectory is considered successful SimpleComparativeSummaryOp Purpose A simplified version of comparative memory extraction. Functionality Groups trajectories by task ID Compares the highest and lowest scoring trajectories for each task Extracts comparative insights without complex segmentation Parameters No specific parameters beyond the LLM configuration.","title":"Summary Ops"},{"location":"task_memory/task_summary_ops/#trajectorypreprocessop","text":"","title":"TrajectoryPreprocessOp"},{"location":"task_memory/task_summary_ops/#purpose","text":"Preprocesses trajectories by validating and classifying them based on their score.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality","text":"Validates and classifies trajectories as success or failure based on a threshold Modifies tool calls in messages to ensure consistent format Sets context for downstream operators with classified trajectories","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters","text":"op.trajectory_preprocess_op.params.success_threshold (float, default: 1.0 ): The threshold score that determines if a trajectory is considered successful Trajectories with scores greater than or equal to this value are classified as successful","title":"Parameters"},{"location":"task_memory/task_summary_ops/#trajectorysegmentationop","text":"","title":"TrajectorySegmentationOp"},{"location":"task_memory/task_summary_ops/#purpose_1","text":"Segments trajectories into meaningful step sequences to enable more granular memory extraction.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_1","text":"Uses LLM to identify logical break points in trajectories Adds segmentation information to trajectory metadata Enables more focused memory extraction from specific parts of conversations","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_1","text":"op.trajectory_segmentation_op.params.segment_target (string, default: \"all\" ): Determines which trajectories to segment Options: \"all\" , \"success\" , \"failure\"","title":"Parameters"},{"location":"task_memory/task_summary_ops/#successextractionop","text":"","title":"SuccessExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_2","text":"Extracts task memories from successful trajectories.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_2","text":"Processes successful trajectories to identify valuable memories Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_2","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"},{"location":"task_memory/task_summary_ops/#failureextractionop","text":"","title":"FailureExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_3","text":"Extracts task memories from failed trajectories to capture lessons learned from unsuccessful attempts.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_3","text":"Processes failed trajectories to identify pitfalls and mistakes Can work with both entire trajectories and segmented step sequences Uses LLM to extract structured task memories with when-to-use conditions","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_3","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"},{"location":"task_memory/task_summary_ops/#comparativeextractionop","text":"","title":"ComparativeExtractionOp"},{"location":"task_memory/task_summary_ops/#purpose_4","text":"Extracts comparative task memories by comparing different scoring trajectories.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_4","text":"Performs \"soft comparison\" between highest and lowest scoring trajectories Can perform \"hard comparison\" between success and failure trajectories using similarity search Identifies key differences that contributed to success or failure","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_4","text":"op.comparative_extraction_op.params.enable_soft_comparison (boolean, default: true ): When true , enables comparison between highest and lowest scoring trajectories op.comparative_extraction_op.params.enable_similarity_comparison (boolean, default: false ): When true , enables similarity-based comparison between success and failure trajectories op.comparative_extraction_op.params.similarity_threshold (float, default: 0.3 ): The threshold for considering two trajectories similar op.comparative_extraction_op.params.max_similarity_sequences (integer, default: 5 ): Maximum number of sequences to compare to avoid computational overload op.comparative_extraction_op.params.max_similarity_pairs (integer, default: 3 ): Maximum number of similar pairs to process","title":"Parameters"},{"location":"task_memory/task_summary_ops/#memoryvalidationop","text":"","title":"MemoryValidationOp"},{"location":"task_memory/task_summary_ops/#purpose_5","text":"Validates the quality of extracted task memories to ensure they are useful and relevant.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_5","text":"Uses LLM to validate each extracted memory Scores memories based on quality and relevance Filters out low-quality memories based on validation threshold","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_5","text":"op.memory_validation_op.params.validation_threshold (float, default: 0.5 ): The minimum score for a memory to be considered valid","title":"Parameters"},{"location":"task_memory/task_summary_ops/#memorydeduplicationop","text":"","title":"MemoryDeduplicationOp"},{"location":"task_memory/task_summary_ops/#purpose_6","text":"Removes duplicate task memories to avoid redundancy in the vector store.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_6","text":"Compares new memories with existing memories in the vector store Uses embedding similarity to identify duplicates Ensures only unique memories are stored","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_6","text":"op.memory_deduplication_op.params.similarity_threshold (float, default: 0.5 ): The threshold for considering two memories similar op.memory_deduplication_op.params.max_existing_task_memories (integer, default: 1000 ): Maximum number of existing memories to check against","title":"Parameters"},{"location":"task_memory/task_summary_ops/#simplesummaryop","text":"","title":"SimpleSummaryOp"},{"location":"task_memory/task_summary_ops/#purpose_7","text":"A simplified version of memory extraction that processes entire trajectories in one step.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_7","text":"Classifies trajectories as success or failure based on score threshold Extracts memories directly from complete trajectories Useful for simpler use cases where detailed segmentation is not required","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_7","text":"op.simple_summary_op.params.success_score_threshold (float, default: 0.9 ): The threshold score that determines if a trajectory is considered successful","title":"Parameters"},{"location":"task_memory/task_summary_ops/#simplecomparativesummaryop","text":"","title":"SimpleComparativeSummaryOp"},{"location":"task_memory/task_summary_ops/#purpose_8","text":"A simplified version of comparative memory extraction.","title":"Purpose"},{"location":"task_memory/task_summary_ops/#functionality_8","text":"Groups trajectories by task ID Compares the highest and lowest scoring trajectories for each task Extracts comparative insights without complex segmentation","title":"Functionality"},{"location":"task_memory/task_summary_ops/#parameters_8","text":"No specific parameters beyond the LLM configuration.","title":"Parameters"}]}